{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCAE_GTSRB Stop Signs With Adversarial Attacks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/raghavchalapathy/one_class_neural_networks/blob/master/RCAE_GTSRB_Stop_Signs_With_Adversarial_Attacks.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "MYHppGoobh5k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "66df1f1b-f5a4-4176-bc1c-6241317963d3"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install fuel\n",
        "!pip install picklable_itertools"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: fuel in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: picklable-itertools in /usr/local/lib/python3.6/dist-packages (from fuel) (0.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from fuel) (16.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fuel) (0.19.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fuel) (1.14.6)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from fuel) (3.38.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from fuel) (2.8.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fuel) (4.0.0)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from fuel) (3.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fuel) (2.18.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fuel) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fuel) (1.11.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->fuel) (2.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->fuel) (0.46)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->fuel) (2.6.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2018.8.24)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (3.0.4)\n",
            "Requirement already satisfied: picklable_itertools in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from picklable_itertools) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kXZkgAwHblKH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "PROJECT_DIR = \"/content/drive/My Drive/one_class_neural_networks/\"\n",
        "import sys,os\n",
        "import numpy as np\n",
        "sys.path.append(PROJECT_DIR)\n",
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OReZApmkbNPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "np.random.seed(42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_g2dvIdDbNPK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Obtain Training and Test Datasets"
      ]
    },
    {
      "metadata": {
        "id": "T1TOklXdbNPL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124108
        },
        "outputId": "67995174-a115-4731-9b50-0e170f0f484c"
      },
      "cell_type": "code",
      "source": [
        "## Obtaining the training and testing data\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from src.models.RCAE import RCAE_AD\n",
        "\n",
        "DATASET = \"gtsrb\"\n",
        "IMG_DIM= 3072\n",
        "IMG_HGT =32\n",
        "IMG_WDT=32\n",
        "IMG_CHANNEL=3\n",
        "HIDDEN_LAYER_SIZE= 32\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/gtsrb/RCAE/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/gtsrb/RCAE/\"\n",
        "PRETRAINED_WT_PATH = \"\"\n",
        "\n",
        "rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n",
        "\n",
        "# print(\"Train Data Shape: \",rcae.data._X_train.shape)\n",
        "# print(\"Train Label Shape: \",rcae.data._y_train.shape)\n",
        "# print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n",
        "# print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n",
        "# print(\"Test Data Shape: \",rcae.data._X_test.shape)\n",
        "# print(\"Test Label Shape: \",rcae.data._y_test.shape)\n",
        "print(\"===========TRAINING AND PREDICTING WITH RCAE============================\")\n",
        "rcae.fit_and_predict()\n",
        "print(\"========================================================================\")\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RCAE.RESULT_PATH: /content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE/\n",
            "[INFO:] Loading data...\n",
            "[INFO:] Loading adversarial data...\n",
            "[INFO:] Negative Y_test labels 50\n",
            "[INFO:] Positive Y_test labels 270\n",
            "\u001b[F\u001b[KData loaded.\n",
            "[INFO:] Assertions of memory muted\n",
            "[INFO:] Loading data...\n",
            "[INFO:] Loading adversarial data...\n",
            "[INFO:] Negative Y_test labels 50\n",
            "[INFO:] Positive Y_test labels 270\n",
            "\u001b[F\u001b[KData loaded.\n",
            "===========TRAINING AND PREDICTING WITH RCAE============================\n",
            "[INFO:]  Length of Positive data 1050\n",
            "[INFO:]  Length of Negative data 50\n",
            "[INFO:] X_train.shape (1100, 32, 32, 3)\n",
            "[INFO:] y_train.shape (1100,)\n",
            "[INFO:] X_test.shape (1100, 32, 32, 3)\n",
            "[INFO:] y_test.shape (1100,)\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 32) (256, 32)\n",
            "Train on 990 samples, validate on 110 samples\n",
            "Epoch 1/500\n",
            "990/990 [==============================] - 9s 9ms/step - loss: 0.8460 - val_loss: 0.8477\n",
            "Epoch 2/500\n",
            "990/990 [==============================] - 1s 797us/step - loss: 0.8084 - val_loss: 0.8216\n",
            "Epoch 3/500\n",
            "990/990 [==============================] - 1s 781us/step - loss: 0.7958 - val_loss: 0.8067\n",
            "Epoch 4/500\n",
            "990/990 [==============================] - 1s 799us/step - loss: 0.7899 - val_loss: 0.7989\n",
            "Epoch 5/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 0.7863 - val_loss: 0.7917\n",
            "Epoch 6/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 0.7834 - val_loss: 0.7891\n",
            "Epoch 7/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 0.7818 - val_loss: 0.7858\n",
            "Epoch 8/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 0.7801 - val_loss: 0.7855\n",
            "Epoch 9/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7791 - val_loss: 0.7827\n",
            "Epoch 10/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7782 - val_loss: 0.7820\n",
            "Epoch 11/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7770 - val_loss: 0.7802\n",
            "Epoch 12/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.7763 - val_loss: 0.7785\n",
            "Epoch 13/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7758 - val_loss: 0.7781\n",
            "Epoch 14/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7760 - val_loss: 0.7772\n",
            "Epoch 15/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7751 - val_loss: 0.7770\n",
            "Epoch 16/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7744 - val_loss: 0.7758\n",
            "Epoch 17/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.7744 - val_loss: 0.7763\n",
            "Epoch 18/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7741 - val_loss: 0.7752\n",
            "Epoch 19/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7734 - val_loss: 0.7747\n",
            "Epoch 20/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7731 - val_loss: 0.7744\n",
            "Epoch 21/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.7728 - val_loss: 0.7741\n",
            "Epoch 22/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7722 - val_loss: 0.7741\n",
            "Epoch 23/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.7722 - val_loss: 0.7734\n",
            "Epoch 24/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 0.7722 - val_loss: 0.7736\n",
            "Epoch 25/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7719 - val_loss: 0.7733\n",
            "Epoch 26/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7716 - val_loss: 0.7732\n",
            "Epoch 27/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7719 - val_loss: 0.7734\n",
            "Epoch 28/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7714 - val_loss: 0.7733\n",
            "Epoch 29/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7707 - val_loss: 0.7726\n",
            "Epoch 30/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.7708 - val_loss: 0.7729\n",
            "Epoch 31/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7707 - val_loss: 0.7723\n",
            "Epoch 32/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7700 - val_loss: 0.7724\n",
            "Epoch 33/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.7698 - val_loss: 0.7722\n",
            "Epoch 34/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7698 - val_loss: 0.7720\n",
            "Epoch 35/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7696 - val_loss: 0.7723\n",
            "Epoch 36/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7695 - val_loss: 0.7726\n",
            "Epoch 37/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 0.7700 - val_loss: 0.7725\n",
            "Epoch 38/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.7698 - val_loss: 0.7719\n",
            "Epoch 39/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7693 - val_loss: 0.7716\n",
            "Epoch 40/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 0.7691 - val_loss: 0.7716\n",
            "Epoch 41/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7697 - val_loss: 0.7726\n",
            "Epoch 42/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.7689 - val_loss: 0.7719\n",
            "Epoch 43/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.7687 - val_loss: 0.7713\n",
            "Epoch 44/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7684 - val_loss: 0.7714\n",
            "Epoch 45/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 0.7684 - val_loss: 0.7712\n",
            "Epoch 46/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7684 - val_loss: 0.7715\n",
            "Epoch 47/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7684 - val_loss: 0.7714\n",
            "Epoch 48/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7687 - val_loss: 0.7716\n",
            "Epoch 49/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.7681 - val_loss: 0.7712\n",
            "Epoch 50/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.7681 - val_loss: 0.7708\n",
            "Epoch 51/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7677 - val_loss: 0.7717\n",
            "Epoch 52/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7676 - val_loss: 0.7709\n",
            "Epoch 53/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.7674 - val_loss: 0.7709\n",
            "Epoch 54/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7673 - val_loss: 0.7706\n",
            "Epoch 55/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 0.7670 - val_loss: 0.7705\n",
            "Epoch 56/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7674 - val_loss: 0.7708\n",
            "Epoch 57/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.7673 - val_loss: 0.7705\n",
            "Epoch 58/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7669 - val_loss: 0.7703\n",
            "Epoch 59/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.7668 - val_loss: 0.7709\n",
            "Epoch 60/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.7681 - val_loss: 0.7707\n",
            "Epoch 61/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7667 - val_loss: 0.7706\n",
            "Epoch 62/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7667 - val_loss: 0.7704\n",
            "Epoch 63/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7667 - val_loss: 0.7708\n",
            "Epoch 64/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7665 - val_loss: 0.7700\n",
            "Epoch 65/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7666 - val_loss: 0.7702\n",
            "Epoch 66/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 0.7661 - val_loss: 0.7706\n",
            "Epoch 67/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7663 - val_loss: 0.7709\n",
            "Epoch 68/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.7663 - val_loss: 0.7706\n",
            "Epoch 69/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7666 - val_loss: 0.7709\n",
            "Epoch 70/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 0.7667 - val_loss: 0.7700\n",
            "Epoch 71/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.7662 - val_loss: 0.7702\n",
            "Epoch 72/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.7664 - val_loss: 0.7704\n",
            "Epoch 73/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7663 - val_loss: 0.7702\n",
            "Epoch 74/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7661 - val_loss: 0.7697\n",
            "Epoch 75/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.7661 - val_loss: 0.7697\n",
            "Epoch 76/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7656 - val_loss: 0.7699\n",
            "Epoch 77/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.7660 - val_loss: 0.7706\n",
            "Epoch 78/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.7660 - val_loss: 0.7707\n",
            "Epoch 79/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7657 - val_loss: 0.7702\n",
            "Epoch 80/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.7660 - val_loss: 0.7699\n",
            "Epoch 81/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7660 - val_loss: 0.7700\n",
            "Epoch 82/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7660 - val_loss: 0.7697\n",
            "Epoch 83/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.7659 - val_loss: 0.7698\n",
            "Epoch 84/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7658 - val_loss: 0.7699\n",
            "Epoch 85/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7654 - val_loss: 0.7694\n",
            "Epoch 86/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.7655 - val_loss: 0.7694\n",
            "Epoch 87/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.7656 - val_loss: 0.7698\n",
            "Epoch 88/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7656 - val_loss: 0.7697\n",
            "Epoch 89/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7659 - val_loss: 0.7694\n",
            "Epoch 90/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 0.7659 - val_loss: 0.7713\n",
            "Epoch 91/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.7657 - val_loss: 0.7696\n",
            "Epoch 92/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7654 - val_loss: 0.7696\n",
            "Epoch 93/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7651 - val_loss: 0.7695\n",
            "Epoch 94/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7654 - val_loss: 0.7690\n",
            "Epoch 95/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 0.7658 - val_loss: 0.7698\n",
            "Epoch 96/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7652 - val_loss: 0.7692\n",
            "Epoch 97/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.7650 - val_loss: 0.7702\n",
            "Epoch 98/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7654 - val_loss: 0.7691\n",
            "Epoch 99/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.7655 - val_loss: 0.7692\n",
            "Epoch 100/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7652 - val_loss: 0.7701\n",
            "Epoch 101/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7652 - val_loss: 0.7691\n",
            "Epoch 102/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.7647 - val_loss: 0.7691\n",
            "Epoch 103/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7649 - val_loss: 0.7695\n",
            "Epoch 104/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7649 - val_loss: 0.7694\n",
            "Epoch 105/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7645 - val_loss: 0.7691\n",
            "Epoch 106/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 0.7646 - val_loss: 0.7697\n",
            "Epoch 107/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.7644 - val_loss: 0.7690\n",
            "Epoch 108/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.7645 - val_loss: 0.7695\n",
            "Epoch 109/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.7643 - val_loss: 0.7689\n",
            "Epoch 110/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7642 - val_loss: 0.7692\n",
            "Epoch 111/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7642 - val_loss: 0.7689\n",
            "Epoch 112/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7645 - val_loss: 0.7694\n",
            "Epoch 113/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.7647 - val_loss: 0.7688\n",
            "Epoch 114/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7651 - val_loss: 0.7690\n",
            "Epoch 115/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.7650 - val_loss: 0.7691\n",
            "Epoch 116/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7645 - val_loss: 0.7689\n",
            "Epoch 117/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7645 - val_loss: 0.7693\n",
            "Epoch 118/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7642 - val_loss: 0.7688\n",
            "Epoch 119/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7641 - val_loss: 0.7686\n",
            "Epoch 120/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.7644 - val_loss: 0.7688\n",
            "Epoch 121/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7644 - val_loss: 0.7690\n",
            "Epoch 122/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.7644 - val_loss: 0.7686\n",
            "Epoch 123/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7639 - val_loss: 0.7685\n",
            "Epoch 124/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7641 - val_loss: 0.7685\n",
            "Epoch 125/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7642 - val_loss: 0.7688\n",
            "Epoch 126/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7642 - val_loss: 0.7686\n",
            "Epoch 127/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7643 - val_loss: 0.7686\n",
            "Epoch 128/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7644 - val_loss: 0.7686\n",
            "Epoch 129/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7641 - val_loss: 0.7683\n",
            "Epoch 130/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.7640 - val_loss: 0.7693\n",
            "Epoch 131/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.7639 - val_loss: 0.7684\n",
            "Epoch 132/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7636 - val_loss: 0.7686\n",
            "Epoch 133/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 0.7643 - val_loss: 0.7687\n",
            "Epoch 134/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7635 - val_loss: 0.7684\n",
            "Epoch 135/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.7638 - val_loss: 0.7689\n",
            "Epoch 136/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7636 - val_loss: 0.7685\n",
            "Epoch 137/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.7637 - val_loss: 0.7682\n",
            "Epoch 138/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.7640 - val_loss: 0.7685\n",
            "Epoch 139/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7635 - val_loss: 0.7685\n",
            "Epoch 140/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7640 - val_loss: 0.7685\n",
            "Epoch 141/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7633 - val_loss: 0.7685\n",
            "Epoch 142/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7644 - val_loss: 0.7685\n",
            "Epoch 143/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7639 - val_loss: 0.7684\n",
            "Epoch 144/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7633 - val_loss: 0.7685\n",
            "Epoch 145/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7634 - val_loss: 0.7684\n",
            "Epoch 146/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7636 - val_loss: 0.7682\n",
            "Epoch 147/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7633 - val_loss: 0.7680\n",
            "Epoch 148/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7638 - val_loss: 0.7682\n",
            "Epoch 149/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7635 - val_loss: 0.7684\n",
            "Epoch 150/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.7637 - val_loss: 0.7691\n",
            "Epoch 151/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7634 - val_loss: 0.7686\n",
            "Epoch 152/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7631 - val_loss: 0.7681\n",
            "Epoch 153/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.7639 - val_loss: 0.7683\n",
            "Epoch 154/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7637 - val_loss: 0.7686\n",
            "Epoch 155/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.7637 - val_loss: 0.7687\n",
            "Epoch 156/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7639 - val_loss: 0.7681\n",
            "Epoch 157/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7629 - val_loss: 0.7680\n",
            "Epoch 158/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.7632 - val_loss: 0.7681\n",
            "Epoch 159/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7633 - val_loss: 0.7686\n",
            "Epoch 160/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7632 - val_loss: 0.7682\n",
            "Epoch 161/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7632 - val_loss: 0.7685\n",
            "Epoch 162/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 0.7631 - val_loss: 0.7682\n",
            "Epoch 163/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 0.7630 - val_loss: 0.7684\n",
            "Epoch 164/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7634 - val_loss: 0.7691\n",
            "Epoch 165/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7631 - val_loss: 0.7688\n",
            "Epoch 166/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7633 - val_loss: 0.7684\n",
            "Epoch 167/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7635 - val_loss: 0.7679\n",
            "Epoch 168/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.7633 - val_loss: 0.7685\n",
            "Epoch 169/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.7637 - val_loss: 0.7683\n",
            "Epoch 170/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7638 - val_loss: 0.7698\n",
            "Epoch 171/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 0.7632 - val_loss: 0.7684\n",
            "Epoch 172/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7626 - val_loss: 0.7680\n",
            "Epoch 173/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.7628 - val_loss: 0.7685\n",
            "Epoch 174/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.7627 - val_loss: 0.7684\n",
            "Epoch 175/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7633 - val_loss: 0.7684\n",
            "Epoch 176/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7629 - val_loss: 0.7681\n",
            "Epoch 177/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7636 - val_loss: 0.7681\n",
            "Epoch 178/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7630 - val_loss: 0.7679\n",
            "Epoch 179/500\n",
            "990/990 [==============================] - 1s 734us/step - loss: 0.7631 - val_loss: 0.7679\n",
            "Epoch 180/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7625 - val_loss: 0.7681\n",
            "Epoch 181/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7628 - val_loss: 0.7679\n",
            "Epoch 182/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7630 - val_loss: 0.7678\n",
            "Epoch 183/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7630 - val_loss: 0.7685\n",
            "Epoch 184/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7634 - val_loss: 0.7679\n",
            "Epoch 185/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7632 - val_loss: 0.7691\n",
            "Epoch 186/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7635 - val_loss: 0.7681\n",
            "Epoch 187/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.7632 - val_loss: 0.7682\n",
            "Epoch 188/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 0.7632 - val_loss: 0.7681\n",
            "Epoch 189/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7630 - val_loss: 0.7678\n",
            "Epoch 190/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.7628 - val_loss: 0.7679\n",
            "Epoch 191/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7626 - val_loss: 0.7682\n",
            "Epoch 192/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7625 - val_loss: 0.7677\n",
            "Epoch 193/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 0.7627 - val_loss: 0.7679\n",
            "Epoch 194/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.7628 - val_loss: 0.7681\n",
            "Epoch 195/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7626 - val_loss: 0.7679\n",
            "Epoch 196/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.7625 - val_loss: 0.7678\n",
            "Epoch 197/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.7626 - val_loss: 0.7681\n",
            "Epoch 198/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.7628 - val_loss: 0.7679\n",
            "Epoch 199/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.7626 - val_loss: 0.7677\n",
            "Epoch 200/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7626 - val_loss: 0.7681\n",
            "Epoch 201/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7628 - val_loss: 0.7681\n",
            "Epoch 202/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7624 - val_loss: 0.7679\n",
            "Epoch 203/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7623 - val_loss: 0.7676\n",
            "Epoch 204/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 0.7629 - val_loss: 0.7684\n",
            "Epoch 205/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7631 - val_loss: 0.7681\n",
            "Epoch 206/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7630 - val_loss: 0.7678\n",
            "Epoch 207/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.7625 - val_loss: 0.7678\n",
            "Epoch 208/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.7627 - val_loss: 0.7676\n",
            "Epoch 209/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.7626 - val_loss: 0.7676\n",
            "Epoch 210/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7626 - val_loss: 0.7678\n",
            "Epoch 211/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7626 - val_loss: 0.7680\n",
            "Epoch 212/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7629 - val_loss: 0.7681\n",
            "Epoch 213/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7627 - val_loss: 0.7685\n",
            "Epoch 214/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 0.7625 - val_loss: 0.7676\n",
            "Epoch 215/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7624 - val_loss: 0.7675\n",
            "Epoch 216/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7620 - val_loss: 0.7678\n",
            "Epoch 217/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.7624 - val_loss: 0.7677\n",
            "Epoch 218/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7623 - val_loss: 0.7675\n",
            "Epoch 219/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.7622 - val_loss: 0.7678\n",
            "Epoch 220/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7625 - val_loss: 0.7676\n",
            "Epoch 221/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 0.7625 - val_loss: 0.7678\n",
            "Epoch 222/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.7622 - val_loss: 0.7681\n",
            "Epoch 223/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.7626 - val_loss: 0.7675\n",
            "Epoch 224/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 0.7621 - val_loss: 0.7674\n",
            "Epoch 225/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7623 - val_loss: 0.7679\n",
            "Epoch 226/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7626 - val_loss: 0.7676\n",
            "Epoch 227/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7624 - val_loss: 0.7678\n",
            "Epoch 228/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7624 - val_loss: 0.7677\n",
            "Epoch 229/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 0.7620 - val_loss: 0.7677\n",
            "Epoch 230/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7622 - val_loss: 0.7679\n",
            "Epoch 231/500\n",
            "990/990 [==============================] - 1s 734us/step - loss: 0.7622 - val_loss: 0.7676\n",
            "Epoch 232/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7620 - val_loss: 0.7675\n",
            "Epoch 233/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7623 - val_loss: 0.7676\n",
            "Epoch 234/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 0.7621 - val_loss: 0.7673\n",
            "Epoch 235/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7620 - val_loss: 0.7674\n",
            "Epoch 236/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.7621 - val_loss: 0.7680\n",
            "Epoch 237/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7629 - val_loss: 0.7677\n",
            "Epoch 238/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7625 - val_loss: 0.7679\n",
            "Epoch 239/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.7623 - val_loss: 0.7680\n",
            "Epoch 240/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7620 - val_loss: 0.7675\n",
            "Epoch 241/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7620 - val_loss: 0.7676\n",
            "Epoch 242/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 0.7622 - val_loss: 0.7678\n",
            "Epoch 243/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7629 - val_loss: 0.7678\n",
            "Epoch 244/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.7625 - val_loss: 0.7675\n",
            "Epoch 245/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.7627 - val_loss: 0.7678\n",
            "Epoch 246/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7622 - val_loss: 0.7674\n",
            "Epoch 247/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7622 - val_loss: 0.7673\n",
            "Epoch 248/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.7623 - val_loss: 0.7675\n",
            "Epoch 249/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7621 - val_loss: 0.7674\n",
            "Epoch 250/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7621 - val_loss: 0.7677\n",
            "Epoch 251/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.7622 - val_loss: 0.7673\n",
            "Epoch 252/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7620 - val_loss: 0.7674\n",
            "Epoch 253/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7620 - val_loss: 0.7675\n",
            "Epoch 254/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7619 - val_loss: 0.7674\n",
            "Epoch 255/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7622 - val_loss: 0.7675\n",
            "Epoch 256/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7621 - val_loss: 0.7675\n",
            "Epoch 257/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7621 - val_loss: 0.7676\n",
            "Epoch 258/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7618 - val_loss: 0.7674\n",
            "Epoch 259/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7622 - val_loss: 0.7680\n",
            "Epoch 260/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7623 - val_loss: 0.7677\n",
            "Epoch 261/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.7621 - val_loss: 0.7682\n",
            "Epoch 262/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7621 - val_loss: 0.7674\n",
            "Epoch 263/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7620 - val_loss: 0.7675\n",
            "Epoch 264/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.7618 - val_loss: 0.7675\n",
            "Epoch 265/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 0.7620 - val_loss: 0.7678\n",
            "Epoch 266/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7618 - val_loss: 0.7674\n",
            "Epoch 267/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.7618 - val_loss: 0.7672\n",
            "Epoch 268/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7619 - val_loss: 0.7674\n",
            "Epoch 269/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7620 - val_loss: 0.7672\n",
            "Epoch 270/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7616 - val_loss: 0.7671\n",
            "Epoch 271/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7620 - val_loss: 0.7672\n",
            "Epoch 272/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.7615 - val_loss: 0.7672\n",
            "Epoch 273/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7616 - val_loss: 0.7673\n",
            "Epoch 274/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7616 - val_loss: 0.7672\n",
            "Epoch 275/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 0.7625 - val_loss: 0.7676\n",
            "Epoch 276/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7618 - val_loss: 0.7671\n",
            "Epoch 277/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7619 - val_loss: 0.7679\n",
            "Epoch 278/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7623 - val_loss: 0.7675\n",
            "Epoch 279/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 0.7615 - val_loss: 0.7673\n",
            "Epoch 280/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7619 - val_loss: 0.7678\n",
            "Epoch 281/500\n",
            "990/990 [==============================] - 1s 723us/step - loss: 0.7619 - val_loss: 0.7676\n",
            "Epoch 282/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.7620 - val_loss: 0.7674\n",
            "Epoch 283/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7617 - val_loss: 0.7670\n",
            "Epoch 284/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7617 - val_loss: 0.7673\n",
            "Epoch 285/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 0.7619 - val_loss: 0.7673\n",
            "Epoch 286/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7620 - val_loss: 0.7674\n",
            "Epoch 287/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7619 - val_loss: 0.7673\n",
            "Epoch 288/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7615 - val_loss: 0.7674\n",
            "Epoch 289/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.7620 - val_loss: 0.7672\n",
            "Epoch 290/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7617 - val_loss: 0.7671\n",
            "Epoch 291/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7616 - val_loss: 0.7673\n",
            "Epoch 292/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7620 - val_loss: 0.7673\n",
            "Epoch 293/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.7617 - val_loss: 0.7675\n",
            "Epoch 294/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7616 - val_loss: 0.7679\n",
            "Epoch 295/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 0.7624 - val_loss: 0.7670\n",
            "Epoch 296/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7617 - val_loss: 0.7673\n",
            "Epoch 297/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7625 - val_loss: 0.7671\n",
            "Epoch 298/500\n",
            "990/990 [==============================] - 1s 733us/step - loss: 0.7625 - val_loss: 0.7677\n",
            "Epoch 299/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7617 - val_loss: 0.7671\n",
            "Epoch 300/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7620 - val_loss: 0.7673\n",
            "Epoch 301/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7618 - val_loss: 0.7671\n",
            "Epoch 302/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7616 - val_loss: 0.7669\n",
            "Epoch 303/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.7617 - val_loss: 0.7669\n",
            "Epoch 304/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7616 - val_loss: 0.7673\n",
            "Epoch 305/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7617 - val_loss: 0.7670\n",
            "Epoch 306/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7615 - val_loss: 0.7671\n",
            "Epoch 307/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7616 - val_loss: 0.7670\n",
            "Epoch 308/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7618 - val_loss: 0.7672\n",
            "Epoch 309/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7617 - val_loss: 0.7672\n",
            "Epoch 310/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.7614 - val_loss: 0.7672\n",
            "Epoch 311/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.7617 - val_loss: 0.7670\n",
            "Epoch 312/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7615 - val_loss: 0.7672\n",
            "Epoch 313/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7615 - val_loss: 0.7671\n",
            "Epoch 314/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7616 - val_loss: 0.7673\n",
            "Epoch 315/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7617 - val_loss: 0.7670\n",
            "Epoch 316/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7618 - val_loss: 0.7674\n",
            "Epoch 317/500\n",
            "990/990 [==============================] - 1s 734us/step - loss: 0.7622 - val_loss: 0.7676\n",
            "Epoch 318/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.7615 - val_loss: 0.7672\n",
            "Epoch 319/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7613 - val_loss: 0.7668\n",
            "Epoch 320/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 0.7615 - val_loss: 0.7671\n",
            "Epoch 321/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7613 - val_loss: 0.7670\n",
            "Epoch 322/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7612 - val_loss: 0.7669\n",
            "Epoch 323/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.7617 - val_loss: 0.7668\n",
            "Epoch 324/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7614 - val_loss: 0.7669\n",
            "Epoch 325/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7614 - val_loss: 0.7668\n",
            "Epoch 326/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.7616 - val_loss: 0.7670\n",
            "Epoch 327/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7618 - val_loss: 0.7669\n",
            "Epoch 328/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7617 - val_loss: 0.7670\n",
            "Epoch 329/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.7617 - val_loss: 0.7669\n",
            "Epoch 330/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7617 - val_loss: 0.7669\n",
            "Epoch 331/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7613 - val_loss: 0.7669\n",
            "Epoch 332/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.7615 - val_loss: 0.7672\n",
            "Epoch 333/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7617 - val_loss: 0.7668\n",
            "Epoch 334/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7615 - val_loss: 0.7673\n",
            "Epoch 335/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7617 - val_loss: 0.7672\n",
            "Epoch 336/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.7621 - val_loss: 0.7671\n",
            "Epoch 337/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7614 - val_loss: 0.7670\n",
            "Epoch 338/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7615 - val_loss: 0.7670\n",
            "Epoch 339/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 0.7613 - val_loss: 0.7670\n",
            "Epoch 340/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.7616 - val_loss: 0.7671\n",
            "Epoch 341/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7614 - val_loss: 0.7672\n",
            "Epoch 342/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.7615 - val_loss: 0.7670\n",
            "Epoch 343/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.7619 - val_loss: 0.7671\n",
            "Epoch 344/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7617 - val_loss: 0.7670\n",
            "Epoch 345/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7616 - val_loss: 0.7673\n",
            "Epoch 346/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7616 - val_loss: 0.7667\n",
            "Epoch 347/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7611 - val_loss: 0.7668\n",
            "Epoch 348/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 0.7612 - val_loss: 0.7668\n",
            "Epoch 349/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7616 - val_loss: 0.7672\n",
            "Epoch 350/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 0.7616 - val_loss: 0.7673\n",
            "Epoch 351/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7614 - val_loss: 0.7673\n",
            "Epoch 352/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.7616 - val_loss: 0.7672\n",
            "Epoch 353/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7611 - val_loss: 0.7669\n",
            "Epoch 354/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7618 - val_loss: 0.7678\n",
            "Epoch 355/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7614 - val_loss: 0.7673\n",
            "Epoch 356/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.7613 - val_loss: 0.7668\n",
            "Epoch 357/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7615 - val_loss: 0.7668\n",
            "Epoch 358/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7614 - val_loss: 0.7669\n",
            "Epoch 359/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7611 - val_loss: 0.7670\n",
            "Epoch 360/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7611 - val_loss: 0.7668\n",
            "Epoch 361/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 0.7612 - val_loss: 0.7670\n",
            "Epoch 362/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7610 - val_loss: 0.7668\n",
            "Epoch 363/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.7611 - val_loss: 0.7672\n",
            "Epoch 364/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7612 - val_loss: 0.7669\n",
            "Epoch 365/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7613 - val_loss: 0.7672\n",
            "Epoch 366/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7611 - val_loss: 0.7667\n",
            "Epoch 367/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7614 - val_loss: 0.7669\n",
            "Epoch 368/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7612 - val_loss: 0.7671\n",
            "Epoch 369/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7615 - val_loss: 0.7672\n",
            "Epoch 370/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.7610 - val_loss: 0.7670\n",
            "Epoch 371/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7612 - val_loss: 0.7670\n",
            "Epoch 372/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.7610 - val_loss: 0.7669\n",
            "Epoch 373/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7614 - val_loss: 0.7669\n",
            "Epoch 374/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7618 - val_loss: 0.7670\n",
            "Epoch 375/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.7614 - val_loss: 0.7671\n",
            "Epoch 376/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7615 - val_loss: 0.7670\n",
            "Epoch 377/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.7611 - val_loss: 0.7672\n",
            "Epoch 378/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.7613 - val_loss: 0.7671\n",
            "Epoch 379/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.7613 - val_loss: 0.7674\n",
            "Epoch 380/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7613 - val_loss: 0.7672\n",
            "Epoch 381/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7612 - val_loss: 0.7668\n",
            "Epoch 382/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7614 - val_loss: 0.7669\n",
            "Epoch 383/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.7610 - val_loss: 0.7671\n",
            "Epoch 384/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7613 - val_loss: 0.7669\n",
            "Epoch 385/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.7615 - val_loss: 0.7671\n",
            "Epoch 386/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7614 - val_loss: 0.7671\n",
            "Epoch 387/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7609 - val_loss: 0.7669\n",
            "Epoch 388/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7610 - val_loss: 0.7668\n",
            "Epoch 389/500\n",
            "990/990 [==============================] - 1s 734us/step - loss: 0.7610 - val_loss: 0.7667\n",
            "Epoch 390/500\n",
            "990/990 [==============================] - 1s 730us/step - loss: 0.7608 - val_loss: 0.7667\n",
            "Epoch 391/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7610 - val_loss: 0.7668\n",
            "Epoch 392/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7611 - val_loss: 0.7670\n",
            "Epoch 393/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.7611 - val_loss: 0.7666\n",
            "Epoch 394/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7610 - val_loss: 0.7666\n",
            "Epoch 395/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 0.7613 - val_loss: 0.7666\n",
            "Epoch 396/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 0.7610 - val_loss: 0.7666\n",
            "Epoch 397/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 0.7612 - val_loss: 0.7669\n",
            "Epoch 398/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7612 - val_loss: 0.7669\n",
            "Epoch 399/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7607 - val_loss: 0.7668\n",
            "Epoch 400/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 0.7609 - val_loss: 0.7671\n",
            "Epoch 401/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7610 - val_loss: 0.7669\n",
            "Epoch 402/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7613 - val_loss: 0.7667\n",
            "Epoch 403/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7609 - val_loss: 0.7669\n",
            "Epoch 404/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7610 - val_loss: 0.7669\n",
            "Epoch 405/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.7611 - val_loss: 0.7668\n",
            "Epoch 406/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7611 - val_loss: 0.7670\n",
            "Epoch 407/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 0.7609 - val_loss: 0.7667\n",
            "Epoch 408/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7607 - val_loss: 0.7667\n",
            "Epoch 409/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.7610 - val_loss: 0.7672\n",
            "Epoch 410/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.7610 - val_loss: 0.7668\n",
            "Epoch 411/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7608 - val_loss: 0.7669\n",
            "Epoch 412/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.7612 - val_loss: 0.7667\n",
            "Epoch 413/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.7607 - val_loss: 0.7666\n",
            "Epoch 414/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7612 - val_loss: 0.7665\n",
            "Epoch 415/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7611 - val_loss: 0.7671\n",
            "Epoch 416/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.7613 - val_loss: 0.7669\n",
            "Epoch 417/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.7611 - val_loss: 0.7667\n",
            "Epoch 418/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7611 - val_loss: 0.7669\n",
            "Epoch 419/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7610 - val_loss: 0.7669\n",
            "Epoch 420/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.7614 - val_loss: 0.7667\n",
            "Epoch 421/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7612 - val_loss: 0.7672\n",
            "Epoch 422/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.7613 - val_loss: 0.7678\n",
            "Epoch 423/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7615 - val_loss: 0.7670\n",
            "Epoch 424/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7616 - val_loss: 0.7669\n",
            "Epoch 425/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.7612 - val_loss: 0.7667\n",
            "Epoch 426/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.7612 - val_loss: 0.7670\n",
            "Epoch 427/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7613 - val_loss: 0.7667\n",
            "Epoch 428/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7610 - val_loss: 0.7666\n",
            "Epoch 429/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.7609 - val_loss: 0.7668\n",
            "Epoch 430/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7612 - val_loss: 0.7672\n",
            "Epoch 431/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 0.7609 - val_loss: 0.7666\n",
            "Epoch 432/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7607 - val_loss: 0.7665\n",
            "Epoch 433/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7609 - val_loss: 0.7667\n",
            "Epoch 434/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7608 - val_loss: 0.7668\n",
            "Epoch 435/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7610 - val_loss: 0.7674\n",
            "Epoch 436/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7610 - val_loss: 0.7686\n",
            "Epoch 437/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7609 - val_loss: 0.7668\n",
            "Epoch 438/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.7607 - val_loss: 0.7668\n",
            "Epoch 439/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 0.7611 - val_loss: 0.7669\n",
            "Epoch 440/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 0.7612 - val_loss: 0.7669\n",
            "Epoch 441/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7610 - val_loss: 0.7668\n",
            "Epoch 442/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 0.7606 - val_loss: 0.7665\n",
            "Epoch 443/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 0.7611 - val_loss: 0.7666\n",
            "Epoch 444/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 0.7615 - val_loss: 0.7666\n",
            "Epoch 445/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7610 - val_loss: 0.7666\n",
            "Epoch 446/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7608 - val_loss: 0.7668\n",
            "Epoch 447/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.7613 - val_loss: 0.7667\n",
            "Epoch 448/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7608 - val_loss: 0.7665\n",
            "Epoch 449/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7610 - val_loss: 0.7668\n",
            "Epoch 450/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7608 - val_loss: 0.7670\n",
            "Epoch 451/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7606 - val_loss: 0.7667\n",
            "Epoch 452/500\n",
            "990/990 [==============================] - 1s 730us/step - loss: 0.7608 - val_loss: 0.7674\n",
            "Epoch 453/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.7608 - val_loss: 0.7673\n",
            "Epoch 454/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.7611 - val_loss: 0.7670\n",
            "Epoch 455/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7609 - val_loss: 0.7669\n",
            "Epoch 456/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.7608 - val_loss: 0.7665\n",
            "Epoch 457/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.7612 - val_loss: 0.7668\n",
            "Epoch 458/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7612 - val_loss: 0.7665\n",
            "Epoch 459/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7608 - val_loss: 0.7669\n",
            "Epoch 460/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.7606 - val_loss: 0.7671\n",
            "Epoch 461/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7608 - val_loss: 0.7690\n",
            "Epoch 462/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7613 - val_loss: 0.7686\n",
            "Epoch 463/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.7610 - val_loss: 0.7689\n",
            "Epoch 464/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7609 - val_loss: 0.7684\n",
            "Epoch 465/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7610 - val_loss: 0.7687\n",
            "Epoch 466/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7609 - val_loss: 0.7676\n",
            "Epoch 467/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7608 - val_loss: 0.7693\n",
            "Epoch 468/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 0.7608 - val_loss: 0.7681\n",
            "Epoch 469/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7607 - val_loss: 0.7691\n",
            "Epoch 470/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.7608 - val_loss: 0.7695\n",
            "Epoch 471/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7608 - val_loss: 0.7689\n",
            "Epoch 472/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.7607 - val_loss: 0.7690\n",
            "Epoch 473/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.7609 - val_loss: 0.7699\n",
            "Epoch 474/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.7612 - val_loss: 0.7684\n",
            "Epoch 475/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.7613 - val_loss: 0.7694\n",
            "Epoch 476/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 0.7606 - val_loss: 0.7705\n",
            "Epoch 477/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.7610 - val_loss: 0.7698\n",
            "Epoch 478/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.7610 - val_loss: 0.7692\n",
            "Epoch 479/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.7611 - val_loss: 0.7696\n",
            "Epoch 480/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.7609 - val_loss: 0.7688\n",
            "Epoch 481/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.7607 - val_loss: 0.7702\n",
            "Epoch 482/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7608 - val_loss: 0.7698\n",
            "Epoch 483/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 0.7608 - val_loss: 0.7711\n",
            "Epoch 484/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.7612 - val_loss: 0.7711\n",
            "Epoch 485/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.7610 - val_loss: 0.7700\n",
            "Epoch 486/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 0.7608 - val_loss: 0.7702\n",
            "Epoch 487/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.7605 - val_loss: 0.7699\n",
            "Epoch 488/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.7610 - val_loss: 0.7692\n",
            "Epoch 489/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.7609 - val_loss: 0.7693\n",
            "Epoch 490/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.7609 - val_loss: 0.7706\n",
            "Epoch 491/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7609 - val_loss: 0.7703\n",
            "Epoch 492/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.7606 - val_loss: 0.7688\n",
            "Epoch 493/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.7611 - val_loss: 0.7716\n",
            "Epoch 494/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.7607 - val_loss: 0.7690\n",
            "Epoch 495/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.7606 - val_loss: 0.7683\n",
            "Epoch 496/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.7608 - val_loss: 0.7679\n",
            "Epoch 497/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.7611 - val_loss: 0.7694\n",
            "Epoch 498/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.7607 - val_loss: 0.7689\n",
            "Epoch 499/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.7606 - val_loss: 0.7692\n",
            "Epoch 500/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 0.7612 - val_loss: 0.7690\n",
            "(lamda,Threshold) 0.0 0.0\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (1100, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 3379200 0.0\n",
            "The shape of N (1100, 3072)\n",
            "The minimum value of N  -0.90271777\n",
            "The max value of N 0.84282863\n",
            "[INFO:] Xclean  MSE Computed shape (1100, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (1100, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0061614686])\n",
            "[INFO:] The anomaly threshold computed is  0.0061614686\n",
            "[INFO:] The shape of input data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (1100, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [1057, 1081, 1088, 1052, 1083, 1089, 1076, 1080, 1071, 988, 1090, 1063, 1067, 1079, 1069, 1098, 1042, 1095, 1018, 1056, 1050, 1092, 1091, 1054, 997, 512, 1065, 1066, 1075, 1061, 1040, 1060, 991, 1072, 1093, 1087, 1086, 1094, 1073, 359, 1023, 381, 356, 552, 584, 1070, 230, 522, 986, 1025]\n",
            "[INFO:] The  Positive boundry 1050\n",
            "[INFO:] The  Negative boundry 50\n",
            "[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n",
            "img shape: (160, 320, 3)\n",
            "\n",
            "Saving Top-10 most anomalous digit: @\n",
            "=====================\n",
            "AUROC 0.0 0.83\n",
            "=======================\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 32) (256, 32)\n",
            "Train on 990 samples, validate on 110 samples\n",
            "Epoch 1/500\n",
            "990/990 [==============================] - 9s 9ms/step - loss: 0.9971 - val_loss: 1.0063\n",
            "Epoch 2/500\n",
            "990/990 [==============================] - 1s 794us/step - loss: 0.9969 - val_loss: 1.0063\n",
            "Epoch 3/500\n",
            "990/990 [==============================] - 1s 787us/step - loss: 0.9970 - val_loss: 1.0065\n",
            "Epoch 4/500\n",
            "990/990 [==============================] - 1s 785us/step - loss: 0.9971 - val_loss: 1.0072\n",
            "Epoch 5/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.9969 - val_loss: 1.0065\n",
            "Epoch 6/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 0.9972 - val_loss: 1.0057\n",
            "Epoch 7/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 0.9969 - val_loss: 1.0056\n",
            "Epoch 8/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9969 - val_loss: 1.0053\n",
            "Epoch 9/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 0.9968 - val_loss: 1.0070\n",
            "Epoch 10/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9970 - val_loss: 1.0052\n",
            "Epoch 11/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.9971 - val_loss: 1.0058\n",
            "Epoch 12/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 0.9970 - val_loss: 1.0063\n",
            "Epoch 13/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9967 - val_loss: 1.0066\n",
            "Epoch 14/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 0.9968 - val_loss: 1.0054\n",
            "Epoch 15/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9972 - val_loss: 1.0080\n",
            "Epoch 16/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9968 - val_loss: 1.0064\n",
            "Epoch 17/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 0.9968 - val_loss: 1.0063\n",
            "Epoch 18/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 0.9971 - val_loss: 1.0071\n",
            "Epoch 19/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.9971 - val_loss: 1.0081\n",
            "Epoch 20/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9969 - val_loss: 1.0074\n",
            "Epoch 21/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 0.9968 - val_loss: 1.0069\n",
            "Epoch 22/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 0.9965 - val_loss: 1.0073\n",
            "Epoch 23/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 0.9971 - val_loss: 1.0060\n",
            "Epoch 24/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 0.9972 - val_loss: 1.0071\n",
            "Epoch 25/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9972 - val_loss: 1.0049\n",
            "Epoch 26/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.9968 - val_loss: 1.0060\n",
            "Epoch 27/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 0.9967 - val_loss: 1.0073\n",
            "Epoch 28/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 0.9968 - val_loss: 1.0047\n",
            "Epoch 29/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 0.9967 - val_loss: 1.0055\n",
            "Epoch 30/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 0.9966 - val_loss: 1.0051\n",
            "Epoch 31/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.9964 - val_loss: 1.0051\n",
            "Epoch 32/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 0.9971 - val_loss: 1.0055\n",
            "Epoch 33/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.9968 - val_loss: 1.0064\n",
            "Epoch 34/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9969 - val_loss: 1.0053\n",
            "Epoch 35/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.9967 - val_loss: 1.0059\n",
            "Epoch 36/500\n",
            "990/990 [==============================] - 1s 784us/step - loss: 0.9967 - val_loss: 1.0025\n",
            "Epoch 37/500\n",
            "990/990 [==============================] - 1s 791us/step - loss: 0.9971 - val_loss: 1.0026\n",
            "Epoch 38/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 0.9967 - val_loss: 1.0026\n",
            "Epoch 39/500\n",
            "990/990 [==============================] - 1s 782us/step - loss: 0.9967 - val_loss: 1.0029\n",
            "Epoch 40/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 0.9966 - val_loss: 1.0028\n",
            "Epoch 41/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9966 - val_loss: 1.0026\n",
            "Epoch 42/500\n",
            "990/990 [==============================] - 1s 788us/step - loss: 0.9967 - val_loss: 1.0026\n",
            "Epoch 43/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 0.9966 - val_loss: 1.0025\n",
            "Epoch 44/500\n",
            "990/990 [==============================] - 1s 781us/step - loss: 0.9966 - val_loss: 1.0028\n",
            "Epoch 45/500\n",
            "990/990 [==============================] - 1s 787us/step - loss: 0.9968 - val_loss: 1.0031\n",
            "Epoch 46/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 0.9969 - val_loss: 1.0024\n",
            "Epoch 47/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 0.9969 - val_loss: 1.0028\n",
            "Epoch 48/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.9967 - val_loss: 1.0027\n",
            "Epoch 49/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 0.9966 - val_loss: 1.0028\n",
            "Epoch 50/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 0.9969 - val_loss: 1.0027\n",
            "Epoch 51/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.9969 - val_loss: 1.0026\n",
            "Epoch 52/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 0.9967 - val_loss: 1.0025\n",
            "Epoch 53/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 0.9971 - val_loss: 1.0027\n",
            "Epoch 54/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9969 - val_loss: 1.0025\n",
            "Epoch 55/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 0.9964 - val_loss: 1.0028\n",
            "Epoch 56/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 0.9965 - val_loss: 1.0025\n",
            "Epoch 57/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 0.9969 - val_loss: 1.0030\n",
            "Epoch 58/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 0.9967 - val_loss: 1.0027\n",
            "Epoch 59/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.9966 - val_loss: 1.0025\n",
            "Epoch 60/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 0.9967 - val_loss: 1.0025\n",
            "Epoch 61/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 0.9965 - val_loss: 1.0028\n",
            "Epoch 62/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 0.9967 - val_loss: 1.0027\n",
            "Epoch 63/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 0.9965 - val_loss: 1.0025\n",
            "Epoch 64/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.9964 - val_loss: 1.0028\n",
            "Epoch 65/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 0.9967 - val_loss: 1.0026\n",
            "Epoch 66/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 0.9971 - val_loss: 1.0027\n",
            "Epoch 67/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 0.9965 - val_loss: 1.0026\n",
            "Epoch 68/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 0.9966 - val_loss: 1.0025\n",
            "Epoch 69/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 0.9967 - val_loss: 1.0026\n",
            "Epoch 70/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.9966 - val_loss: 1.0029\n",
            "Epoch 71/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.9970 - val_loss: 1.0025\n",
            "Epoch 72/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9967 - val_loss: 1.0026\n",
            "Epoch 73/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9967 - val_loss: 1.0030\n",
            "Epoch 74/500\n",
            "990/990 [==============================] - 1s 734us/step - loss: 0.9965 - val_loss: 1.0033\n",
            "Epoch 75/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.9967 - val_loss: 1.0027\n",
            "Epoch 76/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9966 - val_loss: 1.0027\n",
            "Epoch 77/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9964 - val_loss: 1.0029\n",
            "Epoch 78/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.9965 - val_loss: 1.0027\n",
            "Epoch 79/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.9968 - val_loss: 1.0031\n",
            "Epoch 80/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.9965 - val_loss: 1.0028\n",
            "Epoch 81/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 0.9966 - val_loss: 1.0027\n",
            "Epoch 82/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9963 - val_loss: 1.0028\n",
            "Epoch 83/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 0.9966 - val_loss: 1.0029\n",
            "Epoch 84/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9964 - val_loss: 1.0028\n",
            "Epoch 85/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.9964 - val_loss: 1.0028\n",
            "Epoch 86/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9968 - val_loss: 1.0047\n",
            "Epoch 87/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9967 - val_loss: 1.0026\n",
            "Epoch 88/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9967 - val_loss: 1.0031\n",
            "Epoch 89/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9966 - val_loss: 1.0029\n",
            "Epoch 90/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9964 - val_loss: 1.0029\n",
            "Epoch 91/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.9965 - val_loss: 1.0029\n",
            "Epoch 92/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9968 - val_loss: 1.0031\n",
            "Epoch 93/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9966 - val_loss: 1.0030\n",
            "Epoch 94/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9965 - val_loss: 1.0026\n",
            "Epoch 95/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9966 - val_loss: 1.0027\n",
            "Epoch 96/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 0.9964 - val_loss: 1.0026\n",
            "Epoch 97/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9968 - val_loss: 1.0027\n",
            "Epoch 98/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.9964 - val_loss: 1.0025\n",
            "Epoch 99/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 0.9965 - val_loss: 1.0028\n",
            "Epoch 100/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9965 - val_loss: 1.0035\n",
            "Epoch 101/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9967 - val_loss: 1.0027\n",
            "Epoch 102/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 0.9966 - val_loss: 1.0029\n",
            "Epoch 103/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.9965 - val_loss: 1.0028\n",
            "Epoch 104/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.9965 - val_loss: 1.0029\n",
            "Epoch 105/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9967 - val_loss: 1.0026\n",
            "Epoch 106/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.9966 - val_loss: 1.0026\n",
            "Epoch 107/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9970 - val_loss: 1.0032\n",
            "Epoch 108/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9970 - val_loss: 1.0025\n",
            "Epoch 109/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9966 - val_loss: 1.0028\n",
            "Epoch 110/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9967 - val_loss: 1.0033\n",
            "Epoch 111/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9965 - val_loss: 1.0027\n",
            "Epoch 112/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9968 - val_loss: 1.0028\n",
            "Epoch 113/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9965 - val_loss: 1.0028\n",
            "Epoch 114/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9962 - val_loss: 1.0029\n",
            "Epoch 115/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9962 - val_loss: 1.0022\n",
            "Epoch 116/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9961 - val_loss: 1.0032\n",
            "Epoch 117/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9963 - val_loss: 1.0029\n",
            "Epoch 118/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9968 - val_loss: 1.0028\n",
            "Epoch 119/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9966 - val_loss: 1.0026\n",
            "Epoch 120/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9966 - val_loss: 1.0025\n",
            "Epoch 121/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.9965 - val_loss: 1.0025\n",
            "Epoch 122/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9962 - val_loss: 1.0025\n",
            "Epoch 123/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.9966 - val_loss: 1.0026\n",
            "Epoch 124/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.9965 - val_loss: 1.0025\n",
            "Epoch 125/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.9965 - val_loss: 1.0026\n",
            "Epoch 126/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9963 - val_loss: 1.0026\n",
            "Epoch 127/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9965 - val_loss: 1.0024\n",
            "Epoch 128/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.9964 - val_loss: 1.0024\n",
            "Epoch 129/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9967 - val_loss: 1.0028\n",
            "Epoch 130/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9963 - val_loss: 1.0025\n",
            "Epoch 131/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9967 - val_loss: 1.0027\n",
            "Epoch 132/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9965 - val_loss: 1.0030\n",
            "Epoch 133/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9964 - val_loss: 1.0029\n",
            "Epoch 134/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9964 - val_loss: 1.0026\n",
            "Epoch 135/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.9965 - val_loss: 1.0028\n",
            "Epoch 136/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9968 - val_loss: 1.0026\n",
            "Epoch 137/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.9965 - val_loss: 1.0029\n",
            "Epoch 138/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9965 - val_loss: 1.0030\n",
            "Epoch 139/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.9964 - val_loss: 1.0029\n",
            "Epoch 140/500\n",
            "990/990 [==============================] - 1s 779us/step - loss: 0.9964 - val_loss: 1.0030\n",
            "Epoch 141/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9966 - val_loss: 1.0027\n",
            "Epoch 142/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.9970 - val_loss: 1.0033\n",
            "Epoch 143/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9969 - val_loss: 1.0027\n",
            "Epoch 144/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9966 - val_loss: 1.0033\n",
            "Epoch 145/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9963 - val_loss: 1.0026\n",
            "Epoch 146/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.9962 - val_loss: 1.0026\n",
            "Epoch 147/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.9966 - val_loss: 1.0028\n",
            "Epoch 148/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 0.9967 - val_loss: 1.0029\n",
            "Epoch 149/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.9968 - val_loss: 1.0026\n",
            "Epoch 150/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.9968 - val_loss: 1.0026\n",
            "Epoch 151/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.9964 - val_loss: 1.0031\n",
            "Epoch 152/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9963 - val_loss: 1.0030\n",
            "Epoch 153/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9967 - val_loss: 1.0031\n",
            "Epoch 154/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9965 - val_loss: 1.0027\n",
            "Epoch 155/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 0.9965 - val_loss: 1.0028\n",
            "Epoch 156/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9964 - val_loss: 1.0026\n",
            "Epoch 157/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.9963 - val_loss: 1.0027\n",
            "Epoch 158/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9964 - val_loss: 1.0025\n",
            "Epoch 159/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9963 - val_loss: 1.0027\n",
            "Epoch 160/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 0.9965 - val_loss: 1.0024\n",
            "Epoch 161/500\n",
            "990/990 [==============================] - 1s 728us/step - loss: 0.9963 - val_loss: 1.0025\n",
            "Epoch 162/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 0.9963 - val_loss: 1.0025\n",
            "Epoch 163/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9967 - val_loss: 1.0026\n",
            "Epoch 164/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9963 - val_loss: 1.0028\n",
            "Epoch 165/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9962 - val_loss: 1.0030\n",
            "Epoch 166/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9967 - val_loss: 1.0028\n",
            "Epoch 167/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9964 - val_loss: 1.0027\n",
            "Epoch 168/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9963 - val_loss: 1.0028\n",
            "Epoch 169/500\n",
            "990/990 [==============================] - 1s 728us/step - loss: 0.9962 - val_loss: 1.0029\n",
            "Epoch 170/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.9962 - val_loss: 1.0027\n",
            "Epoch 171/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 0.9965 - val_loss: 1.0032\n",
            "Epoch 172/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 0.9963 - val_loss: 1.0032\n",
            "Epoch 173/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9962 - val_loss: 1.0031\n",
            "Epoch 174/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.9963 - val_loss: 1.0028\n",
            "Epoch 175/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9964 - val_loss: 1.0033\n",
            "Epoch 176/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 0.9965 - val_loss: 1.0028\n",
            "Epoch 177/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 0.9962 - val_loss: 1.0028\n",
            "Epoch 178/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.9961 - val_loss: 1.0026\n",
            "Epoch 179/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.9961 - val_loss: 1.0031\n",
            "Epoch 180/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9965 - val_loss: 1.0030\n",
            "Epoch 181/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9964 - val_loss: 1.0028\n",
            "Epoch 182/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 0.9963 - val_loss: 1.0030\n",
            "Epoch 183/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9968 - val_loss: 1.0035\n",
            "Epoch 184/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.9966 - val_loss: 1.0031\n",
            "Epoch 185/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9964 - val_loss: 1.0032\n",
            "Epoch 186/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9964 - val_loss: 1.0030\n",
            "Epoch 187/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9962 - val_loss: 1.0029\n",
            "Epoch 188/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9965 - val_loss: 1.0028\n",
            "Epoch 189/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 0.9966 - val_loss: 1.0032\n",
            "Epoch 190/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9967 - val_loss: 1.0035\n",
            "Epoch 191/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 0.9961 - val_loss: 1.0034\n",
            "Epoch 192/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9965 - val_loss: 1.0029\n",
            "Epoch 193/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9963 - val_loss: 1.0026\n",
            "Epoch 194/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.9965 - val_loss: 1.0030\n",
            "Epoch 195/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.9965 - val_loss: 1.0031\n",
            "Epoch 196/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.9964 - val_loss: 1.0030\n",
            "Epoch 197/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.9960 - val_loss: 1.0028\n",
            "Epoch 198/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 0.9964 - val_loss: 1.0032\n",
            "Epoch 199/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9965 - val_loss: 1.0029\n",
            "Epoch 200/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.9962 - val_loss: 1.0032\n",
            "Epoch 201/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.9962 - val_loss: 1.0025\n",
            "Epoch 202/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9962 - val_loss: 1.0028\n",
            "Epoch 203/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9963 - val_loss: 1.0030\n",
            "Epoch 204/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 0.9965 - val_loss: 1.0025\n",
            "Epoch 205/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.9964 - val_loss: 1.0028\n",
            "Epoch 206/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.9963 - val_loss: 1.0032\n",
            "Epoch 207/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9963 - val_loss: 1.0024\n",
            "Epoch 208/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9965 - val_loss: 1.0032\n",
            "Epoch 209/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9961 - val_loss: 1.0031\n",
            "Epoch 210/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.9963 - val_loss: 1.0029\n",
            "Epoch 211/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9963 - val_loss: 1.0036\n",
            "Epoch 212/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9963 - val_loss: 1.0031\n",
            "Epoch 213/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.9964 - val_loss: 1.0029\n",
            "Epoch 214/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9964 - val_loss: 1.0031\n",
            "Epoch 215/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9963 - val_loss: 1.0029\n",
            "Epoch 216/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.9964 - val_loss: 1.0029\n",
            "Epoch 217/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.9962 - val_loss: 1.0027\n",
            "Epoch 218/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.9963 - val_loss: 1.0032\n",
            "Epoch 219/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9963 - val_loss: 1.0030\n",
            "Epoch 220/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9962 - val_loss: 1.0031\n",
            "Epoch 221/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9962 - val_loss: 1.0030\n",
            "Epoch 222/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9965 - val_loss: 1.0028\n",
            "Epoch 223/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.9965 - val_loss: 1.0029\n",
            "Epoch 224/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.9965 - val_loss: 1.0035\n",
            "Epoch 225/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9966 - val_loss: 1.0034\n",
            "Epoch 226/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.9966 - val_loss: 1.0031\n",
            "Epoch 227/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9965 - val_loss: 1.0028\n",
            "Epoch 228/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.9964 - val_loss: 1.0031\n",
            "Epoch 229/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9961 - val_loss: 1.0032\n",
            "Epoch 230/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9963 - val_loss: 1.0032\n",
            "Epoch 231/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.9963 - val_loss: 1.0030\n",
            "Epoch 232/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9962 - val_loss: 1.0033\n",
            "Epoch 233/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9959 - val_loss: 1.0032\n",
            "Epoch 234/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9961 - val_loss: 1.0033\n",
            "Epoch 235/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.9963 - val_loss: 1.0028\n",
            "Epoch 236/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9963 - val_loss: 1.0036\n",
            "Epoch 237/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9963 - val_loss: 1.0031\n",
            "Epoch 238/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.9962 - val_loss: 1.0034\n",
            "Epoch 239/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.9966 - val_loss: 1.0030\n",
            "Epoch 240/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9964 - val_loss: 1.0033\n",
            "Epoch 241/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9965 - val_loss: 1.0034\n",
            "Epoch 242/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.9964 - val_loss: 1.0038\n",
            "Epoch 243/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.9962 - val_loss: 1.0031\n",
            "Epoch 244/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9964 - val_loss: 1.0031\n",
            "Epoch 245/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9964 - val_loss: 1.0034\n",
            "Epoch 246/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.9963 - val_loss: 1.0032\n",
            "Epoch 247/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9964 - val_loss: 1.0032\n",
            "Epoch 248/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9962 - val_loss: 1.0035\n",
            "Epoch 249/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.9961 - val_loss: 1.0034\n",
            "Epoch 250/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9963 - val_loss: 1.0031\n",
            "Epoch 251/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.9965 - val_loss: 1.0035\n",
            "Epoch 252/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.9963 - val_loss: 1.0036\n",
            "Epoch 253/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.9963 - val_loss: 1.0037\n",
            "Epoch 254/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 0.9961 - val_loss: 1.0032\n",
            "Epoch 255/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9962 - val_loss: 1.0033\n",
            "Epoch 256/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9963 - val_loss: 1.0040\n",
            "Epoch 257/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9963 - val_loss: 1.0033\n",
            "Epoch 258/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 0.9962 - val_loss: 1.0032\n",
            "Epoch 259/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9961 - val_loss: 1.0031\n",
            "Epoch 260/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9963 - val_loss: 1.0028\n",
            "Epoch 261/500\n",
            "990/990 [==============================] - 1s 730us/step - loss: 0.9964 - val_loss: 1.0030\n",
            "Epoch 262/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.9962 - val_loss: 1.0026\n",
            "Epoch 263/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.9961 - val_loss: 1.0028\n",
            "Epoch 264/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.9960 - val_loss: 1.0029\n",
            "Epoch 265/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.9959 - val_loss: 1.0028\n",
            "Epoch 266/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9962 - val_loss: 1.0033\n",
            "Epoch 267/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9961 - val_loss: 1.0031\n",
            "Epoch 268/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 0.9962 - val_loss: 1.0032\n",
            "Epoch 269/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9961 - val_loss: 1.0033\n",
            "Epoch 270/500\n",
            "990/990 [==============================] - 1s 728us/step - loss: 0.9963 - val_loss: 1.0032\n",
            "Epoch 271/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9964 - val_loss: 1.0035\n",
            "Epoch 272/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9963 - val_loss: 1.0032\n",
            "Epoch 273/500\n",
            "990/990 [==============================] - 1s 733us/step - loss: 0.9962 - val_loss: 1.0035\n",
            "Epoch 274/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.9963 - val_loss: 1.0033\n",
            "Epoch 275/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.9964 - val_loss: 1.0038\n",
            "Epoch 276/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9964 - val_loss: 1.0035\n",
            "Epoch 277/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9962 - val_loss: 1.0030\n",
            "Epoch 278/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.9965 - val_loss: 1.0027\n",
            "Epoch 279/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9965 - val_loss: 1.0039\n",
            "Epoch 280/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.9962 - val_loss: 1.0031\n",
            "Epoch 281/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 0.9963 - val_loss: 1.0029\n",
            "Epoch 282/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9961 - val_loss: 1.0031\n",
            "Epoch 283/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 0.9963 - val_loss: 1.0026\n",
            "Epoch 284/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9961 - val_loss: 1.0028\n",
            "Epoch 285/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 0.9960 - val_loss: 1.0033\n",
            "Epoch 286/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.9963 - val_loss: 1.0029\n",
            "Epoch 287/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.9962 - val_loss: 1.0031\n",
            "Epoch 288/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9962 - val_loss: 1.0032\n",
            "Epoch 289/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.9962 - val_loss: 1.0036\n",
            "Epoch 290/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.9962 - val_loss: 1.0040\n",
            "Epoch 291/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.9961 - val_loss: 1.0032\n",
            "Epoch 292/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 0.9962 - val_loss: 1.0033\n",
            "Epoch 293/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 0.9963 - val_loss: 1.0036\n",
            "Epoch 294/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9961 - val_loss: 1.0026\n",
            "Epoch 295/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.9962 - val_loss: 1.0036\n",
            "Epoch 296/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.9964 - val_loss: 1.0038\n",
            "Epoch 297/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 0.9962 - val_loss: 1.0031\n",
            "Epoch 298/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.9963 - val_loss: 1.0035\n",
            "Epoch 299/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.9963 - val_loss: 1.0031\n",
            "Epoch 300/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9963 - val_loss: 1.0039\n",
            "Epoch 301/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9966 - val_loss: 1.0036\n",
            "Epoch 302/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.9961 - val_loss: 1.0034\n",
            "Epoch 303/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9960 - val_loss: 1.0037\n",
            "Epoch 304/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9963 - val_loss: 1.0040\n",
            "Epoch 305/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.9962 - val_loss: 1.0045\n",
            "Epoch 306/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.9957 - val_loss: 1.0038\n",
            "Epoch 307/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.9960 - val_loss: 1.0036\n",
            "Epoch 308/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.9959 - val_loss: 1.0035\n",
            "Epoch 309/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9961 - val_loss: 1.0039\n",
            "Epoch 310/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9962 - val_loss: 1.0037\n",
            "Epoch 311/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9961 - val_loss: 1.0043\n",
            "Epoch 312/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9963 - val_loss: 1.0037\n",
            "Epoch 313/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 0.9963 - val_loss: 1.0046\n",
            "Epoch 314/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9961 - val_loss: 1.0038\n",
            "Epoch 315/500\n",
            "990/990 [==============================] - 1s 732us/step - loss: 0.9962 - val_loss: 1.0042\n",
            "Epoch 316/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9961 - val_loss: 1.0033\n",
            "Epoch 317/500\n",
            "990/990 [==============================] - 1s 732us/step - loss: 0.9958 - val_loss: 1.0031\n",
            "Epoch 318/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.9961 - val_loss: 1.0030\n",
            "Epoch 319/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.9962 - val_loss: 1.0029\n",
            "Epoch 320/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.9961 - val_loss: 1.0040\n",
            "Epoch 321/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9964 - val_loss: 1.0040\n",
            "Epoch 322/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.9962 - val_loss: 1.0039\n",
            "Epoch 323/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.9960 - val_loss: 1.0046\n",
            "Epoch 324/500\n",
            "990/990 [==============================] - 1s 730us/step - loss: 0.9965 - val_loss: 1.0048\n",
            "Epoch 325/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9961 - val_loss: 1.0036\n",
            "Epoch 326/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9964 - val_loss: 1.0039\n",
            "Epoch 327/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.9960 - val_loss: 1.0037\n",
            "Epoch 328/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.9964 - val_loss: 1.0043\n",
            "Epoch 329/500\n",
            "990/990 [==============================] - 1s 734us/step - loss: 0.9960 - val_loss: 1.0038\n",
            "Epoch 330/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.9964 - val_loss: 1.0036\n",
            "Epoch 331/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9960 - val_loss: 1.0047\n",
            "Epoch 332/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9961 - val_loss: 1.0033\n",
            "Epoch 333/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.9962 - val_loss: 1.0037\n",
            "Epoch 334/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9964 - val_loss: 1.0037\n",
            "Epoch 335/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9961 - val_loss: 1.0036\n",
            "Epoch 336/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.9960 - val_loss: 1.0035\n",
            "Epoch 337/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.9964 - val_loss: 1.0040\n",
            "Epoch 338/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.9960 - val_loss: 1.0040\n",
            "Epoch 339/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.9962 - val_loss: 1.0036\n",
            "Epoch 340/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9962 - val_loss: 1.0042\n",
            "Epoch 341/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.9959 - val_loss: 1.0035\n",
            "Epoch 342/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.9958 - val_loss: 1.0034\n",
            "Epoch 343/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9962 - val_loss: 1.0038\n",
            "Epoch 344/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9962 - val_loss: 1.0037\n",
            "Epoch 345/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.9963 - val_loss: 1.0035\n",
            "Epoch 346/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9961 - val_loss: 1.0036\n",
            "Epoch 347/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.9960 - val_loss: 1.0036\n",
            "Epoch 348/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9959 - val_loss: 1.0034\n",
            "Epoch 349/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.9961 - val_loss: 1.0033\n",
            "Epoch 350/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9961 - val_loss: 1.0045\n",
            "Epoch 351/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 0.9959 - val_loss: 1.0042\n",
            "Epoch 352/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.9959 - val_loss: 1.0039\n",
            "Epoch 353/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9959 - val_loss: 1.0038\n",
            "Epoch 354/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9960 - val_loss: 1.0040\n",
            "Epoch 355/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9960 - val_loss: 1.0039\n",
            "Epoch 356/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9961 - val_loss: 1.0047\n",
            "Epoch 357/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.9960 - val_loss: 1.0039\n",
            "Epoch 358/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9959 - val_loss: 1.0040\n",
            "Epoch 359/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9959 - val_loss: 1.0047\n",
            "Epoch 360/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.9958 - val_loss: 1.0035\n",
            "Epoch 361/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 0.9959 - val_loss: 1.0038\n",
            "Epoch 362/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9960 - val_loss: 1.0038\n",
            "Epoch 363/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9960 - val_loss: 1.0036\n",
            "Epoch 364/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.9961 - val_loss: 1.0029\n",
            "Epoch 365/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9963 - val_loss: 1.0029\n",
            "Epoch 366/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9962 - val_loss: 1.0037\n",
            "Epoch 367/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.9962 - val_loss: 1.0036\n",
            "Epoch 368/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9960 - val_loss: 1.0036\n",
            "Epoch 369/500\n",
            "990/990 [==============================] - 1s 728us/step - loss: 0.9962 - val_loss: 1.0041\n",
            "Epoch 370/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9960 - val_loss: 1.0037\n",
            "Epoch 371/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9962 - val_loss: 1.0037\n",
            "Epoch 372/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.9959 - val_loss: 1.0033\n",
            "Epoch 373/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9960 - val_loss: 1.0038\n",
            "Epoch 374/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 0.9959 - val_loss: 1.0038\n",
            "Epoch 375/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9959 - val_loss: 1.0035\n",
            "Epoch 376/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9960 - val_loss: 1.0036\n",
            "Epoch 377/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.9960 - val_loss: 1.0034\n",
            "Epoch 378/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.9959 - val_loss: 1.0034\n",
            "Epoch 379/500\n",
            "990/990 [==============================] - 1s 732us/step - loss: 0.9958 - val_loss: 1.0033\n",
            "Epoch 380/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9959 - val_loss: 1.0040\n",
            "Epoch 381/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.9960 - val_loss: 1.0037\n",
            "Epoch 382/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9961 - val_loss: 1.0037\n",
            "Epoch 383/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.9959 - val_loss: 1.0042\n",
            "Epoch 384/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.9960 - val_loss: 1.0046\n",
            "Epoch 385/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9962 - val_loss: 1.0038\n",
            "Epoch 386/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9961 - val_loss: 1.0053\n",
            "Epoch 387/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.9962 - val_loss: 1.0046\n",
            "Epoch 388/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9963 - val_loss: 1.0039\n",
            "Epoch 389/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.9960 - val_loss: 1.0038\n",
            "Epoch 390/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9962 - val_loss: 1.0043\n",
            "Epoch 391/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.9961 - val_loss: 1.0032\n",
            "Epoch 392/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9959 - val_loss: 1.0035\n",
            "Epoch 393/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9959 - val_loss: 1.0044\n",
            "Epoch 394/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.9957 - val_loss: 1.0048\n",
            "Epoch 395/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9957 - val_loss: 1.0042\n",
            "Epoch 396/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 0.9961 - val_loss: 1.0037\n",
            "Epoch 397/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9958 - val_loss: 1.0039\n",
            "Epoch 398/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9959 - val_loss: 1.0045\n",
            "Epoch 399/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9959 - val_loss: 1.0037\n",
            "Epoch 400/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9959 - val_loss: 1.0036\n",
            "Epoch 401/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.9962 - val_loss: 1.0034\n",
            "Epoch 402/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9961 - val_loss: 1.0048\n",
            "Epoch 403/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9959 - val_loss: 1.0043\n",
            "Epoch 404/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.9961 - val_loss: 1.0038\n",
            "Epoch 405/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.9962 - val_loss: 1.0044\n",
            "Epoch 406/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.9961 - val_loss: 1.0042\n",
            "Epoch 407/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.9961 - val_loss: 1.0044\n",
            "Epoch 408/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.9961 - val_loss: 1.0051\n",
            "Epoch 409/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 0.9960 - val_loss: 1.0051\n",
            "Epoch 410/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 0.9961 - val_loss: 1.0042\n",
            "Epoch 411/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9961 - val_loss: 1.0039\n",
            "Epoch 412/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.9962 - val_loss: 1.0042\n",
            "Epoch 413/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9961 - val_loss: 1.0040\n",
            "Epoch 414/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9962 - val_loss: 1.0057\n",
            "Epoch 415/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.9963 - val_loss: 1.0048\n",
            "Epoch 416/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9962 - val_loss: 1.0041\n",
            "Epoch 417/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.9958 - val_loss: 1.0044\n",
            "Epoch 418/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.9962 - val_loss: 1.0042\n",
            "Epoch 419/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 0.9960 - val_loss: 1.0039\n",
            "Epoch 420/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.9960 - val_loss: 1.0042\n",
            "Epoch 421/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.9958 - val_loss: 1.0048\n",
            "Epoch 422/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9960 - val_loss: 1.0049\n",
            "Epoch 423/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.9961 - val_loss: 1.0043\n",
            "Epoch 424/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9958 - val_loss: 1.0043\n",
            "Epoch 425/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9959 - val_loss: 1.0051\n",
            "Epoch 426/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 0.9961 - val_loss: 1.0050\n",
            "Epoch 427/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9961 - val_loss: 1.0046\n",
            "Epoch 428/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.9961 - val_loss: 1.0053\n",
            "Epoch 429/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.9959 - val_loss: 1.0057\n",
            "Epoch 430/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.9964 - val_loss: 1.0059\n",
            "Epoch 431/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.9958 - val_loss: 1.0043\n",
            "Epoch 432/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.9961 - val_loss: 1.0040\n",
            "Epoch 433/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.9961 - val_loss: 1.0045\n",
            "Epoch 434/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9959 - val_loss: 1.0043\n",
            "Epoch 435/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9959 - val_loss: 1.0045\n",
            "Epoch 436/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.9961 - val_loss: 1.0045\n",
            "Epoch 437/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 0.9959 - val_loss: 1.0044\n",
            "Epoch 438/500\n",
            "990/990 [==============================] - 1s 732us/step - loss: 0.9958 - val_loss: 1.0043\n",
            "Epoch 439/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9957 - val_loss: 1.0044\n",
            "Epoch 440/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9956 - val_loss: 1.0044\n",
            "Epoch 441/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.9962 - val_loss: 1.0046\n",
            "Epoch 442/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9960 - val_loss: 1.0048\n",
            "Epoch 443/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 0.9956 - val_loss: 1.0042\n",
            "Epoch 444/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.9959 - val_loss: 1.0047\n",
            "Epoch 445/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 0.9958 - val_loss: 1.0040\n",
            "Epoch 446/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.9960 - val_loss: 1.0042\n",
            "Epoch 447/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9958 - val_loss: 1.0041\n",
            "Epoch 448/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 0.9962 - val_loss: 1.0043\n",
            "Epoch 449/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.9962 - val_loss: 1.0048\n",
            "Epoch 450/500\n",
            "990/990 [==============================] - 1s 733us/step - loss: 0.9959 - val_loss: 1.0041\n",
            "Epoch 451/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.9961 - val_loss: 1.0048\n",
            "Epoch 452/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.9962 - val_loss: 1.0045\n",
            "Epoch 453/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 0.9960 - val_loss: 1.0042\n",
            "Epoch 454/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9958 - val_loss: 1.0045\n",
            "Epoch 455/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 0.9965 - val_loss: 1.0040\n",
            "Epoch 456/500\n",
            "990/990 [==============================] - 1s 779us/step - loss: 0.9960 - val_loss: 1.0041\n",
            "Epoch 457/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 0.9958 - val_loss: 1.0036\n",
            "Epoch 458/500\n",
            "990/990 [==============================] - 1s 733us/step - loss: 0.9960 - val_loss: 1.0038\n",
            "Epoch 459/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 0.9958 - val_loss: 1.0037\n",
            "Epoch 460/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.9960 - val_loss: 1.0042\n",
            "Epoch 461/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 0.9961 - val_loss: 1.0042\n",
            "Epoch 462/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9960 - val_loss: 1.0052\n",
            "Epoch 463/500\n",
            "990/990 [==============================] - 1s 730us/step - loss: 0.9957 - val_loss: 1.0047\n",
            "Epoch 464/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 0.9961 - val_loss: 1.0043\n",
            "Epoch 465/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.9959 - val_loss: 1.0041\n",
            "Epoch 466/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 0.9961 - val_loss: 1.0052\n",
            "Epoch 467/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9963 - val_loss: 1.0048\n",
            "Epoch 468/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.9959 - val_loss: 1.0044\n",
            "Epoch 469/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 0.9961 - val_loss: 1.0040\n",
            "Epoch 470/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9958 - val_loss: 1.0055\n",
            "Epoch 471/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9959 - val_loss: 1.0041\n",
            "Epoch 472/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.9959 - val_loss: 1.0051\n",
            "Epoch 473/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9962 - val_loss: 1.0054\n",
            "Epoch 474/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 0.9963 - val_loss: 1.0041\n",
            "Epoch 475/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 0.9958 - val_loss: 1.0067\n",
            "Epoch 476/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9959 - val_loss: 1.0046\n",
            "Epoch 477/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 0.9959 - val_loss: 1.0044\n",
            "Epoch 478/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9958 - val_loss: 1.0044\n",
            "Epoch 479/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 0.9958 - val_loss: 1.0056\n",
            "Epoch 480/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.9960 - val_loss: 1.0043\n",
            "Epoch 481/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 0.9959 - val_loss: 1.0054\n",
            "Epoch 482/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 0.9961 - val_loss: 1.0042\n",
            "Epoch 483/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 0.9962 - val_loss: 1.0061\n",
            "Epoch 484/500\n",
            "990/990 [==============================] - 1s 784us/step - loss: 0.9960 - val_loss: 1.0050\n",
            "Epoch 485/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 0.9958 - val_loss: 1.0053\n",
            "Epoch 486/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 0.9957 - val_loss: 1.0052\n",
            "Epoch 487/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9960 - val_loss: 1.0044\n",
            "Epoch 488/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 0.9960 - val_loss: 1.0052\n",
            "Epoch 489/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 0.9960 - val_loss: 1.0052\n",
            "Epoch 490/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 0.9958 - val_loss: 1.0041\n",
            "Epoch 491/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 0.9958 - val_loss: 1.0050\n",
            "Epoch 492/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 0.9963 - val_loss: 1.0054\n",
            "Epoch 493/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 0.9961 - val_loss: 1.0059\n",
            "Epoch 494/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 0.9959 - val_loss: 1.0054\n",
            "Epoch 495/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.9956 - val_loss: 1.0048\n",
            "Epoch 496/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 0.9961 - val_loss: 1.0045\n",
            "Epoch 497/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 0.9960 - val_loss: 1.0056\n",
            "Epoch 498/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 0.9956 - val_loss: 1.0045\n",
            "Epoch 499/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 0.9958 - val_loss: 1.0048\n",
            "Epoch 500/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 0.9962 - val_loss: 1.0047\n",
            "(lamda,Threshold) 0.01 0.005\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (1100, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 3101909 0.01\n",
            "The shape of N (1100, 3072)\n",
            "The minimum value of N  -0.9276894927024841\n",
            "The max value of N 0.8528975248336792\n",
            "[INFO:] Xclean  MSE Computed shape (1100, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (1100, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0061614686, 0.0052109957])\n",
            "[INFO:] The anomaly threshold computed is  0.0052109957\n",
            "[INFO:] The shape of input data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (1100, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [1080, 1052, 1088, 1063, 1081, 1090, 1079, 1042, 1087, 1067, 1069, 1065, 1095, 1075, 1056, 1040, 1018, 1071, 1086, 1092, 1066, 1091, 1050, 1054, 1076, 1098, 1093, 1094, 1057, 997, 1089, 1061, 1072, 1073, 1070, 1060, 1083, 1062, 1085, 1077, 230, 512, 1023, 1025, 1029, 381, 1014, 991, 426, 397]\n",
            "[INFO:] The  Positive boundry 1050\n",
            "[INFO:] The  Negative boundry 50\n",
            "[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n",
            "img shape: (160, 320, 3)\n",
            "\n",
            "Saving Top-10 most anomalous digit: @\n",
            "=====================\n",
            "AUROC 0.01 0.86\n",
            "=======================\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 32) (256, 32)\n",
            "Train on 990 samples, validate on 110 samples\n",
            "Epoch 1/500\n",
            "990/990 [==============================] - 9s 9ms/step - loss: 1.8687 - val_loss: 1.8788\n",
            "Epoch 2/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8690 - val_loss: 1.8786\n",
            "Epoch 3/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8687 - val_loss: 1.8772\n",
            "Epoch 4/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.8689 - val_loss: 1.8778\n",
            "Epoch 5/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8689 - val_loss: 1.8774\n",
            "Epoch 6/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 1.8688 - val_loss: 1.8771\n",
            "Epoch 7/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8688 - val_loss: 1.8783\n",
            "Epoch 8/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 1.8689 - val_loss: 1.8771\n",
            "Epoch 9/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8687 - val_loss: 1.8773\n",
            "Epoch 10/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8689 - val_loss: 1.8781\n",
            "Epoch 11/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 1.8690 - val_loss: 1.8775\n",
            "Epoch 12/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8690 - val_loss: 1.8775\n",
            "Epoch 13/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 1.8689 - val_loss: 1.8779\n",
            "Epoch 14/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8689 - val_loss: 1.8781\n",
            "Epoch 15/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8692 - val_loss: 1.8782\n",
            "Epoch 16/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 1.8691 - val_loss: 1.8801\n",
            "Epoch 17/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.8692 - val_loss: 1.8804\n",
            "Epoch 18/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8693 - val_loss: 1.8810\n",
            "Epoch 19/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8689 - val_loss: 1.8794\n",
            "Epoch 20/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8689 - val_loss: 1.8791\n",
            "Epoch 21/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8688 - val_loss: 1.8793\n",
            "Epoch 22/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.8689 - val_loss: 1.8801\n",
            "Epoch 23/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8691 - val_loss: 1.8796\n",
            "Epoch 24/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8690 - val_loss: 1.8803\n",
            "Epoch 25/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8690 - val_loss: 1.8798\n",
            "Epoch 26/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8693 - val_loss: 1.8807\n",
            "Epoch 27/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8689 - val_loss: 1.8794\n",
            "Epoch 28/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8688 - val_loss: 1.8807\n",
            "Epoch 29/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.8686 - val_loss: 1.8795\n",
            "Epoch 30/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.8689 - val_loss: 1.8809\n",
            "Epoch 31/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8691 - val_loss: 1.8795\n",
            "Epoch 32/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.8690 - val_loss: 1.8811\n",
            "Epoch 33/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 1.8691 - val_loss: 1.8806\n",
            "Epoch 34/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8696 - val_loss: 1.8815\n",
            "Epoch 35/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.8687 - val_loss: 1.8812\n",
            "Epoch 36/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8691 - val_loss: 1.8815\n",
            "Epoch 37/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8690 - val_loss: 1.8804\n",
            "Epoch 38/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8691 - val_loss: 1.8812\n",
            "Epoch 39/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.8688 - val_loss: 1.8807\n",
            "Epoch 40/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.8689 - val_loss: 1.8813\n",
            "Epoch 41/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8689 - val_loss: 1.8805\n",
            "Epoch 42/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8689 - val_loss: 1.8805\n",
            "Epoch 43/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8688 - val_loss: 1.8813\n",
            "Epoch 44/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 1.8691 - val_loss: 1.8795\n",
            "Epoch 45/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8691 - val_loss: 1.8813\n",
            "Epoch 46/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8688 - val_loss: 1.8800\n",
            "Epoch 47/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8688 - val_loss: 1.8808\n",
            "Epoch 48/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8689 - val_loss: 1.8799\n",
            "Epoch 49/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.8687 - val_loss: 1.8817\n",
            "Epoch 50/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8690 - val_loss: 1.8788\n",
            "Epoch 51/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8688 - val_loss: 1.8803\n",
            "Epoch 52/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8691 - val_loss: 1.8799\n",
            "Epoch 53/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8688 - val_loss: 1.8800\n",
            "Epoch 54/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8687 - val_loss: 1.8809\n",
            "Epoch 55/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8689 - val_loss: 1.8805\n",
            "Epoch 56/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8688 - val_loss: 1.8804\n",
            "Epoch 57/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8691 - val_loss: 1.8798\n",
            "Epoch 58/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 1.8691 - val_loss: 1.8794\n",
            "Epoch 59/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8689 - val_loss: 1.8814\n",
            "Epoch 60/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8688 - val_loss: 1.8799\n",
            "Epoch 61/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8690 - val_loss: 1.8785\n",
            "Epoch 62/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.8688 - val_loss: 1.8801\n",
            "Epoch 63/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8687 - val_loss: 1.8807\n",
            "Epoch 64/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8689 - val_loss: 1.8810\n",
            "Epoch 65/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8691 - val_loss: 1.8797\n",
            "Epoch 66/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8690 - val_loss: 1.8784\n",
            "Epoch 67/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8689 - val_loss: 1.8795\n",
            "Epoch 68/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 1.8688 - val_loss: 1.8818\n",
            "Epoch 69/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 1.8692 - val_loss: 1.8804\n",
            "Epoch 70/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.8689 - val_loss: 1.8812\n",
            "Epoch 71/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8689 - val_loss: 1.8818\n",
            "Epoch 72/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8693 - val_loss: 1.8811\n",
            "Epoch 73/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8689 - val_loss: 1.8813\n",
            "Epoch 74/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8688 - val_loss: 1.8813\n",
            "Epoch 75/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.8686 - val_loss: 1.8812\n",
            "Epoch 76/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8688 - val_loss: 1.8819\n",
            "Epoch 77/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8689 - val_loss: 1.8812\n",
            "Epoch 78/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8688 - val_loss: 1.8806\n",
            "Epoch 79/500\n",
            "990/990 [==============================] - 1s 792us/step - loss: 1.8688 - val_loss: 1.8799\n",
            "Epoch 80/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.8688 - val_loss: 1.8816\n",
            "Epoch 81/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.8690 - val_loss: 1.8798\n",
            "Epoch 82/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8687 - val_loss: 1.8817\n",
            "Epoch 83/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 1.8687 - val_loss: 1.8811\n",
            "Epoch 84/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8690 - val_loss: 1.8816\n",
            "Epoch 85/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8689 - val_loss: 1.8797\n",
            "Epoch 86/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 1.8690 - val_loss: 1.8812\n",
            "Epoch 87/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8687 - val_loss: 1.8806\n",
            "Epoch 88/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.8688 - val_loss: 1.8810\n",
            "Epoch 89/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 1.8688 - val_loss: 1.8814\n",
            "Epoch 90/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8688 - val_loss: 1.8809\n",
            "Epoch 91/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8687 - val_loss: 1.8795\n",
            "Epoch 92/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8687 - val_loss: 1.8807\n",
            "Epoch 93/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8687 - val_loss: 1.8811\n",
            "Epoch 94/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.8689 - val_loss: 1.8817\n",
            "Epoch 95/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.8688 - val_loss: 1.8811\n",
            "Epoch 96/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 1.8690 - val_loss: 1.8819\n",
            "Epoch 97/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8689 - val_loss: 1.8817\n",
            "Epoch 98/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8686 - val_loss: 1.8796\n",
            "Epoch 99/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8686 - val_loss: 1.8824\n",
            "Epoch 100/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8687 - val_loss: 1.8818\n",
            "Epoch 101/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 1.8689 - val_loss: 1.8813\n",
            "Epoch 102/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8690 - val_loss: 1.8823\n",
            "Epoch 103/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8689 - val_loss: 1.8815\n",
            "Epoch 104/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8690 - val_loss: 1.8790\n",
            "Epoch 105/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8691 - val_loss: 1.8816\n",
            "Epoch 106/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8689 - val_loss: 1.8818\n",
            "Epoch 107/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8688 - val_loss: 1.8822\n",
            "Epoch 108/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8687 - val_loss: 1.8830\n",
            "Epoch 109/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.8686 - val_loss: 1.8808\n",
            "Epoch 110/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.8686 - val_loss: 1.8816\n",
            "Epoch 111/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 1.8688 - val_loss: 1.8808\n",
            "Epoch 112/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8690 - val_loss: 1.8803\n",
            "Epoch 113/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8688 - val_loss: 1.8820\n",
            "Epoch 114/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.8687 - val_loss: 1.8828\n",
            "Epoch 115/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.8688 - val_loss: 1.8812\n",
            "Epoch 116/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8688 - val_loss: 1.8812\n",
            "Epoch 117/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.8692 - val_loss: 1.8822\n",
            "Epoch 118/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8687 - val_loss: 1.8801\n",
            "Epoch 119/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 1.8689 - val_loss: 1.8821\n",
            "Epoch 120/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 1.8691 - val_loss: 1.8811\n",
            "Epoch 121/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8688 - val_loss: 1.8827\n",
            "Epoch 122/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8690 - val_loss: 1.8810\n",
            "Epoch 123/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8691 - val_loss: 1.8819\n",
            "Epoch 124/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8688 - val_loss: 1.8802\n",
            "Epoch 125/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8687 - val_loss: 1.8805\n",
            "Epoch 126/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8690 - val_loss: 1.8819\n",
            "Epoch 127/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8688 - val_loss: 1.8815\n",
            "Epoch 128/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8689 - val_loss: 1.8821\n",
            "Epoch 129/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8685 - val_loss: 1.8832\n",
            "Epoch 130/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.8688 - val_loss: 1.8825\n",
            "Epoch 131/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8687 - val_loss: 1.8824\n",
            "Epoch 132/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8687 - val_loss: 1.8820\n",
            "Epoch 133/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8687 - val_loss: 1.8813\n",
            "Epoch 134/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8688 - val_loss: 1.8800\n",
            "Epoch 135/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8688 - val_loss: 1.8820\n",
            "Epoch 136/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8688 - val_loss: 1.8816\n",
            "Epoch 137/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8689 - val_loss: 1.8829\n",
            "Epoch 138/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8688 - val_loss: 1.8834\n",
            "Epoch 139/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8688 - val_loss: 1.8810\n",
            "Epoch 140/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.8687 - val_loss: 1.8842\n",
            "Epoch 141/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.8687 - val_loss: 1.8824\n",
            "Epoch 142/500\n",
            "990/990 [==============================] - 1s 786us/step - loss: 1.8693 - val_loss: 1.8815\n",
            "Epoch 143/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8691 - val_loss: 1.8806\n",
            "Epoch 144/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8688 - val_loss: 1.8806\n",
            "Epoch 145/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8687 - val_loss: 1.8799\n",
            "Epoch 146/500\n",
            "990/990 [==============================] - 1s 730us/step - loss: 1.8687 - val_loss: 1.8829\n",
            "Epoch 147/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8691 - val_loss: 1.8808\n",
            "Epoch 148/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8687 - val_loss: 1.8832\n",
            "Epoch 149/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8689 - val_loss: 1.8821\n",
            "Epoch 150/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8690 - val_loss: 1.8831\n",
            "Epoch 151/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.8690 - val_loss: 1.8832\n",
            "Epoch 152/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.8688 - val_loss: 1.8806\n",
            "Epoch 153/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8692 - val_loss: 1.8832\n",
            "Epoch 154/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8687 - val_loss: 1.8815\n",
            "Epoch 155/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 1.8685 - val_loss: 1.8815\n",
            "Epoch 156/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8686 - val_loss: 1.8810\n",
            "Epoch 157/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 1.8686 - val_loss: 1.8817\n",
            "Epoch 158/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.8688 - val_loss: 1.8823\n",
            "Epoch 159/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8685 - val_loss: 1.8828\n",
            "Epoch 160/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8686 - val_loss: 1.8822\n",
            "Epoch 161/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 1.8689 - val_loss: 1.8833\n",
            "Epoch 162/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8687 - val_loss: 1.8832\n",
            "Epoch 163/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8688 - val_loss: 1.8811\n",
            "Epoch 164/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8687 - val_loss: 1.8811\n",
            "Epoch 165/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8687 - val_loss: 1.8827\n",
            "Epoch 166/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8689 - val_loss: 1.8815\n",
            "Epoch 167/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8688 - val_loss: 1.8820\n",
            "Epoch 168/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8686 - val_loss: 1.8827\n",
            "Epoch 169/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8687 - val_loss: 1.8817\n",
            "Epoch 170/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8685 - val_loss: 1.8815\n",
            "Epoch 171/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 1.8686 - val_loss: 1.8826\n",
            "Epoch 172/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.8687 - val_loss: 1.8814\n",
            "Epoch 173/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 1.8689 - val_loss: 1.8820\n",
            "Epoch 174/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8688 - val_loss: 1.8814\n",
            "Epoch 175/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8687 - val_loss: 1.8816\n",
            "Epoch 176/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8687 - val_loss: 1.8834\n",
            "Epoch 177/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8688 - val_loss: 1.8805\n",
            "Epoch 178/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8687 - val_loss: 1.8820\n",
            "Epoch 179/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8689 - val_loss: 1.8833\n",
            "Epoch 180/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8689 - val_loss: 1.8817\n",
            "Epoch 181/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 1.8690 - val_loss: 1.8822\n",
            "Epoch 182/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8686 - val_loss: 1.8824\n",
            "Epoch 183/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8688 - val_loss: 1.8822\n",
            "Epoch 184/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8690 - val_loss: 1.8931\n",
            "Epoch 185/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8692 - val_loss: 1.8815\n",
            "Epoch 186/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 1.8689 - val_loss: 1.8823\n",
            "Epoch 187/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8689 - val_loss: 1.8827\n",
            "Epoch 188/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 1.8689 - val_loss: 1.8805\n",
            "Epoch 189/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.8686 - val_loss: 1.8817\n",
            "Epoch 190/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8685 - val_loss: 1.8817\n",
            "Epoch 191/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 1.8688 - val_loss: 1.8812\n",
            "Epoch 192/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8687 - val_loss: 1.8822\n",
            "Epoch 193/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 1.8687 - val_loss: 1.8816\n",
            "Epoch 194/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8688 - val_loss: 1.8811\n",
            "Epoch 195/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 1.8688 - val_loss: 1.8814\n",
            "Epoch 196/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 1.8690 - val_loss: 1.8815\n",
            "Epoch 197/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8690 - val_loss: 1.8824\n",
            "Epoch 198/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 1.8688 - val_loss: 1.8806\n",
            "Epoch 199/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8686 - val_loss: 1.8823\n",
            "Epoch 200/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 1.8686 - val_loss: 1.8800\n",
            "Epoch 201/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8691 - val_loss: 1.8814\n",
            "Epoch 202/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8686 - val_loss: 1.8829\n",
            "Epoch 203/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 1.8684 - val_loss: 1.8812\n",
            "Epoch 204/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8687 - val_loss: 1.8809\n",
            "Epoch 205/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8688 - val_loss: 1.8815\n",
            "Epoch 206/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8688 - val_loss: 1.8804\n",
            "Epoch 207/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 1.8691 - val_loss: 1.8813\n",
            "Epoch 208/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 1.8686 - val_loss: 1.8809\n",
            "Epoch 209/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8687 - val_loss: 1.8808\n",
            "Epoch 210/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8689 - val_loss: 1.8808\n",
            "Epoch 211/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8688 - val_loss: 1.8802\n",
            "Epoch 212/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8687 - val_loss: 1.8805\n",
            "Epoch 213/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.8686 - val_loss: 1.8819\n",
            "Epoch 214/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.8687 - val_loss: 1.8821\n",
            "Epoch 215/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8688 - val_loss: 1.8827\n",
            "Epoch 216/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.8690 - val_loss: 1.8822\n",
            "Epoch 217/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8690 - val_loss: 1.8828\n",
            "Epoch 218/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 1.8687 - val_loss: 1.8817\n",
            "Epoch 219/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8686 - val_loss: 1.8819\n",
            "Epoch 220/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8688 - val_loss: 1.8812\n",
            "Epoch 221/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8686 - val_loss: 1.8827\n",
            "Epoch 222/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.8686 - val_loss: 1.8820\n",
            "Epoch 223/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8690 - val_loss: 1.8814\n",
            "Epoch 224/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.8690 - val_loss: 1.8811\n",
            "Epoch 225/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8690 - val_loss: 1.8830\n",
            "Epoch 226/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.8689 - val_loss: 1.8816\n",
            "Epoch 227/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8689 - val_loss: 1.8809\n",
            "Epoch 228/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8687 - val_loss: 1.8803\n",
            "Epoch 229/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8687 - val_loss: 1.8805\n",
            "Epoch 230/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 1.8687 - val_loss: 1.8810\n",
            "Epoch 231/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 1.8688 - val_loss: 1.8823\n",
            "Epoch 232/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8687 - val_loss: 1.8807\n",
            "Epoch 233/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 1.8691 - val_loss: 1.8826\n",
            "Epoch 234/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 1.8687 - val_loss: 1.8821\n",
            "Epoch 235/500\n",
            "990/990 [==============================] - 1s 733us/step - loss: 1.8694 - val_loss: 1.8822\n",
            "Epoch 236/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8688 - val_loss: 1.8826\n",
            "Epoch 237/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8689 - val_loss: 1.8820\n",
            "Epoch 238/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8689 - val_loss: 1.8825\n",
            "Epoch 239/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8687 - val_loss: 1.8822\n",
            "Epoch 240/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.8687 - val_loss: 1.8831\n",
            "Epoch 241/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.8686 - val_loss: 1.8816\n",
            "Epoch 242/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8689 - val_loss: 1.8817\n",
            "Epoch 243/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 1.8686 - val_loss: 1.8821\n",
            "Epoch 244/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8685 - val_loss: 1.8823\n",
            "Epoch 245/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.8687 - val_loss: 1.8830\n",
            "Epoch 246/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8688 - val_loss: 1.8826\n",
            "Epoch 247/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.8687 - val_loss: 1.8823\n",
            "Epoch 248/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8686 - val_loss: 1.8823\n",
            "Epoch 249/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8686 - val_loss: 1.8819\n",
            "Epoch 250/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 1.8690 - val_loss: 1.8826\n",
            "Epoch 251/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.8685 - val_loss: 1.8827\n",
            "Epoch 252/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8687 - val_loss: 1.8831\n",
            "Epoch 253/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.8689 - val_loss: 1.8816\n",
            "Epoch 254/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.8690 - val_loss: 1.8827\n",
            "Epoch 255/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8686 - val_loss: 1.8825\n",
            "Epoch 256/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8687 - val_loss: 1.8823\n",
            "Epoch 257/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8688 - val_loss: 1.8825\n",
            "Epoch 258/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.8687 - val_loss: 1.8822\n",
            "Epoch 259/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8687 - val_loss: 1.8827\n",
            "Epoch 260/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 1.8688 - val_loss: 1.8831\n",
            "Epoch 261/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8692 - val_loss: 1.8837\n",
            "Epoch 262/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8688 - val_loss: 1.8824\n",
            "Epoch 263/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8688 - val_loss: 1.8829\n",
            "Epoch 264/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.8692 - val_loss: 1.8820\n",
            "Epoch 265/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8686 - val_loss: 1.8829\n",
            "Epoch 266/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.8687 - val_loss: 1.8816\n",
            "Epoch 267/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8686 - val_loss: 1.8819\n",
            "Epoch 268/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8690 - val_loss: 1.8824\n",
            "Epoch 269/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8688 - val_loss: 1.8827\n",
            "Epoch 270/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.8687 - val_loss: 1.8815\n",
            "Epoch 271/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8688 - val_loss: 1.8820\n",
            "Epoch 272/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8690 - val_loss: 1.8829\n",
            "Epoch 273/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8690 - val_loss: 1.8815\n",
            "Epoch 274/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8689 - val_loss: 1.8836\n",
            "Epoch 275/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8687 - val_loss: 1.8827\n",
            "Epoch 276/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8686 - val_loss: 1.8837\n",
            "Epoch 277/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8686 - val_loss: 1.8823\n",
            "Epoch 278/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.8685 - val_loss: 1.8824\n",
            "Epoch 279/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8686 - val_loss: 1.8829\n",
            "Epoch 280/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8685 - val_loss: 1.8820\n",
            "Epoch 281/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.8688 - val_loss: 1.8803\n",
            "Epoch 282/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8686 - val_loss: 1.8828\n",
            "Epoch 283/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.8686 - val_loss: 1.8822\n",
            "Epoch 284/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8687 - val_loss: 1.8812\n",
            "Epoch 285/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8691 - val_loss: 1.8809\n",
            "Epoch 286/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.8686 - val_loss: 1.8804\n",
            "Epoch 287/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.8686 - val_loss: 1.8826\n",
            "Epoch 288/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8688 - val_loss: 1.8825\n",
            "Epoch 289/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8690 - val_loss: 1.8813\n",
            "Epoch 290/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8687 - val_loss: 1.8817\n",
            "Epoch 291/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8687 - val_loss: 1.8819\n",
            "Epoch 292/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.8686 - val_loss: 1.8817\n",
            "Epoch 293/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 1.8687 - val_loss: 1.8818\n",
            "Epoch 294/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8687 - val_loss: 1.8816\n",
            "Epoch 295/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8687 - val_loss: 1.8823\n",
            "Epoch 296/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8688 - val_loss: 1.8814\n",
            "Epoch 297/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8685 - val_loss: 1.8823\n",
            "Epoch 298/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8689 - val_loss: 1.8818\n",
            "Epoch 299/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8685 - val_loss: 1.8814\n",
            "Epoch 300/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8686 - val_loss: 1.8818\n",
            "Epoch 301/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8688 - val_loss: 1.8825\n",
            "Epoch 302/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.8688 - val_loss: 1.8830\n",
            "Epoch 303/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8686 - val_loss: 1.8820\n",
            "Epoch 304/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8685 - val_loss: 1.8819\n",
            "Epoch 305/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8688 - val_loss: 1.8815\n",
            "Epoch 306/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8687 - val_loss: 1.8834\n",
            "Epoch 307/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.8690 - val_loss: 1.8820\n",
            "Epoch 308/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 1.8687 - val_loss: 1.8809\n",
            "Epoch 309/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8689 - val_loss: 1.8823\n",
            "Epoch 310/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8688 - val_loss: 1.8822\n",
            "Epoch 311/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8686 - val_loss: 1.8813\n",
            "Epoch 312/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8686 - val_loss: 1.8833\n",
            "Epoch 313/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 1.8685 - val_loss: 1.8827\n",
            "Epoch 314/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8688 - val_loss: 1.8824\n",
            "Epoch 315/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 1.8687 - val_loss: 1.8817\n",
            "Epoch 316/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8689 - val_loss: 1.8818\n",
            "Epoch 317/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8689 - val_loss: 1.8817\n",
            "Epoch 318/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.8690 - val_loss: 1.8825\n",
            "Epoch 319/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8691 - val_loss: 1.8819\n",
            "Epoch 320/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.8687 - val_loss: 1.8819\n",
            "Epoch 321/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.8690 - val_loss: 1.8816\n",
            "Epoch 322/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.8689 - val_loss: 1.8810\n",
            "Epoch 323/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8687 - val_loss: 1.8821\n",
            "Epoch 324/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8688 - val_loss: 1.8822\n",
            "Epoch 325/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.8683 - val_loss: 1.8823\n",
            "Epoch 326/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.8686 - val_loss: 1.8821\n",
            "Epoch 327/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8688 - val_loss: 1.8824\n",
            "Epoch 328/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8686 - val_loss: 1.8823\n",
            "Epoch 329/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8688 - val_loss: 1.8831\n",
            "Epoch 330/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 1.8687 - val_loss: 1.8829\n",
            "Epoch 331/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.8687 - val_loss: 1.8820\n",
            "Epoch 332/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8687 - val_loss: 1.8823\n",
            "Epoch 333/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 1.8685 - val_loss: 1.8819\n",
            "Epoch 334/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8687 - val_loss: 1.8823\n",
            "Epoch 335/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8684 - val_loss: 1.8823\n",
            "Epoch 336/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8685 - val_loss: 1.8817\n",
            "Epoch 337/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.8686 - val_loss: 1.8812\n",
            "Epoch 338/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8686 - val_loss: 1.8825\n",
            "Epoch 339/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.8686 - val_loss: 1.8825\n",
            "Epoch 340/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.8685 - val_loss: 1.8824\n",
            "Epoch 341/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8687 - val_loss: 1.8822\n",
            "Epoch 342/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8687 - val_loss: 1.8818\n",
            "Epoch 343/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 1.8686 - val_loss: 1.8823\n",
            "Epoch 344/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8684 - val_loss: 1.8831\n",
            "Epoch 345/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.8687 - val_loss: 1.8832\n",
            "Epoch 346/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8685 - val_loss: 1.8824\n",
            "Epoch 347/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8691 - val_loss: 1.8830\n",
            "Epoch 348/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.8687 - val_loss: 1.8827\n",
            "Epoch 349/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8687 - val_loss: 1.8814\n",
            "Epoch 350/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 1.8688 - val_loss: 1.8824\n",
            "Epoch 351/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8687 - val_loss: 1.8839\n",
            "Epoch 352/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8686 - val_loss: 1.8823\n",
            "Epoch 353/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8684 - val_loss: 1.8821\n",
            "Epoch 354/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8686 - val_loss: 1.8815\n",
            "Epoch 355/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8687 - val_loss: 1.8815\n",
            "Epoch 356/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.8686 - val_loss: 1.8823\n",
            "Epoch 357/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8687 - val_loss: 1.8822\n",
            "Epoch 358/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.8688 - val_loss: 1.8823\n",
            "Epoch 359/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8685 - val_loss: 1.8814\n",
            "Epoch 360/500\n",
            "990/990 [==============================] - 1s 732us/step - loss: 1.8689 - val_loss: 1.8819\n",
            "Epoch 361/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.8684 - val_loss: 1.8835\n",
            "Epoch 362/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 1.8686 - val_loss: 1.8817\n",
            "Epoch 363/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8686 - val_loss: 1.8821\n",
            "Epoch 364/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8685 - val_loss: 1.8824\n",
            "Epoch 365/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8686 - val_loss: 1.8827\n",
            "Epoch 366/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8686 - val_loss: 1.8824\n",
            "Epoch 367/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.8684 - val_loss: 1.8824\n",
            "Epoch 368/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8684 - val_loss: 1.8814\n",
            "Epoch 369/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8684 - val_loss: 1.8822\n",
            "Epoch 370/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8685 - val_loss: 1.8823\n",
            "Epoch 371/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.8688 - val_loss: 1.8825\n",
            "Epoch 372/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8687 - val_loss: 1.8822\n",
            "Epoch 373/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8686 - val_loss: 1.8822\n",
            "Epoch 374/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8685 - val_loss: 1.8811\n",
            "Epoch 375/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8686 - val_loss: 1.8821\n",
            "Epoch 376/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8688 - val_loss: 1.8825\n",
            "Epoch 377/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8685 - val_loss: 1.8824\n",
            "Epoch 378/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8686 - val_loss: 1.8817\n",
            "Epoch 379/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 1.8685 - val_loss: 1.8824\n",
            "Epoch 380/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.8687 - val_loss: 1.8816\n",
            "Epoch 381/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.8687 - val_loss: 1.8828\n",
            "Epoch 382/500\n",
            "990/990 [==============================] - 1s 732us/step - loss: 1.8686 - val_loss: 1.8831\n",
            "Epoch 383/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.8686 - val_loss: 1.8830\n",
            "Epoch 384/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 1.8687 - val_loss: 1.8835\n",
            "Epoch 385/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8687 - val_loss: 1.8828\n",
            "Epoch 386/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.8688 - val_loss: 1.8825\n",
            "Epoch 387/500\n",
            "990/990 [==============================] - 1s 733us/step - loss: 1.8686 - val_loss: 1.8820\n",
            "Epoch 388/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8686 - val_loss: 1.8820\n",
            "Epoch 389/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8689 - val_loss: 1.8815\n",
            "Epoch 390/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 1.8687 - val_loss: 1.8839\n",
            "Epoch 391/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8692 - val_loss: 1.8829\n",
            "Epoch 392/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 1.8685 - val_loss: 1.8829\n",
            "Epoch 393/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8688 - val_loss: 1.8824\n",
            "Epoch 394/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 1.8686 - val_loss: 1.8828\n",
            "Epoch 395/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.8689 - val_loss: 1.8824\n",
            "Epoch 396/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8686 - val_loss: 1.8830\n",
            "Epoch 397/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8688 - val_loss: 1.8818\n",
            "Epoch 398/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8685 - val_loss: 1.8833\n",
            "Epoch 399/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8686 - val_loss: 1.8826\n",
            "Epoch 400/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8686 - val_loss: 1.8823\n",
            "Epoch 401/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8687 - val_loss: 1.8827\n",
            "Epoch 402/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8687 - val_loss: 1.8827\n",
            "Epoch 403/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8689 - val_loss: 1.8839\n",
            "Epoch 404/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 1.8689 - val_loss: 1.8835\n",
            "Epoch 405/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8686 - val_loss: 1.8821\n",
            "Epoch 406/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8689 - val_loss: 1.8823\n",
            "Epoch 407/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 1.8687 - val_loss: 1.8817\n",
            "Epoch 408/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8685 - val_loss: 1.8831\n",
            "Epoch 409/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 1.8688 - val_loss: 1.8833\n",
            "Epoch 410/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8686 - val_loss: 1.8825\n",
            "Epoch 411/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8686 - val_loss: 1.8828\n",
            "Epoch 412/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8684 - val_loss: 1.8822\n",
            "Epoch 413/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 1.8685 - val_loss: 1.8832\n",
            "Epoch 414/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 1.8684 - val_loss: 1.8826\n",
            "Epoch 415/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8688 - val_loss: 1.8824\n",
            "Epoch 416/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8688 - val_loss: 1.8841\n",
            "Epoch 417/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8684 - val_loss: 1.8825\n",
            "Epoch 418/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8688 - val_loss: 1.8830\n",
            "Epoch 419/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8687 - val_loss: 1.8822\n",
            "Epoch 420/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8688 - val_loss: 1.8829\n",
            "Epoch 421/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.8685 - val_loss: 1.8846\n",
            "Epoch 422/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8687 - val_loss: 1.8826\n",
            "Epoch 423/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8691 - val_loss: 1.8827\n",
            "Epoch 424/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8689 - val_loss: 1.8830\n",
            "Epoch 425/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 1.8684 - val_loss: 1.8829\n",
            "Epoch 426/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8685 - val_loss: 1.8836\n",
            "Epoch 427/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8685 - val_loss: 1.8834\n",
            "Epoch 428/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.8685 - val_loss: 1.8821\n",
            "Epoch 429/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 1.8684 - val_loss: 1.8839\n",
            "Epoch 430/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8683 - val_loss: 1.8832\n",
            "Epoch 431/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.8687 - val_loss: 1.8831\n",
            "Epoch 432/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 1.8685 - val_loss: 1.8832\n",
            "Epoch 433/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8687 - val_loss: 1.8828\n",
            "Epoch 434/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 1.8687 - val_loss: 1.8837\n",
            "Epoch 435/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8691 - val_loss: 1.8842\n",
            "Epoch 436/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.8687 - val_loss: 1.8850\n",
            "Epoch 437/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8685 - val_loss: 1.8834\n",
            "Epoch 438/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8684 - val_loss: 1.8835\n",
            "Epoch 439/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8685 - val_loss: 1.8847\n",
            "Epoch 440/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8687 - val_loss: 1.8835\n",
            "Epoch 441/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8686 - val_loss: 1.8836\n",
            "Epoch 442/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8688 - val_loss: 1.8833\n",
            "Epoch 443/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.8686 - val_loss: 1.8822\n",
            "Epoch 444/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.8685 - val_loss: 1.8823\n",
            "Epoch 445/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.8687 - val_loss: 1.8827\n",
            "Epoch 446/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8689 - val_loss: 1.8833\n",
            "Epoch 447/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8687 - val_loss: 1.8827\n",
            "Epoch 448/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.8686 - val_loss: 1.8836\n",
            "Epoch 449/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 1.8686 - val_loss: 1.8832\n",
            "Epoch 450/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8689 - val_loss: 1.8841\n",
            "Epoch 451/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.8685 - val_loss: 1.8824\n",
            "Epoch 452/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.8687 - val_loss: 1.8835\n",
            "Epoch 453/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8686 - val_loss: 1.8839\n",
            "Epoch 454/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8686 - val_loss: 1.8837\n",
            "Epoch 455/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.8683 - val_loss: 1.8819\n",
            "Epoch 456/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8687 - val_loss: 1.8828\n",
            "Epoch 457/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8688 - val_loss: 1.8824\n",
            "Epoch 458/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.8688 - val_loss: 1.8834\n",
            "Epoch 459/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8686 - val_loss: 1.8824\n",
            "Epoch 460/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.8688 - val_loss: 1.8825\n",
            "Epoch 461/500\n",
            "990/990 [==============================] - 1s 779us/step - loss: 1.8684 - val_loss: 1.8837\n",
            "Epoch 462/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.8687 - val_loss: 1.8837\n",
            "Epoch 463/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 1.8688 - val_loss: 1.8833\n",
            "Epoch 464/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8687 - val_loss: 1.8830\n",
            "Epoch 465/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.8686 - val_loss: 1.8840\n",
            "Epoch 466/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.8688 - val_loss: 1.8834\n",
            "Epoch 467/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.8683 - val_loss: 1.8831\n",
            "Epoch 468/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.8686 - val_loss: 1.8845\n",
            "Epoch 469/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8687 - val_loss: 1.8838\n",
            "Epoch 470/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.8688 - val_loss: 1.8849\n",
            "Epoch 471/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8692 - val_loss: 1.8831\n",
            "Epoch 472/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8686 - val_loss: 1.8832\n",
            "Epoch 473/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8689 - val_loss: 1.8824\n",
            "Epoch 474/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8685 - val_loss: 1.8846\n",
            "Epoch 475/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.8687 - val_loss: 1.8827\n",
            "Epoch 476/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8687 - val_loss: 1.8836\n",
            "Epoch 477/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 1.8684 - val_loss: 1.8830\n",
            "Epoch 478/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8688 - val_loss: 1.8851\n",
            "Epoch 479/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.8689 - val_loss: 1.8821\n",
            "Epoch 480/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.8686 - val_loss: 1.8823\n",
            "Epoch 481/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.8685 - val_loss: 1.8834\n",
            "Epoch 482/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.8685 - val_loss: 1.8831\n",
            "Epoch 483/500\n",
            "990/990 [==============================] - 1s 794us/step - loss: 1.8686 - val_loss: 1.8834\n",
            "Epoch 484/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.8687 - val_loss: 1.8840\n",
            "Epoch 485/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8688 - val_loss: 1.8847\n",
            "Epoch 486/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.8685 - val_loss: 1.8858\n",
            "Epoch 487/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.8688 - val_loss: 1.8842\n",
            "Epoch 488/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8685 - val_loss: 1.8840\n",
            "Epoch 489/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.8687 - val_loss: 1.8840\n",
            "Epoch 490/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.8686 - val_loss: 1.8846\n",
            "Epoch 491/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.8685 - val_loss: 1.8849\n",
            "Epoch 492/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.8686 - val_loss: 1.8838\n",
            "Epoch 493/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.8685 - val_loss: 1.8841\n",
            "Epoch 494/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.8683 - val_loss: 1.8843\n",
            "Epoch 495/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8686 - val_loss: 1.8832\n",
            "Epoch 496/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.8689 - val_loss: 1.8840\n",
            "Epoch 497/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.8686 - val_loss: 1.8832\n",
            "Epoch 498/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.8684 - val_loss: 1.8826\n",
            "Epoch 499/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.8683 - val_loss: 1.8845\n",
            "Epoch 500/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.8683 - val_loss: 1.8847\n",
            "(lamda,Threshold) 0.1 0.05\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (1100, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 1185377 0.1\n",
            "The shape of N (1100, 3072)\n",
            "The minimum value of N  -0.9060615301132202\n",
            "The max value of N 0.7866846919059753\n",
            "[INFO:] Xclean  MSE Computed shape (1100, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (1100, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0061614686, 0.0052109957, 0.0058396035])\n",
            "[INFO:] The anomaly threshold computed is  0.0058396035\n",
            "[INFO:] The shape of input data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (1100, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [1052, 1063, 1080, 1081, 1088, 1098, 1093, 1090, 1057, 1071, 1076, 1067, 473, 1083, 1042, 1079, 1018, 1069, 1056, 1091, 1095, 1050, 1089, 1065, 1092, 1066, 1073, 1040, 1086, 1087, 1075, 1094, 1070, 1060, 1054, 1061, 1072, 997, 1025, 1023, 1077, 991, 1064, 1003, 1062, 998, 397, 1085, 552, 381]\n",
            "[INFO:] The  Positive boundry 1050\n",
            "[INFO:] The  Negative boundry 50\n",
            "[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n",
            "img shape: (160, 320, 3)\n",
            "\n",
            "Saving Top-10 most anomalous digit: @\n",
            "=====================\n",
            "AUROC 0.1 0.87\n",
            "=======================\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 32) (256, 32)\n",
            "Train on 990 samples, validate on 110 samples\n",
            "Epoch 1/500\n",
            "990/990 [==============================] - 9s 10ms/step - loss: 6.1945 - val_loss: 6.2102\n",
            "Epoch 2/500\n",
            "990/990 [==============================] - 1s 783us/step - loss: 6.1942 - val_loss: 6.2111\n",
            "Epoch 3/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1942 - val_loss: 6.2088\n",
            "Epoch 4/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 6.1946 - val_loss: 6.2083\n",
            "Epoch 5/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 6.1945 - val_loss: 6.2101\n",
            "Epoch 6/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1943 - val_loss: 6.2092\n",
            "Epoch 7/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 6.1943 - val_loss: 6.2087\n",
            "Epoch 8/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1941 - val_loss: 6.2092\n",
            "Epoch 9/500\n",
            "990/990 [==============================] - 1s 785us/step - loss: 6.1941 - val_loss: 6.2110\n",
            "Epoch 10/500\n",
            "990/990 [==============================] - 1s 783us/step - loss: 6.1942 - val_loss: 6.2102\n",
            "Epoch 11/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1942 - val_loss: 6.2097\n",
            "Epoch 12/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 6.1944 - val_loss: 6.2102\n",
            "Epoch 13/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 6.1943 - val_loss: 6.2102\n",
            "Epoch 14/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1940 - val_loss: 6.2097\n",
            "Epoch 15/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 6.1939 - val_loss: 6.2093\n",
            "Epoch 16/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1941 - val_loss: 6.2095\n",
            "Epoch 17/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 6.1941 - val_loss: 6.2096\n",
            "Epoch 18/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 6.1944 - val_loss: 6.2093\n",
            "Epoch 19/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1945 - val_loss: 6.2096\n",
            "Epoch 20/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 6.1941 - val_loss: 6.2097\n",
            "Epoch 21/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1943 - val_loss: 6.2090\n",
            "Epoch 22/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1943 - val_loss: 6.2106\n",
            "Epoch 23/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1942 - val_loss: 6.2086\n",
            "Epoch 24/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1944 - val_loss: 6.2091\n",
            "Epoch 25/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1948 - val_loss: 6.2091\n",
            "Epoch 26/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1943 - val_loss: 6.2092\n",
            "Epoch 27/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1940 - val_loss: 6.2087\n",
            "Epoch 28/500\n",
            "990/990 [==============================] - 1s 781us/step - loss: 6.1944 - val_loss: 6.2099\n",
            "Epoch 29/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 6.1943 - val_loss: 6.2085\n",
            "Epoch 30/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 6.1940 - val_loss: 6.2087\n",
            "Epoch 31/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 6.1941 - val_loss: 6.2107\n",
            "Epoch 32/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 6.1944 - val_loss: 6.2081\n",
            "Epoch 33/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 6.1940 - val_loss: 6.2088\n",
            "Epoch 34/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 6.1941 - val_loss: 6.2093\n",
            "Epoch 35/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1943 - val_loss: 6.2086\n",
            "Epoch 36/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 6.1943 - val_loss: 6.2088\n",
            "Epoch 37/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1943 - val_loss: 6.2091\n",
            "Epoch 38/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1942 - val_loss: 6.2090\n",
            "Epoch 39/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1940 - val_loss: 6.2098\n",
            "Epoch 40/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1943 - val_loss: 6.2099\n",
            "Epoch 41/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 6.1941 - val_loss: 6.2088\n",
            "Epoch 42/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1944 - val_loss: 6.2099\n",
            "Epoch 43/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1942 - val_loss: 6.2104\n",
            "Epoch 44/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1944 - val_loss: 6.2097\n",
            "Epoch 45/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1941 - val_loss: 6.2088\n",
            "Epoch 46/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 6.1943 - val_loss: 6.2096\n",
            "Epoch 47/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 6.1945 - val_loss: 6.2093\n",
            "Epoch 48/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 6.1941 - val_loss: 6.2092\n",
            "Epoch 49/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 6.1943 - val_loss: 6.2088\n",
            "Epoch 50/500\n",
            "990/990 [==============================] - 1s 786us/step - loss: 6.1942 - val_loss: 6.2094\n",
            "Epoch 51/500\n",
            "990/990 [==============================] - 1s 781us/step - loss: 6.1942 - val_loss: 6.2092\n",
            "Epoch 52/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1940 - val_loss: 6.2095\n",
            "Epoch 53/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 6.1942 - val_loss: 6.2087\n",
            "Epoch 54/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1942 - val_loss: 6.2084\n",
            "Epoch 55/500\n",
            "990/990 [==============================] - 1s 785us/step - loss: 6.1940 - val_loss: 6.2081\n",
            "Epoch 56/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 6.1940 - val_loss: 6.2098\n",
            "Epoch 57/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 6.1941 - val_loss: 6.2094\n",
            "Epoch 58/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 6.1941 - val_loss: 6.2093\n",
            "Epoch 59/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 6.1940 - val_loss: 6.2094\n",
            "Epoch 60/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1941 - val_loss: 6.2093\n",
            "Epoch 61/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 6.1943 - val_loss: 6.2093\n",
            "Epoch 62/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1944 - val_loss: 6.2094\n",
            "Epoch 63/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 6.1940 - val_loss: 6.2095\n",
            "Epoch 64/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 6.1942 - val_loss: 6.2095\n",
            "Epoch 65/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1941 - val_loss: 6.2085\n",
            "Epoch 66/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1942 - val_loss: 6.2096\n",
            "Epoch 67/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1942 - val_loss: 6.2092\n",
            "Epoch 68/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 6.1942 - val_loss: 6.2093\n",
            "Epoch 69/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 6.1940 - val_loss: 6.2085\n",
            "Epoch 70/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1941 - val_loss: 6.2102\n",
            "Epoch 71/500\n",
            "990/990 [==============================] - 1s 784us/step - loss: 6.1942 - val_loss: 6.2099\n",
            "Epoch 72/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 6.1941 - val_loss: 6.2084\n",
            "Epoch 73/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 6.1942 - val_loss: 6.2091\n",
            "Epoch 74/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 6.1942 - val_loss: 6.2100\n",
            "Epoch 75/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1940 - val_loss: 6.2108\n",
            "Epoch 76/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 6.1941 - val_loss: 6.2100\n",
            "Epoch 77/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1941 - val_loss: 6.2093\n",
            "Epoch 78/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 6.1941 - val_loss: 6.2093\n",
            "Epoch 79/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1944 - val_loss: 6.2102\n",
            "Epoch 80/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1942 - val_loss: 6.2092\n",
            "Epoch 81/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 6.1939 - val_loss: 6.2092\n",
            "Epoch 82/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 6.1948 - val_loss: 6.2097\n",
            "Epoch 83/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 6.1942 - val_loss: 6.2095\n",
            "Epoch 84/500\n",
            "990/990 [==============================] - 1s 786us/step - loss: 6.1942 - val_loss: 6.2101\n",
            "Epoch 85/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1943 - val_loss: 6.2109\n",
            "Epoch 86/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1942 - val_loss: 6.2096\n",
            "Epoch 87/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1942 - val_loss: 6.2096\n",
            "Epoch 88/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 6.1942 - val_loss: 6.2088\n",
            "Epoch 89/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1939 - val_loss: 6.2087\n",
            "Epoch 90/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 6.1942 - val_loss: 6.2087\n",
            "Epoch 91/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 6.1940 - val_loss: 6.2093\n",
            "Epoch 92/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 6.1939 - val_loss: 6.2099\n",
            "Epoch 93/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 6.1939 - val_loss: 6.2099\n",
            "Epoch 94/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 6.1940 - val_loss: 6.2091\n",
            "Epoch 95/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1942 - val_loss: 6.2099\n",
            "Epoch 96/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 6.1941 - val_loss: 6.2097\n",
            "Epoch 97/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1941 - val_loss: 6.2103\n",
            "Epoch 98/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1944 - val_loss: 6.2088\n",
            "Epoch 99/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1940 - val_loss: 6.2100\n",
            "Epoch 100/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 6.1941 - val_loss: 6.2101\n",
            "Epoch 101/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1945 - val_loss: 6.2098\n",
            "Epoch 102/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1944 - val_loss: 6.2106\n",
            "Epoch 103/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1943 - val_loss: 6.2100\n",
            "Epoch 104/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 6.1941 - val_loss: 6.2095\n",
            "Epoch 105/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 6.1940 - val_loss: 6.2111\n",
            "Epoch 106/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 6.1939 - val_loss: 6.2104\n",
            "Epoch 107/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1941 - val_loss: 6.2101\n",
            "Epoch 108/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1943 - val_loss: 6.2100\n",
            "Epoch 109/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 6.1944 - val_loss: 6.2109\n",
            "Epoch 110/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1942 - val_loss: 6.2109\n",
            "Epoch 111/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 6.1940 - val_loss: 6.2101\n",
            "Epoch 112/500\n",
            "990/990 [==============================] - 1s 779us/step - loss: 6.1941 - val_loss: 6.2119\n",
            "Epoch 113/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 6.1941 - val_loss: 6.2114\n",
            "Epoch 114/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1939 - val_loss: 6.2101\n",
            "Epoch 115/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1945 - val_loss: 6.2107\n",
            "Epoch 116/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 6.1941 - val_loss: 6.2104\n",
            "Epoch 117/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1939 - val_loss: 6.2113\n",
            "Epoch 118/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 6.1940 - val_loss: 6.2110\n",
            "Epoch 119/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1945 - val_loss: 6.2106\n",
            "Epoch 120/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1945 - val_loss: 6.2100\n",
            "Epoch 121/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 6.1943 - val_loss: 6.2112\n",
            "Epoch 122/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 6.1944 - val_loss: 6.2090\n",
            "Epoch 123/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1945 - val_loss: 6.2107\n",
            "Epoch 124/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1942 - val_loss: 6.2102\n",
            "Epoch 125/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1943 - val_loss: 6.2108\n",
            "Epoch 126/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1941 - val_loss: 6.2125\n",
            "Epoch 127/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 6.1940 - val_loss: 6.2117\n",
            "Epoch 128/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1940 - val_loss: 6.2107\n",
            "Epoch 129/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1941 - val_loss: 6.2117\n",
            "Epoch 130/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 6.1942 - val_loss: 6.2104\n",
            "Epoch 131/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 6.1940 - val_loss: 6.2108\n",
            "Epoch 132/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 6.1944 - val_loss: 6.2118\n",
            "Epoch 133/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1940 - val_loss: 6.2101\n",
            "Epoch 134/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1942 - val_loss: 6.2101\n",
            "Epoch 135/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1940 - val_loss: 6.2102\n",
            "Epoch 136/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1942 - val_loss: 6.2100\n",
            "Epoch 137/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1940 - val_loss: 6.2097\n",
            "Epoch 138/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1940 - val_loss: 6.2112\n",
            "Epoch 139/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 6.1942 - val_loss: 6.2095\n",
            "Epoch 140/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 6.1940 - val_loss: 6.2108\n",
            "Epoch 141/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1944 - val_loss: 6.2101\n",
            "Epoch 142/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1943 - val_loss: 6.2097\n",
            "Epoch 143/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1942 - val_loss: 6.2120\n",
            "Epoch 144/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 6.1941 - val_loss: 6.2102\n",
            "Epoch 145/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 6.1944 - val_loss: 6.2107\n",
            "Epoch 146/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1942 - val_loss: 6.2093\n",
            "Epoch 147/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1942 - val_loss: 6.2101\n",
            "Epoch 148/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1942 - val_loss: 6.2099\n",
            "Epoch 149/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1941 - val_loss: 6.2108\n",
            "Epoch 150/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 6.1940 - val_loss: 6.2098\n",
            "Epoch 151/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 6.1942 - val_loss: 6.2106\n",
            "Epoch 152/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1943 - val_loss: 6.2112\n",
            "Epoch 153/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 6.1940 - val_loss: 6.2104\n",
            "Epoch 154/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 6.1940 - val_loss: 6.2114\n",
            "Epoch 155/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 6.1941 - val_loss: 6.2109\n",
            "Epoch 156/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1939 - val_loss: 6.2111\n",
            "Epoch 157/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1940 - val_loss: 6.2108\n",
            "Epoch 158/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 6.1941 - val_loss: 6.2118\n",
            "Epoch 159/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1939 - val_loss: 6.2107\n",
            "Epoch 160/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 6.1941 - val_loss: 6.2097\n",
            "Epoch 161/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1943 - val_loss: 6.2107\n",
            "Epoch 162/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1942 - val_loss: 6.2104\n",
            "Epoch 163/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1943 - val_loss: 6.2104\n",
            "Epoch 164/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 6.1940 - val_loss: 6.2092\n",
            "Epoch 165/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 6.1943 - val_loss: 6.2117\n",
            "Epoch 166/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1943 - val_loss: 6.2107\n",
            "Epoch 167/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 6.1939 - val_loss: 6.2112\n",
            "Epoch 168/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 6.1939 - val_loss: 6.2106\n",
            "Epoch 169/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 6.1939 - val_loss: 6.2100\n",
            "Epoch 170/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1944 - val_loss: 6.2101\n",
            "Epoch 171/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1944 - val_loss: 6.2104\n",
            "Epoch 172/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1940 - val_loss: 6.2095\n",
            "Epoch 173/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1941 - val_loss: 6.2101\n",
            "Epoch 174/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1941 - val_loss: 6.2108\n",
            "Epoch 175/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 6.1940 - val_loss: 6.2112\n",
            "Epoch 176/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1938 - val_loss: 6.2103\n",
            "Epoch 177/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1942 - val_loss: 6.2096\n",
            "Epoch 178/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1940 - val_loss: 6.2113\n",
            "Epoch 179/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 6.1943 - val_loss: 6.2096\n",
            "Epoch 180/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1940 - val_loss: 6.2108\n",
            "Epoch 181/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1941 - val_loss: 6.2100\n",
            "Epoch 182/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 6.1941 - val_loss: 6.2108\n",
            "Epoch 183/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 6.1940 - val_loss: 6.2099\n",
            "Epoch 184/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1938 - val_loss: 6.2115\n",
            "Epoch 185/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 6.1938 - val_loss: 6.2103\n",
            "Epoch 186/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1942 - val_loss: 6.2088\n",
            "Epoch 187/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 6.1941 - val_loss: 6.2099\n",
            "Epoch 188/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 6.1942 - val_loss: 6.2098\n",
            "Epoch 189/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1939 - val_loss: 6.2107\n",
            "Epoch 190/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1940 - val_loss: 6.2099\n",
            "Epoch 191/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1941 - val_loss: 6.2110\n",
            "Epoch 192/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 6.1941 - val_loss: 6.2092\n",
            "Epoch 193/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1940 - val_loss: 6.2096\n",
            "Epoch 194/500\n",
            "990/990 [==============================] - 1s 786us/step - loss: 6.1939 - val_loss: 6.2092\n",
            "Epoch 195/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1939 - val_loss: 6.2088\n",
            "Epoch 196/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1941 - val_loss: 6.2088\n",
            "Epoch 197/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 6.1941 - val_loss: 6.2096\n",
            "Epoch 198/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 6.1943 - val_loss: 6.2091\n",
            "Epoch 199/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 6.1939 - val_loss: 6.2094\n",
            "Epoch 200/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 6.1942 - val_loss: 6.2103\n",
            "Epoch 201/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 6.1944 - val_loss: 6.2101\n",
            "Epoch 202/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1940 - val_loss: 6.2105\n",
            "Epoch 203/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1940 - val_loss: 6.2112\n",
            "Epoch 204/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1939 - val_loss: 6.2103\n",
            "Epoch 205/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 6.1940 - val_loss: 6.2105\n",
            "Epoch 206/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 6.1942 - val_loss: 6.2095\n",
            "Epoch 207/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1943 - val_loss: 6.2111\n",
            "Epoch 208/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1938 - val_loss: 6.2108\n",
            "Epoch 209/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1941 - val_loss: 6.2098\n",
            "Epoch 210/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 6.1939 - val_loss: 6.2092\n",
            "Epoch 211/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1940 - val_loss: 6.2093\n",
            "Epoch 212/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1943 - val_loss: 6.2100\n",
            "Epoch 213/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 6.1939 - val_loss: 6.2104\n",
            "Epoch 214/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1940 - val_loss: 6.2108\n",
            "Epoch 215/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 6.1940 - val_loss: 6.2100\n",
            "Epoch 216/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1939 - val_loss: 6.2091\n",
            "Epoch 217/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1939 - val_loss: 6.2106\n",
            "Epoch 218/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 6.1939 - val_loss: 6.2108\n",
            "Epoch 219/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1939 - val_loss: 6.2102\n",
            "Epoch 220/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1941 - val_loss: 6.2114\n",
            "Epoch 221/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1939 - val_loss: 6.2105\n",
            "Epoch 222/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1940 - val_loss: 6.2104\n",
            "Epoch 223/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1942 - val_loss: 6.2111\n",
            "Epoch 224/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1941 - val_loss: 6.2102\n",
            "Epoch 225/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 6.1939 - val_loss: 6.2108\n",
            "Epoch 226/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 6.1940 - val_loss: 6.2100\n",
            "Epoch 227/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 6.1943 - val_loss: 6.2119\n",
            "Epoch 228/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1940 - val_loss: 6.2099\n",
            "Epoch 229/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1942 - val_loss: 6.2111\n",
            "Epoch 230/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1940 - val_loss: 6.2122\n",
            "Epoch 231/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1940 - val_loss: 6.2116\n",
            "Epoch 232/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1938 - val_loss: 6.2104\n",
            "Epoch 233/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 6.1939 - val_loss: 6.2124\n",
            "Epoch 234/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1940 - val_loss: 6.2107\n",
            "Epoch 235/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 6.1944 - val_loss: 6.2111\n",
            "Epoch 236/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 6.1943 - val_loss: 6.2111\n",
            "Epoch 237/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 6.1942 - val_loss: 6.2139\n",
            "Epoch 238/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 6.1943 - val_loss: 6.2134\n",
            "Epoch 239/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1942 - val_loss: 6.2137\n",
            "Epoch 240/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 6.1942 - val_loss: 6.2130\n",
            "Epoch 241/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 6.1939 - val_loss: 6.2113\n",
            "Epoch 242/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1941 - val_loss: 6.2128\n",
            "Epoch 243/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 6.1942 - val_loss: 6.2127\n",
            "Epoch 244/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 6.1945 - val_loss: 6.2136\n",
            "Epoch 245/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1940 - val_loss: 6.2113\n",
            "Epoch 246/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 6.1941 - val_loss: 6.2118\n",
            "Epoch 247/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 6.1941 - val_loss: 6.2122\n",
            "Epoch 248/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1939 - val_loss: 6.2141\n",
            "Epoch 249/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 6.1939 - val_loss: 6.2118\n",
            "Epoch 250/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1941 - val_loss: 6.2130\n",
            "Epoch 251/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1942 - val_loss: 6.2136\n",
            "Epoch 252/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 6.1943 - val_loss: 6.2136\n",
            "Epoch 253/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 6.1944 - val_loss: 6.2129\n",
            "Epoch 254/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1943 - val_loss: 6.2121\n",
            "Epoch 255/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1938 - val_loss: 6.2120\n",
            "Epoch 256/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1938 - val_loss: 6.2130\n",
            "Epoch 257/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1942 - val_loss: 6.2126\n",
            "Epoch 258/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1940 - val_loss: 6.2123\n",
            "Epoch 259/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1943 - val_loss: 6.2144\n",
            "Epoch 260/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1940 - val_loss: 6.2116\n",
            "Epoch 261/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1942 - val_loss: 6.2122\n",
            "Epoch 262/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 6.1943 - val_loss: 6.2117\n",
            "Epoch 263/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1942 - val_loss: 6.2135\n",
            "Epoch 264/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 6.1941 - val_loss: 6.2128\n",
            "Epoch 265/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 6.1942 - val_loss: 6.2126\n",
            "Epoch 266/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1943 - val_loss: 6.2139\n",
            "Epoch 267/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1943 - val_loss: 6.2139\n",
            "Epoch 268/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1942 - val_loss: 6.2123\n",
            "Epoch 269/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1943 - val_loss: 6.2159\n",
            "Epoch 270/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 6.1941 - val_loss: 6.2139\n",
            "Epoch 271/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1940 - val_loss: 6.2154\n",
            "Epoch 272/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 6.1942 - val_loss: 6.2154\n",
            "Epoch 273/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 6.1940 - val_loss: 6.2136\n",
            "Epoch 274/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 6.1940 - val_loss: 6.2149\n",
            "Epoch 275/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1940 - val_loss: 6.2140\n",
            "Epoch 276/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 6.1939 - val_loss: 6.2139\n",
            "Epoch 277/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1942 - val_loss: 6.2138\n",
            "Epoch 278/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 6.1940 - val_loss: 6.2150\n",
            "Epoch 279/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1939 - val_loss: 6.2141\n",
            "Epoch 280/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 6.1940 - val_loss: 6.2130\n",
            "Epoch 281/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1938 - val_loss: 6.2149\n",
            "Epoch 282/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1941 - val_loss: 6.2125\n",
            "Epoch 283/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 6.1940 - val_loss: 6.2141\n",
            "Epoch 284/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1940 - val_loss: 6.2133\n",
            "Epoch 285/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 6.1943 - val_loss: 6.2153\n",
            "Epoch 286/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1943 - val_loss: 6.2138\n",
            "Epoch 287/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1940 - val_loss: 6.2108\n",
            "Epoch 288/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1940 - val_loss: 6.2124\n",
            "Epoch 289/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1940 - val_loss: 6.2129\n",
            "Epoch 290/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 6.1940 - val_loss: 6.2121\n",
            "Epoch 291/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1941 - val_loss: 6.2165\n",
            "Epoch 292/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1942 - val_loss: 6.2135\n",
            "Epoch 293/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 6.1938 - val_loss: 6.2123\n",
            "Epoch 294/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1939 - val_loss: 6.2146\n",
            "Epoch 295/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 6.1941 - val_loss: 6.2122\n",
            "Epoch 296/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1940 - val_loss: 6.2135\n",
            "Epoch 297/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 6.1943 - val_loss: 6.2149\n",
            "Epoch 298/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1939 - val_loss: 6.2127\n",
            "Epoch 299/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1944 - val_loss: 6.2127\n",
            "Epoch 300/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1940 - val_loss: 6.2148\n",
            "Epoch 301/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1939 - val_loss: 6.2139\n",
            "Epoch 302/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 6.1939 - val_loss: 6.2119\n",
            "Epoch 303/500\n",
            "990/990 [==============================] - 1s 730us/step - loss: 6.1939 - val_loss: 6.2135\n",
            "Epoch 304/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1940 - val_loss: 6.2121\n",
            "Epoch 305/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1940 - val_loss: 6.2122\n",
            "Epoch 306/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 6.1940 - val_loss: 6.2144\n",
            "Epoch 307/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 6.1942 - val_loss: 6.2137\n",
            "Epoch 308/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 6.1939 - val_loss: 6.2154\n",
            "Epoch 309/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1938 - val_loss: 6.2130\n",
            "Epoch 310/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1939 - val_loss: 6.2142\n",
            "Epoch 311/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1941 - val_loss: 6.2153\n",
            "Epoch 312/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 6.1943 - val_loss: 6.2080\n",
            "Epoch 313/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1940 - val_loss: 6.2078\n",
            "Epoch 314/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1939 - val_loss: 6.2079\n",
            "Epoch 315/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 6.1938 - val_loss: 6.2091\n",
            "Epoch 316/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 6.1941 - val_loss: 6.2075\n",
            "Epoch 317/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1941 - val_loss: 6.2072\n",
            "Epoch 318/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1940 - val_loss: 6.2109\n",
            "Epoch 319/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 6.1938 - val_loss: 6.2082\n",
            "Epoch 320/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 6.1940 - val_loss: 6.2081\n",
            "Epoch 321/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1940 - val_loss: 6.2087\n",
            "Epoch 322/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1943 - val_loss: 6.2083\n",
            "Epoch 323/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1940 - val_loss: 6.2071\n",
            "Epoch 324/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1939 - val_loss: 6.2092\n",
            "Epoch 325/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 6.1939 - val_loss: 6.2084\n",
            "Epoch 326/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1943 - val_loss: 6.2079\n",
            "Epoch 327/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 6.1940 - val_loss: 6.2065\n",
            "Epoch 328/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 6.1941 - val_loss: 6.2121\n",
            "Epoch 329/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 6.1941 - val_loss: 6.2093\n",
            "Epoch 330/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 6.1941 - val_loss: 6.2096\n",
            "Epoch 331/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 6.1938 - val_loss: 6.2088\n",
            "Epoch 332/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1941 - val_loss: 6.2094\n",
            "Epoch 333/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 6.1942 - val_loss: 6.2083\n",
            "Epoch 334/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1942 - val_loss: 6.2078\n",
            "Epoch 335/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 6.1940 - val_loss: 6.2085\n",
            "Epoch 336/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1939 - val_loss: 6.2091\n",
            "Epoch 337/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1939 - val_loss: 6.2097\n",
            "Epoch 338/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 6.1940 - val_loss: 6.2084\n",
            "Epoch 339/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 6.1940 - val_loss: 6.2094\n",
            "Epoch 340/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1942 - val_loss: 6.2097\n",
            "Epoch 341/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 6.1941 - val_loss: 6.2104\n",
            "Epoch 342/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1941 - val_loss: 6.2102\n",
            "Epoch 343/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 6.1941 - val_loss: 6.2100\n",
            "Epoch 344/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1943 - val_loss: 6.2094\n",
            "Epoch 345/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 6.1943 - val_loss: 6.2074\n",
            "Epoch 346/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1939 - val_loss: 6.2090\n",
            "Epoch 347/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1941 - val_loss: 6.2087\n",
            "Epoch 348/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 6.1942 - val_loss: 6.2089\n",
            "Epoch 349/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1942 - val_loss: 6.2084\n",
            "Epoch 350/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 6.1939 - val_loss: 6.2078\n",
            "Epoch 351/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1940 - val_loss: 6.2092\n",
            "Epoch 352/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1941 - val_loss: 6.2091\n",
            "Epoch 353/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 6.1939 - val_loss: 6.2090\n",
            "Epoch 354/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1941 - val_loss: 6.2085\n",
            "Epoch 355/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 6.1939 - val_loss: 6.2083\n",
            "Epoch 356/500\n",
            "990/990 [==============================] - 1s 783us/step - loss: 6.1940 - val_loss: 6.2087\n",
            "Epoch 357/500\n",
            "990/990 [==============================] - 1s 733us/step - loss: 6.1937 - val_loss: 6.2111\n",
            "Epoch 358/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 6.1938 - val_loss: 6.2076\n",
            "Epoch 359/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1940 - val_loss: 6.2085\n",
            "Epoch 360/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 6.1941 - val_loss: 6.2088\n",
            "Epoch 361/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1938 - val_loss: 6.2095\n",
            "Epoch 362/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 6.1941 - val_loss: 6.2086\n",
            "Epoch 363/500\n",
            "990/990 [==============================] - 1s 779us/step - loss: 6.1940 - val_loss: 6.2079\n",
            "Epoch 364/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1941 - val_loss: 6.2089\n",
            "Epoch 365/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1942 - val_loss: 6.2080\n",
            "Epoch 366/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1938 - val_loss: 6.2088\n",
            "Epoch 367/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1939 - val_loss: 6.2080\n",
            "Epoch 368/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1938 - val_loss: 6.2074\n",
            "Epoch 369/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1939 - val_loss: 6.2081\n",
            "Epoch 370/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1941 - val_loss: 6.2085\n",
            "Epoch 371/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1939 - val_loss: 6.2090\n",
            "Epoch 372/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 6.1938 - val_loss: 6.2081\n",
            "Epoch 373/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1940 - val_loss: 6.2093\n",
            "Epoch 374/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1938 - val_loss: 6.2082\n",
            "Epoch 375/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 6.1939 - val_loss: 6.2102\n",
            "Epoch 376/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1940 - val_loss: 6.2076\n",
            "Epoch 377/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 6.1940 - val_loss: 6.2093\n",
            "Epoch 378/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1942 - val_loss: 6.2084\n",
            "Epoch 379/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1939 - val_loss: 6.2106\n",
            "Epoch 380/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 6.1939 - val_loss: 6.2112\n",
            "Epoch 381/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1940 - val_loss: 6.2130\n",
            "Epoch 382/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 6.1940 - val_loss: 6.2085\n",
            "Epoch 383/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 6.1944 - val_loss: 6.2105\n",
            "Epoch 384/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1941 - val_loss: 6.2093\n",
            "Epoch 385/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 6.1940 - val_loss: 6.2092\n",
            "Epoch 386/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1941 - val_loss: 6.2113\n",
            "Epoch 387/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 6.1941 - val_loss: 6.2087\n",
            "Epoch 388/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1941 - val_loss: 6.2066\n",
            "Epoch 389/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1942 - val_loss: 6.2075\n",
            "Epoch 390/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 6.1941 - val_loss: 6.2056\n",
            "Epoch 391/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 6.1940 - val_loss: 6.2091\n",
            "Epoch 392/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1940 - val_loss: 6.2080\n",
            "Epoch 393/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 6.1938 - val_loss: 6.2085\n",
            "Epoch 394/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1942 - val_loss: 6.2078\n",
            "Epoch 395/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1940 - val_loss: 6.2087\n",
            "Epoch 396/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1940 - val_loss: 6.2080\n",
            "Epoch 397/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 6.1942 - val_loss: 6.2103\n",
            "Epoch 398/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1939 - val_loss: 6.2076\n",
            "Epoch 399/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1943 - val_loss: 6.2084\n",
            "Epoch 400/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 6.1940 - val_loss: 6.2078\n",
            "Epoch 401/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 6.1941 - val_loss: 6.2082\n",
            "Epoch 402/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 6.1942 - val_loss: 6.2082\n",
            "Epoch 403/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 6.1940 - val_loss: 6.2084\n",
            "Epoch 404/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1941 - val_loss: 6.2090\n",
            "Epoch 405/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1940 - val_loss: 6.2088\n",
            "Epoch 406/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 6.1938 - val_loss: 6.2059\n",
            "Epoch 407/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 6.1940 - val_loss: 6.2087\n",
            "Epoch 408/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 6.1940 - val_loss: 6.2108\n",
            "Epoch 409/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 6.1940 - val_loss: 6.2109\n",
            "Epoch 410/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1938 - val_loss: 6.2091\n",
            "Epoch 411/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 6.1940 - val_loss: 6.2084\n",
            "Epoch 412/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1940 - val_loss: 6.2096\n",
            "Epoch 413/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1939 - val_loss: 6.2088\n",
            "Epoch 414/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 6.1940 - val_loss: 6.2071\n",
            "Epoch 415/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 6.1940 - val_loss: 6.2100\n",
            "Epoch 416/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 6.1939 - val_loss: 6.2078\n",
            "Epoch 417/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 6.1942 - val_loss: 6.2087\n",
            "Epoch 418/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1940 - val_loss: 6.2084\n",
            "Epoch 419/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 6.1940 - val_loss: 6.2090\n",
            "Epoch 420/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1941 - val_loss: 6.2084\n",
            "Epoch 421/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1939 - val_loss: 6.2097\n",
            "Epoch 422/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 6.1942 - val_loss: 6.2088\n",
            "Epoch 423/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1938 - val_loss: 6.2096\n",
            "Epoch 424/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1938 - val_loss: 6.2083\n",
            "Epoch 425/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 6.1941 - val_loss: 6.2083\n",
            "Epoch 426/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1939 - val_loss: 6.2101\n",
            "Epoch 427/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 6.1938 - val_loss: 6.2089\n",
            "Epoch 428/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1940 - val_loss: 6.2082\n",
            "Epoch 429/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1944 - val_loss: 6.2081\n",
            "Epoch 430/500\n",
            "990/990 [==============================] - 1s 734us/step - loss: 6.1940 - val_loss: 6.2084\n",
            "Epoch 431/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 6.1940 - val_loss: 6.2074\n",
            "Epoch 432/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 6.1940 - val_loss: 6.2094\n",
            "Epoch 433/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 6.1943 - val_loss: 6.2104\n",
            "Epoch 434/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1938 - val_loss: 6.2092\n",
            "Epoch 435/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1942 - val_loss: 6.2098\n",
            "Epoch 436/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 6.1940 - val_loss: 6.2094\n",
            "Epoch 437/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1940 - val_loss: 6.2079\n",
            "Epoch 438/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 6.1938 - val_loss: 6.2098\n",
            "Epoch 439/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 6.1942 - val_loss: 6.2083\n",
            "Epoch 440/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1939 - val_loss: 6.2085\n",
            "Epoch 441/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1939 - val_loss: 6.2105\n",
            "Epoch 442/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1942 - val_loss: 6.2087\n",
            "Epoch 443/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1940 - val_loss: 6.2095\n",
            "Epoch 444/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 6.1941 - val_loss: 6.2100\n",
            "Epoch 445/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1941 - val_loss: 6.2079\n",
            "Epoch 446/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 6.1941 - val_loss: 6.2093\n",
            "Epoch 447/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1943 - val_loss: 6.2091\n",
            "Epoch 448/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1941 - val_loss: 6.2086\n",
            "Epoch 449/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1939 - val_loss: 6.2116\n",
            "Epoch 450/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1939 - val_loss: 6.2096\n",
            "Epoch 451/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 6.1938 - val_loss: 6.2102\n",
            "Epoch 452/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 6.1940 - val_loss: 6.2082\n",
            "Epoch 453/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 6.1939 - val_loss: 6.2094\n",
            "Epoch 454/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1936 - val_loss: 6.2092\n",
            "Epoch 455/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1940 - val_loss: 6.2082\n",
            "Epoch 456/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 6.1941 - val_loss: 6.2117\n",
            "Epoch 457/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 6.1941 - val_loss: 6.2099\n",
            "Epoch 458/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1940 - val_loss: 6.2080\n",
            "Epoch 459/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1939 - val_loss: 6.2084\n",
            "Epoch 460/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1938 - val_loss: 6.2098\n",
            "Epoch 461/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1937 - val_loss: 6.2103\n",
            "Epoch 462/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 6.1937 - val_loss: 6.2100\n",
            "Epoch 463/500\n",
            "990/990 [==============================] - 1s 779us/step - loss: 6.1939 - val_loss: 6.2109\n",
            "Epoch 464/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1939 - val_loss: 6.2080\n",
            "Epoch 465/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 6.1942 - val_loss: 6.2095\n",
            "Epoch 466/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 6.1940 - val_loss: 6.2108\n",
            "Epoch 467/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1940 - val_loss: 6.2092\n",
            "Epoch 468/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1943 - val_loss: 6.2090\n",
            "Epoch 469/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1939 - val_loss: 6.2115\n",
            "Epoch 470/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1939 - val_loss: 6.2096\n",
            "Epoch 471/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 6.1938 - val_loss: 6.2098\n",
            "Epoch 472/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1941 - val_loss: 6.2097\n",
            "Epoch 473/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1940 - val_loss: 6.2106\n",
            "Epoch 474/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1939 - val_loss: 6.2114\n",
            "Epoch 475/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 6.1939 - val_loss: 6.2070\n",
            "Epoch 476/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 6.1939 - val_loss: 6.2082\n",
            "Epoch 477/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 6.1939 - val_loss: 6.2090\n",
            "Epoch 478/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 6.1938 - val_loss: 6.2092\n",
            "Epoch 479/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 6.1939 - val_loss: 6.2100\n",
            "Epoch 480/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 6.1938 - val_loss: 6.2092\n",
            "Epoch 481/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1939 - val_loss: 6.2128\n",
            "Epoch 482/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1941 - val_loss: 6.2100\n",
            "Epoch 483/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 6.1939 - val_loss: 6.2096\n",
            "Epoch 484/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1939 - val_loss: 6.2069\n",
            "Epoch 485/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 6.1938 - val_loss: 6.2093\n",
            "Epoch 486/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 6.1936 - val_loss: 6.2114\n",
            "Epoch 487/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 6.1941 - val_loss: 6.2098\n",
            "Epoch 488/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1938 - val_loss: 6.2091\n",
            "Epoch 489/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 6.1941 - val_loss: 6.2089\n",
            "Epoch 490/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 6.1940 - val_loss: 6.2089\n",
            "Epoch 491/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 6.1938 - val_loss: 6.2082\n",
            "Epoch 492/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 6.1942 - val_loss: 6.2085\n",
            "Epoch 493/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 6.1940 - val_loss: 6.2079\n",
            "Epoch 494/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 6.1937 - val_loss: 6.2104\n",
            "Epoch 495/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 6.1939 - val_loss: 6.2112\n",
            "Epoch 496/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 6.1941 - val_loss: 6.2085\n",
            "Epoch 497/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 6.1941 - val_loss: 6.2089\n",
            "Epoch 498/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 6.1939 - val_loss: 6.2089\n",
            "Epoch 499/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 6.1940 - val_loss: 6.2092\n",
            "Epoch 500/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 6.1937 - val_loss: 6.2104\n",
            "(lamda,Threshold) 0.5 0.25\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (1100, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 42647 0.5\n",
            "The shape of N (1100, 3072)\n",
            "The minimum value of N  -0.7029118537902832\n",
            "The max value of N 0.6037582159042358\n",
            "[INFO:] Xclean  MSE Computed shape (1100, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (1100, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0061614686, 0.0052109957, 0.0058396035, 0.005597862])\n",
            "[INFO:] The anomaly threshold computed is  0.005597862\n",
            "[INFO:] The shape of input data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (1100, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [1052, 1063, 1081, 1071, 1067, 1092, 1090, 1076, 1080, 1088, 1060, 1056, 1057, 1098, 473, 1087, 1079, 1089, 1083, 1091, 1069, 1065, 1042, 1075, 1095, 1018, 1073, 1086, 1066, 1050, 1094, 1054, 1093, 1040, 1072, 1061, 1070, 997, 1023, 1062, 1077, 1068, 1085, 1064, 1025, 1027, 991, 1074, 614, 1049]\n",
            "[INFO:] The  Positive boundry 1050\n",
            "[INFO:] The  Negative boundry 50\n",
            "[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n",
            "img shape: (160, 320, 3)\n",
            "\n",
            "Saving Top-10 most anomalous digit: @\n",
            "=====================\n",
            "AUROC 0.5 0.89\n",
            "=======================\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 32) (256, 32)\n",
            "Train on 990 samples, validate on 110 samples\n",
            "Epoch 1/500\n",
            "990/990 [==============================] - 10s 10ms/step - loss: 10.2656 - val_loss: 10.2814\n",
            "Epoch 2/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2654 - val_loss: 10.2807\n",
            "Epoch 3/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 10.2654 - val_loss: 10.2831\n",
            "Epoch 4/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2654 - val_loss: 10.2842\n",
            "Epoch 5/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 10.2655 - val_loss: 10.2827\n",
            "Epoch 6/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 10.2654 - val_loss: 10.2826\n",
            "Epoch 7/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 10.2653 - val_loss: 10.2814\n",
            "Epoch 8/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2654 - val_loss: 10.2846\n",
            "Epoch 9/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2653 - val_loss: 10.2824\n",
            "Epoch 10/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 10.2653 - val_loss: 10.2810\n",
            "Epoch 11/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 10.2654 - val_loss: 10.2792\n",
            "Epoch 12/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2652 - val_loss: 10.2795\n",
            "Epoch 13/500\n",
            "990/990 [==============================] - 1s 779us/step - loss: 10.2653 - val_loss: 10.2798\n",
            "Epoch 14/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2651 - val_loss: 10.2801\n",
            "Epoch 15/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2656 - val_loss: 10.2803\n",
            "Epoch 16/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 10.2651 - val_loss: 10.2792\n",
            "Epoch 17/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2653 - val_loss: 10.2811\n",
            "Epoch 18/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2655 - val_loss: 10.2799\n",
            "Epoch 19/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2654 - val_loss: 10.2799\n",
            "Epoch 20/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 10.2652 - val_loss: 10.2802\n",
            "Epoch 21/500\n",
            "990/990 [==============================] - 1s 783us/step - loss: 10.2653 - val_loss: 10.2799\n",
            "Epoch 22/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2655 - val_loss: 10.2801\n",
            "Epoch 23/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2652 - val_loss: 10.2787\n",
            "Epoch 24/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2652 - val_loss: 10.2793\n",
            "Epoch 25/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2653 - val_loss: 10.2805\n",
            "Epoch 26/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 10.2653 - val_loss: 10.2810\n",
            "Epoch 27/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2652 - val_loss: 10.2806\n",
            "Epoch 28/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2654 - val_loss: 10.2824\n",
            "Epoch 29/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2655 - val_loss: 10.2801\n",
            "Epoch 30/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2654 - val_loss: 10.2822\n",
            "Epoch 31/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 10.2653 - val_loss: 10.2790\n",
            "Epoch 32/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2653 - val_loss: 10.2794\n",
            "Epoch 33/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2654 - val_loss: 10.2801\n",
            "Epoch 34/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2654 - val_loss: 10.2796\n",
            "Epoch 35/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 10.2653 - val_loss: 10.2806\n",
            "Epoch 36/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 10.2653 - val_loss: 10.2811\n",
            "Epoch 37/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2653 - val_loss: 10.2806\n",
            "Epoch 38/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2653 - val_loss: 10.2796\n",
            "Epoch 39/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2652 - val_loss: 10.2796\n",
            "Epoch 40/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2651 - val_loss: 10.2793\n",
            "Epoch 41/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2652 - val_loss: 10.2801\n",
            "Epoch 42/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2655 - val_loss: 10.2809\n",
            "Epoch 43/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2652 - val_loss: 10.2793\n",
            "Epoch 44/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2653 - val_loss: 10.2797\n",
            "Epoch 45/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2657 - val_loss: 10.2789\n",
            "Epoch 46/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 10.2654 - val_loss: 10.2811\n",
            "Epoch 47/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2653 - val_loss: 10.2791\n",
            "Epoch 48/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 10.2654 - val_loss: 10.2802\n",
            "Epoch 49/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 10.2653 - val_loss: 10.2823\n",
            "Epoch 50/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2654 - val_loss: 10.2804\n",
            "Epoch 51/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2652 - val_loss: 10.2811\n",
            "Epoch 52/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 10.2653 - val_loss: 10.2783\n",
            "Epoch 53/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2655 - val_loss: 10.2805\n",
            "Epoch 54/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 10.2654 - val_loss: 10.2798\n",
            "Epoch 55/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2653 - val_loss: 10.2799\n",
            "Epoch 56/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 10.2653 - val_loss: 10.2786\n",
            "Epoch 57/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2652 - val_loss: 10.2788\n",
            "Epoch 58/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 10.2655 - val_loss: 10.2812\n",
            "Epoch 59/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 10.2653 - val_loss: 10.2809\n",
            "Epoch 60/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2655 - val_loss: 10.2790\n",
            "Epoch 61/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 10.2650 - val_loss: 10.2790\n",
            "Epoch 62/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2653 - val_loss: 10.2809\n",
            "Epoch 63/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2653 - val_loss: 10.2784\n",
            "Epoch 64/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2652 - val_loss: 10.2803\n",
            "Epoch 65/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2651 - val_loss: 10.2820\n",
            "Epoch 66/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 10.2652 - val_loss: 10.2791\n",
            "Epoch 67/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 10.2656 - val_loss: 10.2813\n",
            "Epoch 68/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2655 - val_loss: 10.2795\n",
            "Epoch 69/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 10.2655 - val_loss: 10.2795\n",
            "Epoch 70/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2654 - val_loss: 10.2797\n",
            "Epoch 71/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2654 - val_loss: 10.2796\n",
            "Epoch 72/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2652 - val_loss: 10.2803\n",
            "Epoch 73/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 10.2653 - val_loss: 10.2812\n",
            "Epoch 74/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 10.2655 - val_loss: 10.2806\n",
            "Epoch 75/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 10.2655 - val_loss: 10.2790\n",
            "Epoch 76/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2653 - val_loss: 10.2785\n",
            "Epoch 77/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 10.2654 - val_loss: 10.2777\n",
            "Epoch 78/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2655 - val_loss: 10.2800\n",
            "Epoch 79/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2653 - val_loss: 10.2793\n",
            "Epoch 80/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2654 - val_loss: 10.2795\n",
            "Epoch 81/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 10.2654 - val_loss: 10.2797\n",
            "Epoch 82/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 10.2654 - val_loss: 10.2812\n",
            "Epoch 83/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2652 - val_loss: 10.2808\n",
            "Epoch 84/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2651 - val_loss: 10.2801\n",
            "Epoch 85/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2653 - val_loss: 10.2796\n",
            "Epoch 86/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2653 - val_loss: 10.2791\n",
            "Epoch 87/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2654 - val_loss: 10.2813\n",
            "Epoch 88/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2655 - val_loss: 10.2782\n",
            "Epoch 89/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2651 - val_loss: 10.2791\n",
            "Epoch 90/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2652 - val_loss: 10.2802\n",
            "Epoch 91/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2651 - val_loss: 10.2794\n",
            "Epoch 92/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2652 - val_loss: 10.2796\n",
            "Epoch 93/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2651 - val_loss: 10.2795\n",
            "Epoch 94/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2654 - val_loss: 10.2801\n",
            "Epoch 95/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2656 - val_loss: 10.2804\n",
            "Epoch 96/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2652 - val_loss: 10.2817\n",
            "Epoch 97/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 10.2652 - val_loss: 10.2794\n",
            "Epoch 98/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 10.2653 - val_loss: 10.2792\n",
            "Epoch 99/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2650 - val_loss: 10.2806\n",
            "Epoch 100/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 10.2653 - val_loss: 10.2782\n",
            "Epoch 101/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2653 - val_loss: 10.2804\n",
            "Epoch 102/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2652 - val_loss: 10.2797\n",
            "Epoch 103/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2655 - val_loss: 10.2801\n",
            "Epoch 104/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 10.2654 - val_loss: 10.2798\n",
            "Epoch 105/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 10.2655 - val_loss: 10.2793\n",
            "Epoch 106/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 10.2652 - val_loss: 10.2812\n",
            "Epoch 107/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2655 - val_loss: 10.2818\n",
            "Epoch 108/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 10.2655 - val_loss: 10.2812\n",
            "Epoch 109/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2653 - val_loss: 10.2792\n",
            "Epoch 110/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 10.2652 - val_loss: 10.2816\n",
            "Epoch 111/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 10.2653 - val_loss: 10.2807\n",
            "Epoch 112/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 10.2653 - val_loss: 10.2814\n",
            "Epoch 113/500\n",
            "990/990 [==============================] - 1s 729us/step - loss: 10.2653 - val_loss: 10.2799\n",
            "Epoch 114/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2653 - val_loss: 10.2806\n",
            "Epoch 115/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 10.2652 - val_loss: 10.2814\n",
            "Epoch 116/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2652 - val_loss: 10.2797\n",
            "Epoch 117/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2656 - val_loss: 10.2816\n",
            "Epoch 118/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 10.2651 - val_loss: 10.2808\n",
            "Epoch 119/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 10.2656 - val_loss: 10.2808\n",
            "Epoch 120/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2653 - val_loss: 10.2802\n",
            "Epoch 121/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2653 - val_loss: 10.2806\n",
            "Epoch 122/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2650 - val_loss: 10.2807\n",
            "Epoch 123/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 10.2653 - val_loss: 10.2803\n",
            "Epoch 124/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2654 - val_loss: 10.2815\n",
            "Epoch 125/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2653 - val_loss: 10.2806\n",
            "Epoch 126/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2655 - val_loss: 10.2822\n",
            "Epoch 127/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2653 - val_loss: 10.2816\n",
            "Epoch 128/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 10.2654 - val_loss: 10.2813\n",
            "Epoch 129/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2655 - val_loss: 10.2807\n",
            "Epoch 130/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2656 - val_loss: 10.2786\n",
            "Epoch 131/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2653 - val_loss: 10.2811\n",
            "Epoch 132/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2653 - val_loss: 10.2812\n",
            "Epoch 133/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 10.2651 - val_loss: 10.2807\n",
            "Epoch 134/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2653 - val_loss: 10.2817\n",
            "Epoch 135/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 10.2650 - val_loss: 10.2795\n",
            "Epoch 136/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2652 - val_loss: 10.2802\n",
            "Epoch 137/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 10.2656 - val_loss: 10.2798\n",
            "Epoch 138/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2655 - val_loss: 10.2823\n",
            "Epoch 139/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 10.2654 - val_loss: 10.2803\n",
            "Epoch 140/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 10.2654 - val_loss: 10.2809\n",
            "Epoch 141/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 10.2651 - val_loss: 10.2779\n",
            "Epoch 142/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 10.2651 - val_loss: 10.2804\n",
            "Epoch 143/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2652 - val_loss: 10.2802\n",
            "Epoch 144/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2657 - val_loss: 10.2802\n",
            "Epoch 145/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 10.2654 - val_loss: 10.2787\n",
            "Epoch 146/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2652 - val_loss: 10.2803\n",
            "Epoch 147/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2653 - val_loss: 10.2811\n",
            "Epoch 148/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 10.2652 - val_loss: 10.2820\n",
            "Epoch 149/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 10.2654 - val_loss: 10.2805\n",
            "Epoch 150/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 10.2651 - val_loss: 10.2795\n",
            "Epoch 151/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2653 - val_loss: 10.2796\n",
            "Epoch 152/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2654 - val_loss: 10.2785\n",
            "Epoch 153/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2651 - val_loss: 10.2802\n",
            "Epoch 154/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2652 - val_loss: 10.2803\n",
            "Epoch 155/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2653 - val_loss: 10.2797\n",
            "Epoch 156/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2651 - val_loss: 10.2798\n",
            "Epoch 157/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2652 - val_loss: 10.2815\n",
            "Epoch 158/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 10.2654 - val_loss: 10.2815\n",
            "Epoch 159/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 10.2655 - val_loss: 10.2797\n",
            "Epoch 160/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 10.2654 - val_loss: 10.2802\n",
            "Epoch 161/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 10.2652 - val_loss: 10.2820\n",
            "Epoch 162/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 10.2653 - val_loss: 10.2811\n",
            "Epoch 163/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2652 - val_loss: 10.2810\n",
            "Epoch 164/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2653 - val_loss: 10.2805\n",
            "Epoch 165/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 10.2654 - val_loss: 10.2804\n",
            "Epoch 166/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 10.2652 - val_loss: 10.2813\n",
            "Epoch 167/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2655 - val_loss: 10.2810\n",
            "Epoch 168/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 10.2655 - val_loss: 10.2815\n",
            "Epoch 169/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 10.2653 - val_loss: 10.2803\n",
            "Epoch 170/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2654 - val_loss: 10.2804\n",
            "Epoch 171/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 10.2658 - val_loss: 10.2804\n",
            "Epoch 172/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2654 - val_loss: 10.2810\n",
            "Epoch 173/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 10.2653 - val_loss: 10.2809\n",
            "Epoch 174/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2655 - val_loss: 10.2817\n",
            "Epoch 175/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2651 - val_loss: 10.2804\n",
            "Epoch 176/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 10.2653 - val_loss: 10.2807\n",
            "Epoch 177/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2654 - val_loss: 10.2823\n",
            "Epoch 178/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 10.2653 - val_loss: 10.2817\n",
            "Epoch 179/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 10.2652 - val_loss: 10.2800\n",
            "Epoch 180/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 10.2651 - val_loss: 10.2817\n",
            "Epoch 181/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 10.2653 - val_loss: 10.2822\n",
            "Epoch 182/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 10.2651 - val_loss: 10.2813\n",
            "Epoch 183/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 10.2653 - val_loss: 10.2810\n",
            "Epoch 184/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 10.2655 - val_loss: 10.2816\n",
            "Epoch 185/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 10.2653 - val_loss: 10.2838\n",
            "Epoch 186/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 10.2653 - val_loss: 10.2807\n",
            "Epoch 187/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2654 - val_loss: 10.2835\n",
            "Epoch 188/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2653 - val_loss: 10.2827\n",
            "Epoch 189/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2652 - val_loss: 10.2809\n",
            "Epoch 190/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2652 - val_loss: 10.2812\n",
            "Epoch 191/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2657 - val_loss: 10.2817\n",
            "Epoch 192/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 10.2655 - val_loss: 10.2828\n",
            "Epoch 193/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 10.2652 - val_loss: 10.2812\n",
            "Epoch 194/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2651 - val_loss: 10.2834\n",
            "Epoch 195/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 10.2652 - val_loss: 10.2818\n",
            "Epoch 196/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2652 - val_loss: 10.2815\n",
            "Epoch 197/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2651 - val_loss: 10.2813\n",
            "Epoch 198/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 10.2652 - val_loss: 10.2806\n",
            "Epoch 199/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 10.2653 - val_loss: 10.2818\n",
            "Epoch 200/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 10.2652 - val_loss: 10.2824\n",
            "Epoch 201/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 10.2652 - val_loss: 10.2831\n",
            "Epoch 202/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2652 - val_loss: 10.2816\n",
            "Epoch 203/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2652 - val_loss: 10.2834\n",
            "Epoch 204/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2652 - val_loss: 10.2818\n",
            "Epoch 205/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 10.2656 - val_loss: 10.2816\n",
            "Epoch 206/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2656 - val_loss: 10.2799\n",
            "Epoch 207/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 10.2653 - val_loss: 10.2815\n",
            "Epoch 208/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2651 - val_loss: 10.2815\n",
            "Epoch 209/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 10.2653 - val_loss: 10.2815\n",
            "Epoch 210/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2652 - val_loss: 10.2815\n",
            "Epoch 211/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 10.2653 - val_loss: 10.2846\n",
            "Epoch 212/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2652 - val_loss: 10.2817\n",
            "Epoch 213/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 10.2652 - val_loss: 10.2815\n",
            "Epoch 214/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2651 - val_loss: 10.2803\n",
            "Epoch 215/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2652 - val_loss: 10.2799\n",
            "Epoch 216/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2652 - val_loss: 10.2800\n",
            "Epoch 217/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2652 - val_loss: 10.2819\n",
            "Epoch 218/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2652 - val_loss: 10.2807\n",
            "Epoch 219/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2653 - val_loss: 10.2808\n",
            "Epoch 220/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 10.2651 - val_loss: 10.2803\n",
            "Epoch 221/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 10.2654 - val_loss: 10.2801\n",
            "Epoch 222/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 10.2652 - val_loss: 10.2818\n",
            "Epoch 223/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2650 - val_loss: 10.2814\n",
            "Epoch 224/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2656 - val_loss: 10.2803\n",
            "Epoch 225/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 10.2654 - val_loss: 10.2821\n",
            "Epoch 226/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 10.2652 - val_loss: 10.2818\n",
            "Epoch 227/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2651 - val_loss: 10.2813\n",
            "Epoch 228/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2652 - val_loss: 10.2805\n",
            "Epoch 229/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 10.2653 - val_loss: 10.2822\n",
            "Epoch 230/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2653 - val_loss: 10.2823\n",
            "Epoch 231/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 10.2654 - val_loss: 10.2809\n",
            "Epoch 232/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2652 - val_loss: 10.2825\n",
            "Epoch 233/500\n",
            "990/990 [==============================] - 1s 732us/step - loss: 10.2652 - val_loss: 10.2822\n",
            "Epoch 234/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2654 - val_loss: 10.2791\n",
            "Epoch 235/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 10.2652 - val_loss: 10.2822\n",
            "Epoch 236/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2652 - val_loss: 10.2825\n",
            "Epoch 237/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2654 - val_loss: 10.2814\n",
            "Epoch 238/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2653 - val_loss: 10.2798\n",
            "Epoch 239/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2651 - val_loss: 10.2809\n",
            "Epoch 240/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 10.2653 - val_loss: 10.2807\n",
            "Epoch 241/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 10.2653 - val_loss: 10.2833\n",
            "Epoch 242/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2651 - val_loss: 10.2813\n",
            "Epoch 243/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2652 - val_loss: 10.2816\n",
            "Epoch 244/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2652 - val_loss: 10.2817\n",
            "Epoch 245/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2652 - val_loss: 10.2800\n",
            "Epoch 246/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2652 - val_loss: 10.2807\n",
            "Epoch 247/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2652 - val_loss: 10.2817\n",
            "Epoch 248/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 10.2650 - val_loss: 10.2821\n",
            "Epoch 249/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2651 - val_loss: 10.2811\n",
            "Epoch 250/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2653 - val_loss: 10.2821\n",
            "Epoch 251/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2653 - val_loss: 10.2826\n",
            "Epoch 252/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2653 - val_loss: 10.2810\n",
            "Epoch 253/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2654 - val_loss: 10.2832\n",
            "Epoch 254/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2652 - val_loss: 10.2810\n",
            "Epoch 255/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2651 - val_loss: 10.2808\n",
            "Epoch 256/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2651 - val_loss: 10.2810\n",
            "Epoch 257/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2650 - val_loss: 10.2818\n",
            "Epoch 258/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2650 - val_loss: 10.2821\n",
            "Epoch 259/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 10.2654 - val_loss: 10.2814\n",
            "Epoch 260/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 10.2651 - val_loss: 10.2817\n",
            "Epoch 261/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 10.2654 - val_loss: 10.2822\n",
            "Epoch 262/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2651 - val_loss: 10.2824\n",
            "Epoch 263/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2653 - val_loss: 10.2816\n",
            "Epoch 264/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2655 - val_loss: 10.2825\n",
            "Epoch 265/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2651 - val_loss: 10.2830\n",
            "Epoch 266/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2651 - val_loss: 10.2813\n",
            "Epoch 267/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2651 - val_loss: 10.2833\n",
            "Epoch 268/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 10.2655 - val_loss: 10.2814\n",
            "Epoch 269/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2651 - val_loss: 10.2808\n",
            "Epoch 270/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 10.2652 - val_loss: 10.2822\n",
            "Epoch 271/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2656 - val_loss: 10.2818\n",
            "Epoch 272/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2653 - val_loss: 10.2813\n",
            "Epoch 273/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2652 - val_loss: 10.2812\n",
            "Epoch 274/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2655 - val_loss: 10.2809\n",
            "Epoch 275/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 10.2650 - val_loss: 10.2827\n",
            "Epoch 276/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 10.2654 - val_loss: 10.2815\n",
            "Epoch 277/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2653 - val_loss: 10.2801\n",
            "Epoch 278/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 10.2654 - val_loss: 10.2826\n",
            "Epoch 279/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 10.2653 - val_loss: 10.2821\n",
            "Epoch 280/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2654 - val_loss: 10.2807\n",
            "Epoch 281/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2651 - val_loss: 10.2828\n",
            "Epoch 282/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2654 - val_loss: 10.2817\n",
            "Epoch 283/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2655 - val_loss: 10.2831\n",
            "Epoch 284/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2653 - val_loss: 10.2847\n",
            "Epoch 285/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 10.2654 - val_loss: 10.2812\n",
            "Epoch 286/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2651 - val_loss: 10.2830\n",
            "Epoch 287/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 10.2652 - val_loss: 10.2823\n",
            "Epoch 288/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 10.2653 - val_loss: 10.2833\n",
            "Epoch 289/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2652 - val_loss: 10.2825\n",
            "Epoch 290/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2653 - val_loss: 10.2842\n",
            "Epoch 291/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2651 - val_loss: 10.2813\n",
            "Epoch 292/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2653 - val_loss: 10.2847\n",
            "Epoch 293/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2652 - val_loss: 10.2824\n",
            "Epoch 294/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2653 - val_loss: 10.2831\n",
            "Epoch 295/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2653 - val_loss: 10.2819\n",
            "Epoch 296/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2651 - val_loss: 10.2835\n",
            "Epoch 297/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2654 - val_loss: 10.2835\n",
            "Epoch 298/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 10.2655 - val_loss: 10.2824\n",
            "Epoch 299/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2652 - val_loss: 10.2829\n",
            "Epoch 300/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2652 - val_loss: 10.2815\n",
            "Epoch 301/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2657 - val_loss: 10.2823\n",
            "Epoch 302/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2654 - val_loss: 10.2816\n",
            "Epoch 303/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 10.2651 - val_loss: 10.2843\n",
            "Epoch 304/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2657 - val_loss: 10.2804\n",
            "Epoch 305/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 10.2654 - val_loss: 10.2824\n",
            "Epoch 306/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2652 - val_loss: 10.2838\n",
            "Epoch 307/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2652 - val_loss: 10.2843\n",
            "Epoch 308/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2653 - val_loss: 10.2825\n",
            "Epoch 309/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2652 - val_loss: 10.2818\n",
            "Epoch 310/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2652 - val_loss: 10.2842\n",
            "Epoch 311/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2653 - val_loss: 10.2811\n",
            "Epoch 312/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 10.2653 - val_loss: 10.2839\n",
            "Epoch 313/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2653 - val_loss: 10.2814\n",
            "Epoch 314/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 10.2653 - val_loss: 10.2831\n",
            "Epoch 315/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2651 - val_loss: 10.2825\n",
            "Epoch 316/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 10.2651 - val_loss: 10.2829\n",
            "Epoch 317/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2652 - val_loss: 10.2825\n",
            "Epoch 318/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 10.2654 - val_loss: 10.2829\n",
            "Epoch 319/500\n",
            "990/990 [==============================] - 1s 785us/step - loss: 10.2651 - val_loss: 10.2840\n",
            "Epoch 320/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2651 - val_loss: 10.2827\n",
            "Epoch 321/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2652 - val_loss: 10.2832\n",
            "Epoch 322/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 10.2654 - val_loss: 10.2814\n",
            "Epoch 323/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2651 - val_loss: 10.2853\n",
            "Epoch 324/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2651 - val_loss: 10.2815\n",
            "Epoch 325/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2655 - val_loss: 10.2832\n",
            "Epoch 326/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2652 - val_loss: 10.2837\n",
            "Epoch 327/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2650 - val_loss: 10.2845\n",
            "Epoch 328/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 10.2653 - val_loss: 10.2827\n",
            "Epoch 329/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2656 - val_loss: 10.2843\n",
            "Epoch 330/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2652 - val_loss: 10.2808\n",
            "Epoch 331/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 10.2652 - val_loss: 10.2838\n",
            "Epoch 332/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2655 - val_loss: 10.2825\n",
            "Epoch 333/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2650 - val_loss: 10.2837\n",
            "Epoch 334/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2650 - val_loss: 10.2827\n",
            "Epoch 335/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2655 - val_loss: 10.2808\n",
            "Epoch 336/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2652 - val_loss: 10.2835\n",
            "Epoch 337/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2651 - val_loss: 10.2818\n",
            "Epoch 338/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 10.2653 - val_loss: 10.2844\n",
            "Epoch 339/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2653 - val_loss: 10.2828\n",
            "Epoch 340/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 10.2654 - val_loss: 10.2824\n",
            "Epoch 341/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 10.2655 - val_loss: 10.2830\n",
            "Epoch 342/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2653 - val_loss: 10.2817\n",
            "Epoch 343/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2652 - val_loss: 10.2845\n",
            "Epoch 344/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2652 - val_loss: 10.2825\n",
            "Epoch 345/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2652 - val_loss: 10.2833\n",
            "Epoch 346/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2656 - val_loss: 10.2832\n",
            "Epoch 347/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2654 - val_loss: 10.2811\n",
            "Epoch 348/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 10.2655 - val_loss: 10.2821\n",
            "Epoch 349/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2654 - val_loss: 10.2815\n",
            "Epoch 350/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2653 - val_loss: 10.2829\n",
            "Epoch 351/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 10.2655 - val_loss: 10.2813\n",
            "Epoch 352/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 10.2654 - val_loss: 10.2828\n",
            "Epoch 353/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 10.2653 - val_loss: 10.2838\n",
            "Epoch 354/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2654 - val_loss: 10.2832\n",
            "Epoch 355/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 10.2652 - val_loss: 10.2833\n",
            "Epoch 356/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2654 - val_loss: 10.2826\n",
            "Epoch 357/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2652 - val_loss: 10.2839\n",
            "Epoch 358/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 10.2652 - val_loss: 10.2820\n",
            "Epoch 359/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2651 - val_loss: 10.2831\n",
            "Epoch 360/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2651 - val_loss: 10.2816\n",
            "Epoch 361/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 10.2652 - val_loss: 10.2817\n",
            "Epoch 362/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 10.2653 - val_loss: 10.2806\n",
            "Epoch 363/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2652 - val_loss: 10.2815\n",
            "Epoch 364/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 10.2650 - val_loss: 10.2822\n",
            "Epoch 365/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2652 - val_loss: 10.2820\n",
            "Epoch 366/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2652 - val_loss: 10.2828\n",
            "Epoch 367/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 10.2653 - val_loss: 10.2803\n",
            "Epoch 368/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2651 - val_loss: 10.2832\n",
            "Epoch 369/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2654 - val_loss: 10.2843\n",
            "Epoch 370/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 10.2654 - val_loss: 10.2839\n",
            "Epoch 371/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2653 - val_loss: 10.2805\n",
            "Epoch 372/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2651 - val_loss: 10.2823\n",
            "Epoch 373/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 10.2651 - val_loss: 10.2818\n",
            "Epoch 374/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2652 - val_loss: 10.2829\n",
            "Epoch 375/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 10.2652 - val_loss: 10.2824\n",
            "Epoch 376/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2651 - val_loss: 10.2843\n",
            "Epoch 377/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 10.2652 - val_loss: 10.2825\n",
            "Epoch 378/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 10.2650 - val_loss: 10.2830\n",
            "Epoch 379/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2652 - val_loss: 10.2830\n",
            "Epoch 380/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 10.2651 - val_loss: 10.2816\n",
            "Epoch 381/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 10.2655 - val_loss: 10.2842\n",
            "Epoch 382/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2654 - val_loss: 10.2826\n",
            "Epoch 383/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2653 - val_loss: 10.2804\n",
            "Epoch 384/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 10.2652 - val_loss: 10.2816\n",
            "Epoch 385/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2652 - val_loss: 10.2815\n",
            "Epoch 386/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2653 - val_loss: 10.2842\n",
            "Epoch 387/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2653 - val_loss: 10.2823\n",
            "Epoch 388/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 10.2651 - val_loss: 10.2824\n",
            "Epoch 389/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 10.2652 - val_loss: 10.2814\n",
            "Epoch 390/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 10.2654 - val_loss: 10.2807\n",
            "Epoch 391/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2653 - val_loss: 10.2834\n",
            "Epoch 392/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2654 - val_loss: 10.2814\n",
            "Epoch 393/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2653 - val_loss: 10.2825\n",
            "Epoch 394/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2654 - val_loss: 10.2837\n",
            "Epoch 395/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2651 - val_loss: 10.2822\n",
            "Epoch 396/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 10.2651 - val_loss: 10.2807\n",
            "Epoch 397/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2651 - val_loss: 10.2809\n",
            "Epoch 398/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2652 - val_loss: 10.2794\n",
            "Epoch 399/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2651 - val_loss: 10.2826\n",
            "Epoch 400/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 10.2652 - val_loss: 10.2829\n",
            "Epoch 401/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2653 - val_loss: 10.2812\n",
            "Epoch 402/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 10.2653 - val_loss: 10.2851\n",
            "Epoch 403/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2652 - val_loss: 10.2833\n",
            "Epoch 404/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2651 - val_loss: 10.2836\n",
            "Epoch 405/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2654 - val_loss: 10.2815\n",
            "Epoch 406/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 10.2651 - val_loss: 10.2828\n",
            "Epoch 407/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2652 - val_loss: 10.2817\n",
            "Epoch 408/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 10.2651 - val_loss: 10.2823\n",
            "Epoch 409/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2654 - val_loss: 10.2827\n",
            "Epoch 410/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 10.2651 - val_loss: 10.2819\n",
            "Epoch 411/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2654 - val_loss: 10.2823\n",
            "Epoch 412/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 10.2652 - val_loss: 10.2848\n",
            "Epoch 413/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 10.2654 - val_loss: 10.2825\n",
            "Epoch 414/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2654 - val_loss: 10.2820\n",
            "Epoch 415/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 10.2653 - val_loss: 10.2842\n",
            "Epoch 416/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 10.2651 - val_loss: 10.2830\n",
            "Epoch 417/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2653 - val_loss: 10.2829\n",
            "Epoch 418/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2651 - val_loss: 10.2820\n",
            "Epoch 419/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2655 - val_loss: 10.2825\n",
            "Epoch 420/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2652 - val_loss: 10.2826\n",
            "Epoch 421/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2653 - val_loss: 10.2845\n",
            "Epoch 422/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 10.2651 - val_loss: 10.2838\n",
            "Epoch 423/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 10.2653 - val_loss: 10.2830\n",
            "Epoch 424/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 10.2652 - val_loss: 10.2832\n",
            "Epoch 425/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 10.2654 - val_loss: 10.2825\n",
            "Epoch 426/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 10.2652 - val_loss: 10.2832\n",
            "Epoch 427/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2653 - val_loss: 10.2836\n",
            "Epoch 428/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2651 - val_loss: 10.2816\n",
            "Epoch 429/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2652 - val_loss: 10.2841\n",
            "Epoch 430/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2652 - val_loss: 10.2815\n",
            "Epoch 431/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2652 - val_loss: 10.2823\n",
            "Epoch 432/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2650 - val_loss: 10.2827\n",
            "Epoch 433/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 10.2653 - val_loss: 10.2837\n",
            "Epoch 434/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 10.2654 - val_loss: 10.2820\n",
            "Epoch 435/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 10.2653 - val_loss: 10.2844\n",
            "Epoch 436/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 10.2653 - val_loss: 10.2823\n",
            "Epoch 437/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 10.2652 - val_loss: 10.2839\n",
            "Epoch 438/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2651 - val_loss: 10.2836\n",
            "Epoch 439/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2651 - val_loss: 10.2824\n",
            "Epoch 440/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 10.2652 - val_loss: 10.2818\n",
            "Epoch 441/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2651 - val_loss: 10.2827\n",
            "Epoch 442/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2652 - val_loss: 10.2829\n",
            "Epoch 443/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2653 - val_loss: 10.2849\n",
            "Epoch 444/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 10.2652 - val_loss: 10.2811\n",
            "Epoch 445/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2651 - val_loss: 10.2846\n",
            "Epoch 446/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 10.2651 - val_loss: 10.2830\n",
            "Epoch 447/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2653 - val_loss: 10.2853\n",
            "Epoch 448/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2650 - val_loss: 10.2828\n",
            "Epoch 449/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 10.2650 - val_loss: 10.2836\n",
            "Epoch 450/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 10.2655 - val_loss: 10.2840\n",
            "Epoch 451/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 10.2653 - val_loss: 10.2826\n",
            "Epoch 452/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2655 - val_loss: 10.2840\n",
            "Epoch 453/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 10.2653 - val_loss: 10.2847\n",
            "Epoch 454/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2655 - val_loss: 10.2851\n",
            "Epoch 455/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 10.2650 - val_loss: 10.2832\n",
            "Epoch 456/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2651 - val_loss: 10.2837\n",
            "Epoch 457/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 10.2650 - val_loss: 10.2826\n",
            "Epoch 458/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 10.2649 - val_loss: 10.2842\n",
            "Epoch 459/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 10.2652 - val_loss: 10.2849\n",
            "Epoch 460/500\n",
            "990/990 [==============================] - 1s 739us/step - loss: 10.2653 - val_loss: 10.2832\n",
            "Epoch 461/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 10.2651 - val_loss: 10.2849\n",
            "Epoch 462/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2652 - val_loss: 10.2831\n",
            "Epoch 463/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 10.2652 - val_loss: 10.2830\n",
            "Epoch 464/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 10.2653 - val_loss: 10.2824\n",
            "Epoch 465/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2652 - val_loss: 10.2822\n",
            "Epoch 466/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 10.2651 - val_loss: 10.2834\n",
            "Epoch 467/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2654 - val_loss: 10.2818\n",
            "Epoch 468/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2652 - val_loss: 10.2831\n",
            "Epoch 469/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2652 - val_loss: 10.2837\n",
            "Epoch 470/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2652 - val_loss: 10.2853\n",
            "Epoch 471/500\n",
            "990/990 [==============================] - 1s 783us/step - loss: 10.2652 - val_loss: 10.2821\n",
            "Epoch 472/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 10.2653 - val_loss: 10.2857\n",
            "Epoch 473/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 10.2651 - val_loss: 10.2843\n",
            "Epoch 474/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 10.2654 - val_loss: 10.2829\n",
            "Epoch 475/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 10.2653 - val_loss: 10.2843\n",
            "Epoch 476/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 10.2655 - val_loss: 10.2837\n",
            "Epoch 477/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 10.2651 - val_loss: 10.2846\n",
            "Epoch 478/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2650 - val_loss: 10.2838\n",
            "Epoch 479/500\n",
            "990/990 [==============================] - 1s 804us/step - loss: 10.2651 - val_loss: 10.2829\n",
            "Epoch 480/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 10.2650 - val_loss: 10.2826\n",
            "Epoch 481/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 10.2653 - val_loss: 10.2851\n",
            "Epoch 482/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2653 - val_loss: 10.2823\n",
            "Epoch 483/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 10.2652 - val_loss: 10.2824\n",
            "Epoch 484/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 10.2650 - val_loss: 10.2830\n",
            "Epoch 485/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2653 - val_loss: 10.2825\n",
            "Epoch 486/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2654 - val_loss: 10.2832\n",
            "Epoch 487/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2654 - val_loss: 10.2826\n",
            "Epoch 488/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 10.2652 - val_loss: 10.2848\n",
            "Epoch 489/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2651 - val_loss: 10.2845\n",
            "Epoch 490/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 10.2652 - val_loss: 10.2850\n",
            "Epoch 491/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2651 - val_loss: 10.2836\n",
            "Epoch 492/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 10.2652 - val_loss: 10.2829\n",
            "Epoch 493/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 10.2658 - val_loss: 10.2833\n",
            "Epoch 494/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 10.2654 - val_loss: 10.2844\n",
            "Epoch 495/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 10.2652 - val_loss: 10.2849\n",
            "Epoch 496/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 10.2652 - val_loss: 10.2826\n",
            "Epoch 497/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 10.2653 - val_loss: 10.2845\n",
            "Epoch 498/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 10.2652 - val_loss: 10.2832\n",
            "Epoch 499/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 10.2651 - val_loss: 10.2838\n",
            "Epoch 500/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 10.2651 - val_loss: 10.2834\n",
            "(lamda,Threshold) 1.0 0.5\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (1100, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 6334 1.0\n",
            "The shape of N (1100, 3072)\n",
            "The minimum value of N  -0.4641662836074829\n",
            "The max value of N 0.35666710138320923\n",
            "[INFO:] Xclean  MSE Computed shape (1100, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (1100, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0061614686, 0.0052109957, 0.0058396035, 0.005597862, 0.0055936617])\n",
            "[INFO:] The anomaly threshold computed is  0.0055936617\n",
            "[INFO:] The shape of input data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (1100, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [1052, 1063, 1081, 1067, 1071, 1090, 1092, 1076, 1080, 1056, 1088, 1098, 1091, 1060, 1057, 1089, 1087, 1065, 473, 1079, 1069, 1042, 1066, 1073, 1095, 1050, 1018, 1075, 1086, 1072, 1093, 1040, 1083, 997, 1070, 1077, 1054, 1094, 1064, 1061, 1023, 1025, 1074, 1053, 1029, 1062, 1085, 381, 991, 1049]\n",
            "[INFO:] The  Positive boundry 1050\n",
            "[INFO:] The  Negative boundry 50\n",
            "[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n",
            "img shape: (160, 320, 3)\n",
            "\n",
            "Saving Top-10 most anomalous digit: @\n",
            "=====================\n",
            "AUROC 1.0 0.89\n",
            "=======================\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 32) (256, 32)\n",
            "Train on 990 samples, validate on 110 samples\n",
            "Epoch 1/500\n",
            "990/990 [==============================] - 10s 10ms/step - loss: 8.2778 - val_loss: 8.2971\n",
            "Epoch 2/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2776 - val_loss: 8.2957\n",
            "Epoch 3/500\n",
            "990/990 [==============================] - 1s 779us/step - loss: 8.2774 - val_loss: 8.2953\n",
            "Epoch 4/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2775 - val_loss: 8.2957\n",
            "Epoch 5/500\n",
            "990/990 [==============================] - 1s 784us/step - loss: 8.2775 - val_loss: 8.2945\n",
            "Epoch 6/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2777 - val_loss: 8.2959\n",
            "Epoch 7/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 8.2776 - val_loss: 8.2957\n",
            "Epoch 8/500\n",
            "990/990 [==============================] - 1s 781us/step - loss: 8.2776 - val_loss: 8.2965\n",
            "Epoch 9/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 8.2775 - val_loss: 8.2974\n",
            "Epoch 10/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 8.2775 - val_loss: 8.2991\n",
            "Epoch 11/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2776 - val_loss: 8.2984\n",
            "Epoch 12/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2774 - val_loss: 8.2973\n",
            "Epoch 13/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 8.2774 - val_loss: 8.2963\n",
            "Epoch 14/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2774 - val_loss: 8.2974\n",
            "Epoch 15/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 8.2776 - val_loss: 8.2958\n",
            "Epoch 16/500\n",
            "990/990 [==============================] - 1s 781us/step - loss: 8.2777 - val_loss: 8.2972\n",
            "Epoch 17/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2774 - val_loss: 8.2970\n",
            "Epoch 18/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2775 - val_loss: 8.2982\n",
            "Epoch 19/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2776 - val_loss: 8.2949\n",
            "Epoch 20/500\n",
            "990/990 [==============================] - 1s 793us/step - loss: 8.2775 - val_loss: 8.2965\n",
            "Epoch 21/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 8.2774 - val_loss: 8.2973\n",
            "Epoch 22/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2775 - val_loss: 8.2982\n",
            "Epoch 23/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 8.2776 - val_loss: 8.2947\n",
            "Epoch 24/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2775 - val_loss: 8.2967\n",
            "Epoch 25/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2776 - val_loss: 8.2973\n",
            "Epoch 26/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 8.2775 - val_loss: 8.2973\n",
            "Epoch 27/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2776 - val_loss: 8.2994\n",
            "Epoch 28/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2774 - val_loss: 8.2956\n",
            "Epoch 29/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 8.2772 - val_loss: 8.2971\n",
            "Epoch 30/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2775 - val_loss: 8.2971\n",
            "Epoch 31/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 8.2773 - val_loss: 8.2968\n",
            "Epoch 32/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2774 - val_loss: 8.2982\n",
            "Epoch 33/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 8.2775 - val_loss: 8.2964\n",
            "Epoch 34/500\n",
            "990/990 [==============================] - 1s 779us/step - loss: 8.2776 - val_loss: 8.2976\n",
            "Epoch 35/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 8.2775 - val_loss: 8.2969\n",
            "Epoch 36/500\n",
            "990/990 [==============================] - 1s 786us/step - loss: 8.2775 - val_loss: 8.2955\n",
            "Epoch 37/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2775 - val_loss: 8.2959\n",
            "Epoch 38/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2775 - val_loss: 8.2960\n",
            "Epoch 39/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 8.2775 - val_loss: 8.2949\n",
            "Epoch 40/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2776 - val_loss: 8.2961\n",
            "Epoch 41/500\n",
            "990/990 [==============================] - 1s 784us/step - loss: 8.2773 - val_loss: 8.2970\n",
            "Epoch 42/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 8.2775 - val_loss: 8.2959\n",
            "Epoch 43/500\n",
            "990/990 [==============================] - 1s 784us/step - loss: 8.2776 - val_loss: 8.2968\n",
            "Epoch 44/500\n",
            "990/990 [==============================] - 1s 779us/step - loss: 8.2777 - val_loss: 8.2981\n",
            "Epoch 45/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2773 - val_loss: 8.2957\n",
            "Epoch 46/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 8.2774 - val_loss: 8.2975\n",
            "Epoch 47/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 8.2776 - val_loss: 8.2976\n",
            "Epoch 48/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2775 - val_loss: 8.2977\n",
            "Epoch 49/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 8.2776 - val_loss: 8.2976\n",
            "Epoch 50/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2775 - val_loss: 8.2959\n",
            "Epoch 51/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 8.2776 - val_loss: 8.2947\n",
            "Epoch 52/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 8.2776 - val_loss: 8.2930\n",
            "Epoch 53/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 8.2777 - val_loss: 8.2951\n",
            "Epoch 54/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2776 - val_loss: 8.2945\n",
            "Epoch 55/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2773 - val_loss: 8.2920\n",
            "Epoch 56/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2775 - val_loss: 8.2912\n",
            "Epoch 57/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2774 - val_loss: 8.2926\n",
            "Epoch 58/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 8.2776 - val_loss: 8.2918\n",
            "Epoch 59/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2776 - val_loss: 8.2930\n",
            "Epoch 60/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 8.2775 - val_loss: 8.2930\n",
            "Epoch 61/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2774 - val_loss: 8.2950\n",
            "Epoch 62/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2777 - val_loss: 8.2949\n",
            "Epoch 63/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2776 - val_loss: 8.2941\n",
            "Epoch 64/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2775 - val_loss: 8.2944\n",
            "Epoch 65/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2774 - val_loss: 8.2950\n",
            "Epoch 66/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2776 - val_loss: 8.2959\n",
            "Epoch 67/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 8.2774 - val_loss: 8.2957\n",
            "Epoch 68/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2775 - val_loss: 8.2953\n",
            "Epoch 69/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2775 - val_loss: 8.2946\n",
            "Epoch 70/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 8.2776 - val_loss: 8.2971\n",
            "Epoch 71/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2773 - val_loss: 8.2961\n",
            "Epoch 72/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2775 - val_loss: 8.2972\n",
            "Epoch 73/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 8.2773 - val_loss: 8.2952\n",
            "Epoch 74/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 8.2775 - val_loss: 8.2984\n",
            "Epoch 75/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2776 - val_loss: 8.2974\n",
            "Epoch 76/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2777 - val_loss: 8.2959\n",
            "Epoch 77/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2779 - val_loss: 8.2983\n",
            "Epoch 78/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2779 - val_loss: 8.2906\n",
            "Epoch 79/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2778 - val_loss: 8.2892\n",
            "Epoch 80/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 8.2780 - val_loss: 8.2890\n",
            "Epoch 81/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2777 - val_loss: 8.2882\n",
            "Epoch 82/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 8.2776 - val_loss: 8.2886\n",
            "Epoch 83/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2777 - val_loss: 8.2891\n",
            "Epoch 84/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2775 - val_loss: 8.2887\n",
            "Epoch 85/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2774 - val_loss: 8.2889\n",
            "Epoch 86/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2775 - val_loss: 8.2883\n",
            "Epoch 87/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 8.2775 - val_loss: 8.2893\n",
            "Epoch 88/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2776 - val_loss: 8.2897\n",
            "Epoch 89/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2776 - val_loss: 8.2893\n",
            "Epoch 90/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2775 - val_loss: 8.2895\n",
            "Epoch 91/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 8.2777 - val_loss: 8.2898\n",
            "Epoch 92/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2774 - val_loss: 8.2888\n",
            "Epoch 93/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 8.2775 - val_loss: 8.2893\n",
            "Epoch 94/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2776 - val_loss: 8.2893\n",
            "Epoch 95/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2775 - val_loss: 8.2895\n",
            "Epoch 96/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 8.2775 - val_loss: 8.2900\n",
            "Epoch 97/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 8.2774 - val_loss: 8.2891\n",
            "Epoch 98/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 8.2774 - val_loss: 8.2900\n",
            "Epoch 99/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2775 - val_loss: 8.2895\n",
            "Epoch 100/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2777 - val_loss: 8.2894\n",
            "Epoch 101/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 8.2774 - val_loss: 8.2896\n",
            "Epoch 102/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2775 - val_loss: 8.2892\n",
            "Epoch 103/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 8.2774 - val_loss: 8.2893\n",
            "Epoch 104/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2775 - val_loss: 8.2889\n",
            "Epoch 105/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 8.2775 - val_loss: 8.2897\n",
            "Epoch 106/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2775 - val_loss: 8.2902\n",
            "Epoch 107/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 8.2775 - val_loss: 8.2894\n",
            "Epoch 108/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2774 - val_loss: 8.2898\n",
            "Epoch 109/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2776 - val_loss: 8.2894\n",
            "Epoch 110/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 8.2775 - val_loss: 8.2899\n",
            "Epoch 111/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 8.2774 - val_loss: 8.2901\n",
            "Epoch 112/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2776 - val_loss: 8.2897\n",
            "Epoch 113/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2773 - val_loss: 8.2897\n",
            "Epoch 114/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 8.2773 - val_loss: 8.2905\n",
            "Epoch 115/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 8.2775 - val_loss: 8.2898\n",
            "Epoch 116/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2774 - val_loss: 8.2897\n",
            "Epoch 117/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2773 - val_loss: 8.2903\n",
            "Epoch 118/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2777 - val_loss: 8.2895\n",
            "Epoch 119/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2777 - val_loss: 8.2901\n",
            "Epoch 120/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 8.2775 - val_loss: 8.2900\n",
            "Epoch 121/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2773 - val_loss: 8.2893\n",
            "Epoch 122/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 8.2774 - val_loss: 8.2900\n",
            "Epoch 123/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2774 - val_loss: 8.2900\n",
            "Epoch 124/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 8.2774 - val_loss: 8.2902\n",
            "Epoch 125/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 8.2774 - val_loss: 8.2899\n",
            "Epoch 126/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2776 - val_loss: 8.2896\n",
            "Epoch 127/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2776 - val_loss: 8.2900\n",
            "Epoch 128/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2778 - val_loss: 8.2907\n",
            "Epoch 129/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 8.2774 - val_loss: 8.2898\n",
            "Epoch 130/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 8.2775 - val_loss: 8.2907\n",
            "Epoch 131/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 8.2774 - val_loss: 8.2895\n",
            "Epoch 132/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2774 - val_loss: 8.2891\n",
            "Epoch 133/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2776 - val_loss: 8.2904\n",
            "Epoch 134/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 8.2773 - val_loss: 8.2900\n",
            "Epoch 135/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2774 - val_loss: 8.2900\n",
            "Epoch 136/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 8.2773 - val_loss: 8.2890\n",
            "Epoch 137/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 8.2775 - val_loss: 8.2898\n",
            "Epoch 138/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2775 - val_loss: 8.2896\n",
            "Epoch 139/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2774 - val_loss: 8.2897\n",
            "Epoch 140/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 8.2776 - val_loss: 8.2900\n",
            "Epoch 141/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 8.2774 - val_loss: 8.2889\n",
            "Epoch 142/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2774 - val_loss: 8.2903\n",
            "Epoch 143/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2775 - val_loss: 8.2898\n",
            "Epoch 144/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 8.2775 - val_loss: 8.2905\n",
            "Epoch 145/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2775 - val_loss: 8.2896\n",
            "Epoch 146/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2773 - val_loss: 8.2903\n",
            "Epoch 147/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2775 - val_loss: 8.2901\n",
            "Epoch 148/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2775 - val_loss: 8.2904\n",
            "Epoch 149/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 8.2773 - val_loss: 8.2899\n",
            "Epoch 150/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2772 - val_loss: 8.2900\n",
            "Epoch 151/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2774 - val_loss: 8.2902\n",
            "Epoch 152/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 8.2773 - val_loss: 8.2892\n",
            "Epoch 153/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 8.2776 - val_loss: 8.2904\n",
            "Epoch 154/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2781 - val_loss: 8.2901\n",
            "Epoch 155/500\n",
            "990/990 [==============================] - 1s 734us/step - loss: 8.2777 - val_loss: 8.2901\n",
            "Epoch 156/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2777 - val_loss: 8.2909\n",
            "Epoch 157/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2776 - val_loss: 8.2901\n",
            "Epoch 158/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 8.2774 - val_loss: 8.2904\n",
            "Epoch 159/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 8.2773 - val_loss: 8.2900\n",
            "Epoch 160/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 8.2773 - val_loss: 8.2894\n",
            "Epoch 161/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 8.2774 - val_loss: 8.2900\n",
            "Epoch 162/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2774 - val_loss: 8.2895\n",
            "Epoch 163/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 8.2774 - val_loss: 8.2902\n",
            "Epoch 164/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2774 - val_loss: 8.2903\n",
            "Epoch 165/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 8.2774 - val_loss: 8.2901\n",
            "Epoch 166/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 8.2776 - val_loss: 8.2900\n",
            "Epoch 167/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 8.2773 - val_loss: 8.2902\n",
            "Epoch 168/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2774 - val_loss: 8.2900\n",
            "Epoch 169/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 8.2776 - val_loss: 8.2903\n",
            "Epoch 170/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2776 - val_loss: 8.2904\n",
            "Epoch 171/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2778 - val_loss: 8.2899\n",
            "Epoch 172/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2774 - val_loss: 8.2909\n",
            "Epoch 173/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2773 - val_loss: 8.2903\n",
            "Epoch 174/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 8.2773 - val_loss: 8.2903\n",
            "Epoch 175/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2773 - val_loss: 8.2901\n",
            "Epoch 176/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2773 - val_loss: 8.2911\n",
            "Epoch 177/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2773 - val_loss: 8.2894\n",
            "Epoch 178/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 8.2775 - val_loss: 8.2897\n",
            "Epoch 179/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2776 - val_loss: 8.2908\n",
            "Epoch 180/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2773 - val_loss: 8.2900\n",
            "Epoch 181/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2775 - val_loss: 8.2903\n",
            "Epoch 182/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 8.2775 - val_loss: 8.2902\n",
            "Epoch 183/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2775 - val_loss: 8.2903\n",
            "Epoch 184/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2776 - val_loss: 8.2905\n",
            "Epoch 185/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2775 - val_loss: 8.2901\n",
            "Epoch 186/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2778 - val_loss: 8.2910\n",
            "Epoch 187/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2777 - val_loss: 8.2905\n",
            "Epoch 188/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 8.2774 - val_loss: 8.2901\n",
            "Epoch 189/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 190/500\n",
            "990/990 [==============================] - 1s 781us/step - loss: 8.2774 - val_loss: 8.2909\n",
            "Epoch 191/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 8.2777 - val_loss: 8.2904\n",
            "Epoch 192/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2774 - val_loss: 8.2894\n",
            "Epoch 193/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2773 - val_loss: 8.2894\n",
            "Epoch 194/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 8.2774 - val_loss: 8.2904\n",
            "Epoch 195/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 8.2774 - val_loss: 8.2897\n",
            "Epoch 196/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2774 - val_loss: 8.2902\n",
            "Epoch 197/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2774 - val_loss: 8.2902\n",
            "Epoch 198/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2775 - val_loss: 8.2906\n",
            "Epoch 199/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2777 - val_loss: 8.2895\n",
            "Epoch 200/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2775 - val_loss: 8.2903\n",
            "Epoch 201/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2774 - val_loss: 8.2911\n",
            "Epoch 202/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2775 - val_loss: 8.2899\n",
            "Epoch 203/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2774 - val_loss: 8.2898\n",
            "Epoch 204/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2776 - val_loss: 8.2902\n",
            "Epoch 205/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2775 - val_loss: 8.2902\n",
            "Epoch 206/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2775 - val_loss: 8.2907\n",
            "Epoch 207/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2776 - val_loss: 8.2908\n",
            "Epoch 208/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2778 - val_loss: 8.2908\n",
            "Epoch 209/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2777 - val_loss: 8.2912\n",
            "Epoch 210/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 8.2775 - val_loss: 8.2906\n",
            "Epoch 211/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 8.2776 - val_loss: 8.2894\n",
            "Epoch 212/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 8.2774 - val_loss: 8.2902\n",
            "Epoch 213/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 8.2774 - val_loss: 8.2910\n",
            "Epoch 214/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2775 - val_loss: 8.2906\n",
            "Epoch 215/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 8.2777 - val_loss: 8.2897\n",
            "Epoch 216/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 8.2774 - val_loss: 8.2903\n",
            "Epoch 217/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2775 - val_loss: 8.2906\n",
            "Epoch 218/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 8.2775 - val_loss: 8.2900\n",
            "Epoch 219/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 8.2774 - val_loss: 8.2904\n",
            "Epoch 220/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2776 - val_loss: 8.2907\n",
            "Epoch 221/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2774 - val_loss: 8.2906\n",
            "Epoch 222/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2773 - val_loss: 8.2907\n",
            "Epoch 223/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2774 - val_loss: 8.2905\n",
            "Epoch 224/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 225/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2773 - val_loss: 8.2906\n",
            "Epoch 226/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2774 - val_loss: 8.2905\n",
            "Epoch 227/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2775 - val_loss: 8.2905\n",
            "Epoch 228/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 8.2773 - val_loss: 8.2907\n",
            "Epoch 229/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 8.2778 - val_loss: 8.2906\n",
            "Epoch 230/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 8.2774 - val_loss: 8.2899\n",
            "Epoch 231/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 8.2780 - val_loss: 8.2894\n",
            "Epoch 232/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 8.2774 - val_loss: 8.2899\n",
            "Epoch 233/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 234/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 8.2772 - val_loss: 8.2908\n",
            "Epoch 235/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2775 - val_loss: 8.2905\n",
            "Epoch 236/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2774 - val_loss: 8.2906\n",
            "Epoch 237/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 8.2775 - val_loss: 8.2909\n",
            "Epoch 238/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2774 - val_loss: 8.2910\n",
            "Epoch 239/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2773 - val_loss: 8.2908\n",
            "Epoch 240/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2774 - val_loss: 8.2905\n",
            "Epoch 241/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2773 - val_loss: 8.2906\n",
            "Epoch 242/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2775 - val_loss: 8.2903\n",
            "Epoch 243/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2774 - val_loss: 8.2905\n",
            "Epoch 244/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 8.2774 - val_loss: 8.2910\n",
            "Epoch 245/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2775 - val_loss: 8.2899\n",
            "Epoch 246/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 8.2775 - val_loss: 8.2904\n",
            "Epoch 247/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 8.2773 - val_loss: 8.2897\n",
            "Epoch 248/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 249/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 250/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 251/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2775 - val_loss: 8.2905\n",
            "Epoch 252/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 8.2775 - val_loss: 8.2907\n",
            "Epoch 253/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2775 - val_loss: 8.2912\n",
            "Epoch 254/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2775 - val_loss: 8.2902\n",
            "Epoch 255/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2777 - val_loss: 8.2904\n",
            "Epoch 256/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 8.2775 - val_loss: 8.2899\n",
            "Epoch 257/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2773 - val_loss: 8.2905\n",
            "Epoch 258/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2773 - val_loss: 8.2905\n",
            "Epoch 259/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 8.2774 - val_loss: 8.2910\n",
            "Epoch 260/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 8.2772 - val_loss: 8.2914\n",
            "Epoch 261/500\n",
            "990/990 [==============================] - 1s 734us/step - loss: 8.2773 - val_loss: 8.2905\n",
            "Epoch 262/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2777 - val_loss: 8.2909\n",
            "Epoch 263/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 8.2776 - val_loss: 8.2915\n",
            "Epoch 264/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 8.2772 - val_loss: 8.2904\n",
            "Epoch 265/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2774 - val_loss: 8.2915\n",
            "Epoch 266/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2774 - val_loss: 8.2910\n",
            "Epoch 267/500\n",
            "990/990 [==============================] - 1s 785us/step - loss: 8.2775 - val_loss: 8.2910\n",
            "Epoch 268/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2777 - val_loss: 8.2906\n",
            "Epoch 269/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2775 - val_loss: 8.2910\n",
            "Epoch 270/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2774 - val_loss: 8.2905\n",
            "Epoch 271/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2775 - val_loss: 8.2905\n",
            "Epoch 272/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 8.2774 - val_loss: 8.2908\n",
            "Epoch 273/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 8.2773 - val_loss: 8.2911\n",
            "Epoch 274/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 8.2774 - val_loss: 8.2904\n",
            "Epoch 275/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2775 - val_loss: 8.2909\n",
            "Epoch 276/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 8.2774 - val_loss: 8.2902\n",
            "Epoch 277/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 8.2774 - val_loss: 8.2908\n",
            "Epoch 278/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 8.2774 - val_loss: 8.2910\n",
            "Epoch 279/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 8.2775 - val_loss: 8.2911\n",
            "Epoch 280/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 8.2772 - val_loss: 8.2911\n",
            "Epoch 281/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2773 - val_loss: 8.2912\n",
            "Epoch 282/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2772 - val_loss: 8.2909\n",
            "Epoch 283/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2773 - val_loss: 8.2909\n",
            "Epoch 284/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 8.2773 - val_loss: 8.2910\n",
            "Epoch 285/500\n",
            "990/990 [==============================] - 1s 791us/step - loss: 8.2773 - val_loss: 8.2907\n",
            "Epoch 286/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 8.2779 - val_loss: 8.2909\n",
            "Epoch 287/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2776 - val_loss: 8.2907\n",
            "Epoch 288/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2772 - val_loss: 8.2903\n",
            "Epoch 289/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2775 - val_loss: 8.2904\n",
            "Epoch 290/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 8.2775 - val_loss: 8.2906\n",
            "Epoch 291/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2776 - val_loss: 8.2903\n",
            "Epoch 292/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 8.2775 - val_loss: 8.2900\n",
            "Epoch 293/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 8.2773 - val_loss: 8.2910\n",
            "Epoch 294/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2775 - val_loss: 8.2904\n",
            "Epoch 295/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2773 - val_loss: 8.2914\n",
            "Epoch 296/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2774 - val_loss: 8.2908\n",
            "Epoch 297/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2773 - val_loss: 8.2913\n",
            "Epoch 298/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2775 - val_loss: 8.2908\n",
            "Epoch 299/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 300/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2772 - val_loss: 8.2902\n",
            "Epoch 301/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 8.2773 - val_loss: 8.2906\n",
            "Epoch 302/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2774 - val_loss: 8.2914\n",
            "Epoch 303/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2777 - val_loss: 8.2906\n",
            "Epoch 304/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2780 - val_loss: 8.2905\n",
            "Epoch 305/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2776 - val_loss: 8.2913\n",
            "Epoch 306/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2772 - val_loss: 8.2914\n",
            "Epoch 307/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 8.2773 - val_loss: 8.2911\n",
            "Epoch 308/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2775 - val_loss: 8.2904\n",
            "Epoch 309/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2773 - val_loss: 8.2915\n",
            "Epoch 310/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2775 - val_loss: 8.2912\n",
            "Epoch 311/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 312/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2772 - val_loss: 8.2910\n",
            "Epoch 313/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 8.2774 - val_loss: 8.2908\n",
            "Epoch 314/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 8.2774 - val_loss: 8.2906\n",
            "Epoch 315/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2775 - val_loss: 8.2909\n",
            "Epoch 316/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 8.2773 - val_loss: 8.2905\n",
            "Epoch 317/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2775 - val_loss: 8.2907\n",
            "Epoch 318/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2775 - val_loss: 8.2903\n",
            "Epoch 319/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2772 - val_loss: 8.2914\n",
            "Epoch 320/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 321/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2773 - val_loss: 8.2908\n",
            "Epoch 322/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2773 - val_loss: 8.2908\n",
            "Epoch 323/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2776 - val_loss: 8.2911\n",
            "Epoch 324/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 325/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2774 - val_loss: 8.2906\n",
            "Epoch 326/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2774 - val_loss: 8.2906\n",
            "Epoch 327/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 8.2774 - val_loss: 8.2911\n",
            "Epoch 328/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2773 - val_loss: 8.2908\n",
            "Epoch 329/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 8.2774 - val_loss: 8.2910\n",
            "Epoch 330/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2776 - val_loss: 8.2908\n",
            "Epoch 331/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 332/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 8.2774 - val_loss: 8.2911\n",
            "Epoch 333/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 8.2774 - val_loss: 8.2905\n",
            "Epoch 334/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 8.2772 - val_loss: 8.2908\n",
            "Epoch 335/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 8.2775 - val_loss: 8.2911\n",
            "Epoch 336/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2775 - val_loss: 8.2909\n",
            "Epoch 337/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 8.2774 - val_loss: 8.2910\n",
            "Epoch 338/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2774 - val_loss: 8.2901\n",
            "Epoch 339/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 8.2775 - val_loss: 8.2908\n",
            "Epoch 340/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2773 - val_loss: 8.2907\n",
            "Epoch 341/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2777 - val_loss: 8.2906\n",
            "Epoch 342/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 8.2776 - val_loss: 8.2908\n",
            "Epoch 343/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2775 - val_loss: 8.2904\n",
            "Epoch 344/500\n",
            "990/990 [==============================] - 1s 735us/step - loss: 8.2774 - val_loss: 8.2904\n",
            "Epoch 345/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 8.2777 - val_loss: 8.2909\n",
            "Epoch 346/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2774 - val_loss: 8.2903\n",
            "Epoch 347/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2774 - val_loss: 8.2905\n",
            "Epoch 348/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 8.2774 - val_loss: 8.2903\n",
            "Epoch 349/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2773 - val_loss: 8.2911\n",
            "Epoch 350/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2774 - val_loss: 8.2908\n",
            "Epoch 351/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2774 - val_loss: 8.2912\n",
            "Epoch 352/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 8.2771 - val_loss: 8.2912\n",
            "Epoch 353/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2776 - val_loss: 8.2903\n",
            "Epoch 354/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 8.2775 - val_loss: 8.2905\n",
            "Epoch 355/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 8.2774 - val_loss: 8.2928\n",
            "Epoch 356/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2776 - val_loss: 8.2907\n",
            "Epoch 357/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2775 - val_loss: 8.2906\n",
            "Epoch 358/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2774 - val_loss: 8.2908\n",
            "Epoch 359/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 8.2776 - val_loss: 8.2906\n",
            "Epoch 360/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2774 - val_loss: 8.2909\n",
            "Epoch 361/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2773 - val_loss: 8.2911\n",
            "Epoch 362/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2778 - val_loss: 8.2908\n",
            "Epoch 363/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 8.2775 - val_loss: 8.2914\n",
            "Epoch 364/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 8.2774 - val_loss: 8.2903\n",
            "Epoch 365/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2776 - val_loss: 8.2903\n",
            "Epoch 366/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2772 - val_loss: 8.2902\n",
            "Epoch 367/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2773 - val_loss: 8.2909\n",
            "Epoch 368/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2774 - val_loss: 8.2901\n",
            "Epoch 369/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2773 - val_loss: 8.2907\n",
            "Epoch 370/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2774 - val_loss: 8.2904\n",
            "Epoch 371/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2773 - val_loss: 8.2903\n",
            "Epoch 372/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2772 - val_loss: 8.2907\n",
            "Epoch 373/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2773 - val_loss: 8.2910\n",
            "Epoch 374/500\n",
            "990/990 [==============================] - 1s 736us/step - loss: 8.2774 - val_loss: 8.2906\n",
            "Epoch 375/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2775 - val_loss: 8.2904\n",
            "Epoch 376/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 8.2774 - val_loss: 8.2908\n",
            "Epoch 377/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2777 - val_loss: 8.2907\n",
            "Epoch 378/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 8.2775 - val_loss: 8.2910\n",
            "Epoch 379/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 8.2777 - val_loss: 8.2906\n",
            "Epoch 380/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2775 - val_loss: 8.2904\n",
            "Epoch 381/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 8.2773 - val_loss: 8.2906\n",
            "Epoch 382/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 8.2775 - val_loss: 8.2907\n",
            "Epoch 383/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 8.2774 - val_loss: 8.2909\n",
            "Epoch 384/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 8.2776 - val_loss: 8.2913\n",
            "Epoch 385/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2777 - val_loss: 8.2912\n",
            "Epoch 386/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2775 - val_loss: 8.2913\n",
            "Epoch 387/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 8.2775 - val_loss: 8.2912\n",
            "Epoch 388/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2775 - val_loss: 8.2903\n",
            "Epoch 389/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2776 - val_loss: 8.2913\n",
            "Epoch 390/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 8.2773 - val_loss: 8.2913\n",
            "Epoch 391/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2773 - val_loss: 8.2905\n",
            "Epoch 392/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 8.2774 - val_loss: 8.2908\n",
            "Epoch 393/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2775 - val_loss: 8.2908\n",
            "Epoch 394/500\n",
            "990/990 [==============================] - 1s 731us/step - loss: 8.2773 - val_loss: 8.2907\n",
            "Epoch 395/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 396/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2773 - val_loss: 8.2906\n",
            "Epoch 397/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2775 - val_loss: 8.2903\n",
            "Epoch 398/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 8.2774 - val_loss: 8.2908\n",
            "Epoch 399/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2773 - val_loss: 8.2907\n",
            "Epoch 400/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2774 - val_loss: 8.2910\n",
            "Epoch 401/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2775 - val_loss: 8.2907\n",
            "Epoch 402/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 8.2776 - val_loss: 8.2913\n",
            "Epoch 403/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2777 - val_loss: 8.2914\n",
            "Epoch 404/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2776 - val_loss: 8.2905\n",
            "Epoch 405/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2776 - val_loss: 8.2909\n",
            "Epoch 406/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 407/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2774 - val_loss: 8.2905\n",
            "Epoch 408/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 8.2773 - val_loss: 8.2906\n",
            "Epoch 409/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2775 - val_loss: 8.2918\n",
            "Epoch 410/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 8.2777 - val_loss: 8.2911\n",
            "Epoch 411/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 8.2774 - val_loss: 8.2917\n",
            "Epoch 412/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 8.2775 - val_loss: 8.2907\n",
            "Epoch 413/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 8.2775 - val_loss: 8.2908\n",
            "Epoch 414/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 8.2773 - val_loss: 8.2907\n",
            "Epoch 415/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 8.2774 - val_loss: 8.2909\n",
            "Epoch 416/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 8.2775 - val_loss: 8.2921\n",
            "Epoch 417/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2773 - val_loss: 8.2906\n",
            "Epoch 418/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2775 - val_loss: 8.2910\n",
            "Epoch 419/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2773 - val_loss: 8.2908\n",
            "Epoch 420/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2773 - val_loss: 8.2912\n",
            "Epoch 421/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 8.2775 - val_loss: 8.2906\n",
            "Epoch 422/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 8.2773 - val_loss: 8.2910\n",
            "Epoch 423/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2775 - val_loss: 8.2913\n",
            "Epoch 424/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 8.2774 - val_loss: 8.2912\n",
            "Epoch 425/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 8.2774 - val_loss: 8.2918\n",
            "Epoch 426/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2774 - val_loss: 8.2913\n",
            "Epoch 427/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 8.2773 - val_loss: 8.2911\n",
            "Epoch 428/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2774 - val_loss: 8.2914\n",
            "Epoch 429/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2772 - val_loss: 8.2908\n",
            "Epoch 430/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2773 - val_loss: 8.2908\n",
            "Epoch 431/500\n",
            "990/990 [==============================] - 1s 782us/step - loss: 8.2776 - val_loss: 8.2904\n",
            "Epoch 432/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 8.2774 - val_loss: 8.2906\n",
            "Epoch 433/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2772 - val_loss: 8.2916\n",
            "Epoch 434/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 8.2773 - val_loss: 8.2910\n",
            "Epoch 435/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2773 - val_loss: 8.2915\n",
            "Epoch 436/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2774 - val_loss: 8.2911\n",
            "Epoch 437/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 8.2775 - val_loss: 8.2909\n",
            "Epoch 438/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2776 - val_loss: 8.2909\n",
            "Epoch 439/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2772 - val_loss: 8.2908\n",
            "Epoch 440/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 8.2773 - val_loss: 8.2907\n",
            "Epoch 441/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2773 - val_loss: 8.2908\n",
            "Epoch 442/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 8.2778 - val_loss: 8.2912\n",
            "Epoch 443/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2776 - val_loss: 8.2916\n",
            "Epoch 444/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 8.2773 - val_loss: 8.2903\n",
            "Epoch 445/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 8.2775 - val_loss: 8.2914\n",
            "Epoch 446/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 8.2772 - val_loss: 8.2909\n",
            "Epoch 447/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 8.2777 - val_loss: 8.2911\n",
            "Epoch 448/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2774 - val_loss: 8.2911\n",
            "Epoch 449/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 8.2775 - val_loss: 8.2929\n",
            "Epoch 450/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 8.2774 - val_loss: 8.2909\n",
            "Epoch 451/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2777 - val_loss: 8.2910\n",
            "Epoch 452/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 8.2773 - val_loss: 8.2907\n",
            "Epoch 453/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2772 - val_loss: 8.2910\n",
            "Epoch 454/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 455/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 8.2774 - val_loss: 8.2912\n",
            "Epoch 456/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 8.2774 - val_loss: 8.2911\n",
            "Epoch 457/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2773 - val_loss: 8.2907\n",
            "Epoch 458/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 8.2773 - val_loss: 8.2914\n",
            "Epoch 459/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2772 - val_loss: 8.2911\n",
            "Epoch 460/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 8.2774 - val_loss: 8.2914\n",
            "Epoch 461/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2775 - val_loss: 8.2915\n",
            "Epoch 462/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2775 - val_loss: 8.2909\n",
            "Epoch 463/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 8.2776 - val_loss: 8.2908\n",
            "Epoch 464/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2774 - val_loss: 8.2907\n",
            "Epoch 465/500\n",
            "990/990 [==============================] - 1s 733us/step - loss: 8.2773 - val_loss: 8.2917\n",
            "Epoch 466/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2774 - val_loss: 8.2913\n",
            "Epoch 467/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2777 - val_loss: 8.2917\n",
            "Epoch 468/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 8.2775 - val_loss: 8.2914\n",
            "Epoch 469/500\n",
            "990/990 [==============================] - 1s 801us/step - loss: 8.2775 - val_loss: 8.2914\n",
            "Epoch 470/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2775 - val_loss: 8.2923\n",
            "Epoch 471/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2774 - val_loss: 8.2915\n",
            "Epoch 472/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2773 - val_loss: 8.2915\n",
            "Epoch 473/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2772 - val_loss: 8.2926\n",
            "Epoch 474/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 8.2772 - val_loss: 8.2911\n",
            "Epoch 475/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2774 - val_loss: 8.2911\n",
            "Epoch 476/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2774 - val_loss: 8.2913\n",
            "Epoch 477/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 8.2774 - val_loss: 8.2916\n",
            "Epoch 478/500\n",
            "990/990 [==============================] - 1s 734us/step - loss: 8.2775 - val_loss: 8.2913\n",
            "Epoch 479/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 8.2774 - val_loss: 8.2913\n",
            "Epoch 480/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 8.2772 - val_loss: 8.2911\n",
            "Epoch 481/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2773 - val_loss: 8.2912\n",
            "Epoch 482/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 8.2776 - val_loss: 8.2916\n",
            "Epoch 483/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 8.2775 - val_loss: 8.2917\n",
            "Epoch 484/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2772 - val_loss: 8.2915\n",
            "Epoch 485/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 8.2773 - val_loss: 8.2922\n",
            "Epoch 486/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 8.2773 - val_loss: 8.2915\n",
            "Epoch 487/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 8.2774 - val_loss: 8.2912\n",
            "Epoch 488/500\n",
            "990/990 [==============================] - 1s 733us/step - loss: 8.2774 - val_loss: 8.2908\n",
            "Epoch 489/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 8.2776 - val_loss: 8.2907\n",
            "Epoch 490/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2774 - val_loss: 8.2919\n",
            "Epoch 491/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 8.2775 - val_loss: 8.2914\n",
            "Epoch 492/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 8.2774 - val_loss: 8.2921\n",
            "Epoch 493/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 8.2773 - val_loss: 8.2917\n",
            "Epoch 494/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 8.2776 - val_loss: 8.2912\n",
            "Epoch 495/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 8.2775 - val_loss: 8.2914\n",
            "Epoch 496/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2776 - val_loss: 8.2911\n",
            "Epoch 497/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 8.2774 - val_loss: 8.2915\n",
            "Epoch 498/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 8.2775 - val_loss: 8.2908\n",
            "Epoch 499/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 8.2774 - val_loss: 8.2910\n",
            "Epoch 500/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 8.2774 - val_loss: 8.2925\n",
            "(lamda,Threshold) 10.0 5.0\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (1100, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 0 10.0\n",
            "The shape of N (1100, 3072)\n",
            "The minimum value of N  0.0\n",
            "The max value of N 0.0\n",
            "[INFO:] Xclean  MSE Computed shape (1100, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (1100, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0061614686, 0.0052109957, 0.0058396035, 0.005597862, 0.0055936617, 0.0052953674])\n",
            "[INFO:] The anomaly threshold computed is  0.0052953674\n",
            "[INFO:] The shape of input data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (1100, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [1091, 1090, 1050, 1057, 1076, 1088, 1071, 1056, 1079, 1089, 1052, 1081, 1083, 1072, 1087, 473, 1060, 1069, 1098, 1067, 1095, 1080, 1063, 1092, 1065, 1073, 1093, 1066, 1042, 1018, 1075, 1086, 1040, 1077, 997, 1094, 1070, 1054, 1064, 1023, 1061, 1074, 1025, 1053, 1062, 1029, 1068, 1085, 381, 426]\n",
            "[INFO:] The  Positive boundry 1050\n",
            "[INFO:] The  Negative boundry 50\n",
            "[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n",
            "img shape: (160, 320, 3)\n",
            "\n",
            "Saving Top-10 most anomalous digit: @\n",
            "=====================\n",
            "AUROC 10.0 0.9\n",
            "=======================\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 32) (256, 32)\n",
            "Train on 990 samples, validate on 110 samples\n",
            "Epoch 1/500\n",
            "990/990 [==============================] - 10s 10ms/step - loss: 1.9869 - val_loss: 2.0025\n",
            "Epoch 2/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9869 - val_loss: 2.0029\n",
            "Epoch 3/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9866 - val_loss: 2.0016\n",
            "Epoch 4/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9867 - val_loss: 2.0007\n",
            "Epoch 5/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9867 - val_loss: 2.0005\n",
            "Epoch 6/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.9867 - val_loss: 2.0010\n",
            "Epoch 7/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9864 - val_loss: 2.0007\n",
            "Epoch 8/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.9866 - val_loss: 2.0005\n",
            "Epoch 9/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9865 - val_loss: 2.0010\n",
            "Epoch 10/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.9867 - val_loss: 2.0012\n",
            "Epoch 11/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.9867 - val_loss: 2.0012\n",
            "Epoch 12/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9868 - val_loss: 2.0008\n",
            "Epoch 13/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.9867 - val_loss: 2.0003\n",
            "Epoch 14/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9869 - val_loss: 2.0006\n",
            "Epoch 15/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.9867 - val_loss: 2.0010\n",
            "Epoch 16/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9865 - val_loss: 2.0002\n",
            "Epoch 17/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9866 - val_loss: 1.9997\n",
            "Epoch 18/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.9869 - val_loss: 2.0004\n",
            "Epoch 19/500\n",
            "990/990 [==============================] - 1s 800us/step - loss: 1.9865 - val_loss: 2.0010\n",
            "Epoch 20/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9867 - val_loss: 2.0008\n",
            "Epoch 21/500\n",
            "990/990 [==============================] - 1s 787us/step - loss: 1.9867 - val_loss: 1.9997\n",
            "Epoch 22/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9867 - val_loss: 2.0011\n",
            "Epoch 23/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9867 - val_loss: 2.0005\n",
            "Epoch 24/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9867 - val_loss: 2.0002\n",
            "Epoch 25/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9870 - val_loss: 2.0002\n",
            "Epoch 26/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9866 - val_loss: 2.0011\n",
            "Epoch 27/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9867 - val_loss: 1.9998\n",
            "Epoch 28/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9865 - val_loss: 2.0006\n",
            "Epoch 29/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9867 - val_loss: 2.0000\n",
            "Epoch 30/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9865 - val_loss: 2.0004\n",
            "Epoch 31/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 1.9866 - val_loss: 2.0001\n",
            "Epoch 32/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9867 - val_loss: 2.0017\n",
            "Epoch 33/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.9866 - val_loss: 2.0012\n",
            "Epoch 34/500\n",
            "990/990 [==============================] - 1s 783us/step - loss: 1.9867 - val_loss: 2.0008\n",
            "Epoch 35/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9867 - val_loss: 2.0006\n",
            "Epoch 36/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 1.9871 - val_loss: 2.0002\n",
            "Epoch 37/500\n",
            "990/990 [==============================] - 1s 785us/step - loss: 1.9866 - val_loss: 2.0002\n",
            "Epoch 38/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9870 - val_loss: 2.0014\n",
            "Epoch 39/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 1.9869 - val_loss: 2.0010\n",
            "Epoch 40/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9866 - val_loss: 2.0006\n",
            "Epoch 41/500\n",
            "990/990 [==============================] - 1s 787us/step - loss: 1.9867 - val_loss: 2.0010\n",
            "Epoch 42/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.9870 - val_loss: 2.0021\n",
            "Epoch 43/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9868 - val_loss: 2.0016\n",
            "Epoch 44/500\n",
            "990/990 [==============================] - 1s 788us/step - loss: 1.9868 - val_loss: 2.0012\n",
            "Epoch 45/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9866 - val_loss: 2.0020\n",
            "Epoch 46/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 1.9868 - val_loss: 2.0017\n",
            "Epoch 47/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 1.9865 - val_loss: 2.0004\n",
            "Epoch 48/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9864 - val_loss: 2.0017\n",
            "Epoch 49/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9866 - val_loss: 2.0014\n",
            "Epoch 50/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9867 - val_loss: 2.0007\n",
            "Epoch 51/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9867 - val_loss: 2.0018\n",
            "Epoch 52/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9866 - val_loss: 2.0010\n",
            "Epoch 53/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9867 - val_loss: 1.9999\n",
            "Epoch 54/500\n",
            "990/990 [==============================] - 1s 801us/step - loss: 1.9867 - val_loss: 2.0011\n",
            "Epoch 55/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9865 - val_loss: 2.0006\n",
            "Epoch 56/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9869 - val_loss: 2.0005\n",
            "Epoch 57/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9869 - val_loss: 2.0004\n",
            "Epoch 58/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.9869 - val_loss: 2.0007\n",
            "Epoch 59/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9865 - val_loss: 2.0009\n",
            "Epoch 60/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.9867 - val_loss: 2.0015\n",
            "Epoch 61/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9869 - val_loss: 2.0013\n",
            "Epoch 62/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9865 - val_loss: 2.0004\n",
            "Epoch 63/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9867 - val_loss: 2.0006\n",
            "Epoch 64/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9867 - val_loss: 2.0003\n",
            "Epoch 65/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9867 - val_loss: 2.0001\n",
            "Epoch 66/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.9869 - val_loss: 2.0000\n",
            "Epoch 67/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.9866 - val_loss: 2.0002\n",
            "Epoch 68/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9867 - val_loss: 2.0009\n",
            "Epoch 69/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.9866 - val_loss: 2.0015\n",
            "Epoch 70/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.9864 - val_loss: 2.0008\n",
            "Epoch 71/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9866 - val_loss: 2.0002\n",
            "Epoch 72/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9865 - val_loss: 2.0001\n",
            "Epoch 73/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9866 - val_loss: 2.0016\n",
            "Epoch 74/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 1.9865 - val_loss: 2.0009\n",
            "Epoch 75/500\n",
            "990/990 [==============================] - 1s 793us/step - loss: 1.9866 - val_loss: 2.0011\n",
            "Epoch 76/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9867 - val_loss: 2.0005\n",
            "Epoch 77/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9866 - val_loss: 2.0005\n",
            "Epoch 78/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9865 - val_loss: 1.9997\n",
            "Epoch 79/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9866 - val_loss: 2.0003\n",
            "Epoch 80/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 1.9868 - val_loss: 2.0002\n",
            "Epoch 81/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9869 - val_loss: 2.0005\n",
            "Epoch 82/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9866 - val_loss: 2.0012\n",
            "Epoch 83/500\n",
            "990/990 [==============================] - 1s 788us/step - loss: 1.9869 - val_loss: 2.0013\n",
            "Epoch 84/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9868 - val_loss: 2.0006\n",
            "Epoch 85/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.9866 - val_loss: 2.0008\n",
            "Epoch 86/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.9868 - val_loss: 2.0002\n",
            "Epoch 87/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9867 - val_loss: 2.0008\n",
            "Epoch 88/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9866 - val_loss: 2.0007\n",
            "Epoch 89/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.9865 - val_loss: 2.0010\n",
            "Epoch 90/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9866 - val_loss: 2.0007\n",
            "Epoch 91/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9867 - val_loss: 2.0011\n",
            "Epoch 92/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9867 - val_loss: 2.0004\n",
            "Epoch 93/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9866 - val_loss: 2.0031\n",
            "Epoch 94/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9867 - val_loss: 2.0022\n",
            "Epoch 95/500\n",
            "990/990 [==============================] - 1s 790us/step - loss: 1.9865 - val_loss: 2.0009\n",
            "Epoch 96/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9867 - val_loss: 2.0025\n",
            "Epoch 97/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9867 - val_loss: 2.0005\n",
            "Epoch 98/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 1.9867 - val_loss: 2.0007\n",
            "Epoch 99/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.9867 - val_loss: 2.0001\n",
            "Epoch 100/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.9865 - val_loss: 2.0009\n",
            "Epoch 101/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.9867 - val_loss: 2.0008\n",
            "Epoch 102/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 1.9864 - val_loss: 2.0005\n",
            "Epoch 103/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 1.9865 - val_loss: 2.0017\n",
            "Epoch 104/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9867 - val_loss: 2.0005\n",
            "Epoch 105/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9868 - val_loss: 2.0008\n",
            "Epoch 106/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9868 - val_loss: 2.0007\n",
            "Epoch 107/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.9867 - val_loss: 2.0010\n",
            "Epoch 108/500\n",
            "990/990 [==============================] - 1s 782us/step - loss: 1.9867 - val_loss: 2.0006\n",
            "Epoch 109/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9867 - val_loss: 2.0004\n",
            "Epoch 110/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.9868 - val_loss: 2.0013\n",
            "Epoch 111/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9867 - val_loss: 2.0014\n",
            "Epoch 112/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.9866 - val_loss: 2.0012\n",
            "Epoch 113/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.9866 - val_loss: 2.0001\n",
            "Epoch 114/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9868 - val_loss: 2.0004\n",
            "Epoch 115/500\n",
            "990/990 [==============================] - 1s 779us/step - loss: 1.9867 - val_loss: 2.0009\n",
            "Epoch 116/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.9865 - val_loss: 2.0022\n",
            "Epoch 117/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9868 - val_loss: 2.0009\n",
            "Epoch 118/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9865 - val_loss: 2.0015\n",
            "Epoch 119/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9866 - val_loss: 2.0016\n",
            "Epoch 120/500\n",
            "990/990 [==============================] - 1s 786us/step - loss: 1.9867 - val_loss: 2.0018\n",
            "Epoch 121/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 1.9865 - val_loss: 2.0023\n",
            "Epoch 122/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9865 - val_loss: 2.0010\n",
            "Epoch 123/500\n",
            "990/990 [==============================] - 1s 796us/step - loss: 1.9865 - val_loss: 2.0015\n",
            "Epoch 124/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 1.9865 - val_loss: 2.0002\n",
            "Epoch 125/500\n",
            "990/990 [==============================] - 1s 775us/step - loss: 1.9865 - val_loss: 2.0010\n",
            "Epoch 126/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.9865 - val_loss: 2.0020\n",
            "Epoch 127/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9865 - val_loss: 2.0013\n",
            "Epoch 128/500\n",
            "990/990 [==============================] - 1s 788us/step - loss: 1.9864 - val_loss: 2.0011\n",
            "Epoch 129/500\n",
            "990/990 [==============================] - 1s 781us/step - loss: 1.9866 - val_loss: 2.0007\n",
            "Epoch 130/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 1.9868 - val_loss: 1.9998\n",
            "Epoch 131/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.9867 - val_loss: 2.0006\n",
            "Epoch 132/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9866 - val_loss: 2.0013\n",
            "Epoch 133/500\n",
            "990/990 [==============================] - 1s 784us/step - loss: 1.9867 - val_loss: 2.0024\n",
            "Epoch 134/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9865 - val_loss: 2.0011\n",
            "Epoch 135/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 1.9866 - val_loss: 2.0015\n",
            "Epoch 136/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9867 - val_loss: 2.0010\n",
            "Epoch 137/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.9864 - val_loss: 2.0007\n",
            "Epoch 138/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9866 - val_loss: 2.0012\n",
            "Epoch 139/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.9865 - val_loss: 2.0009\n",
            "Epoch 140/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.9866 - val_loss: 2.0005\n",
            "Epoch 141/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9866 - val_loss: 2.0014\n",
            "Epoch 142/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9868 - val_loss: 2.0007\n",
            "Epoch 143/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9864 - val_loss: 2.0017\n",
            "Epoch 144/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9865 - val_loss: 2.0006\n",
            "Epoch 145/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.9868 - val_loss: 2.0012\n",
            "Epoch 146/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.9868 - val_loss: 2.0012\n",
            "Epoch 147/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 1.9867 - val_loss: 2.0009\n",
            "Epoch 148/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9866 - val_loss: 2.0020\n",
            "Epoch 149/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9869 - val_loss: 2.0011\n",
            "Epoch 150/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9866 - val_loss: 2.0021\n",
            "Epoch 151/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9866 - val_loss: 2.0014\n",
            "Epoch 152/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.9864 - val_loss: 2.0020\n",
            "Epoch 153/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9866 - val_loss: 2.0013\n",
            "Epoch 154/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9867 - val_loss: 2.0009\n",
            "Epoch 155/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9867 - val_loss: 2.0012\n",
            "Epoch 156/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9867 - val_loss: 2.0015\n",
            "Epoch 157/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.9866 - val_loss: 2.0014\n",
            "Epoch 158/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.9865 - val_loss: 2.0024\n",
            "Epoch 159/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9867 - val_loss: 2.0028\n",
            "Epoch 160/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9866 - val_loss: 2.0033\n",
            "Epoch 161/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9865 - val_loss: 2.0012\n",
            "Epoch 162/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.9865 - val_loss: 2.0021\n",
            "Epoch 163/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.9869 - val_loss: 2.0010\n",
            "Epoch 164/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9867 - val_loss: 2.0018\n",
            "Epoch 165/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.9866 - val_loss: 2.0016\n",
            "Epoch 166/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.9865 - val_loss: 2.0014\n",
            "Epoch 167/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9866 - val_loss: 2.0014\n",
            "Epoch 168/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.9866 - val_loss: 2.0015\n",
            "Epoch 169/500\n",
            "990/990 [==============================] - 1s 780us/step - loss: 1.9865 - val_loss: 2.0015\n",
            "Epoch 170/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9866 - val_loss: 2.0015\n",
            "Epoch 171/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9867 - val_loss: 2.0026\n",
            "Epoch 172/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9865 - val_loss: 2.0015\n",
            "Epoch 173/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.9867 - val_loss: 2.0014\n",
            "Epoch 174/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9865 - val_loss: 2.0028\n",
            "Epoch 175/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9867 - val_loss: 2.0010\n",
            "Epoch 176/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9870 - val_loss: 2.0010\n",
            "Epoch 177/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9866 - val_loss: 2.0008\n",
            "Epoch 178/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9865 - val_loss: 2.0006\n",
            "Epoch 179/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9865 - val_loss: 2.0011\n",
            "Epoch 180/500\n",
            "990/990 [==============================] - 1s 737us/step - loss: 1.9866 - val_loss: 2.0010\n",
            "Epoch 181/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9866 - val_loss: 2.0009\n",
            "Epoch 182/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9866 - val_loss: 2.0016\n",
            "Epoch 183/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9866 - val_loss: 2.0025\n",
            "Epoch 184/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9865 - val_loss: 2.0018\n",
            "Epoch 185/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.9865 - val_loss: 2.0029\n",
            "Epoch 186/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9865 - val_loss: 2.0007\n",
            "Epoch 187/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.9867 - val_loss: 2.0016\n",
            "Epoch 188/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.9868 - val_loss: 2.0027\n",
            "Epoch 189/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9865 - val_loss: 2.0014\n",
            "Epoch 190/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9865 - val_loss: 2.0016\n",
            "Epoch 191/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.9867 - val_loss: 2.0016\n",
            "Epoch 192/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9866 - val_loss: 2.0014\n",
            "Epoch 193/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.9868 - val_loss: 2.0019\n",
            "Epoch 194/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9865 - val_loss: 2.0010\n",
            "Epoch 195/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.9868 - val_loss: 2.0027\n",
            "Epoch 196/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9868 - val_loss: 2.0011\n",
            "Epoch 197/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9866 - val_loss: 2.0010\n",
            "Epoch 198/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.9867 - val_loss: 2.0013\n",
            "Epoch 199/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9864 - val_loss: 2.0013\n",
            "Epoch 200/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.9866 - val_loss: 2.0010\n",
            "Epoch 201/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.9865 - val_loss: 2.0026\n",
            "Epoch 202/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9868 - val_loss: 2.0016\n",
            "Epoch 203/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9866 - val_loss: 2.0006\n",
            "Epoch 204/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9867 - val_loss: 2.0002\n",
            "Epoch 205/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.9867 - val_loss: 2.0010\n",
            "Epoch 206/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.9867 - val_loss: 2.0009\n",
            "Epoch 207/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9867 - val_loss: 2.0013\n",
            "Epoch 208/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 1.9867 - val_loss: 2.0003\n",
            "Epoch 209/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9867 - val_loss: 1.9999\n",
            "Epoch 210/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.9866 - val_loss: 1.9997\n",
            "Epoch 211/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.9866 - val_loss: 2.0002\n",
            "Epoch 212/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.9868 - val_loss: 2.0010\n",
            "Epoch 213/500\n",
            "990/990 [==============================] - 1s 784us/step - loss: 1.9868 - val_loss: 2.0001\n",
            "Epoch 214/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9867 - val_loss: 2.0000\n",
            "Epoch 215/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9865 - val_loss: 1.9995\n",
            "Epoch 216/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9867 - val_loss: 2.0002\n",
            "Epoch 217/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 1.9869 - val_loss: 2.0005\n",
            "Epoch 218/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9866 - val_loss: 2.0001\n",
            "Epoch 219/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9867 - val_loss: 2.0017\n",
            "Epoch 220/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 1.9865 - val_loss: 2.0015\n",
            "Epoch 221/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9867 - val_loss: 1.9996\n",
            "Epoch 222/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.9866 - val_loss: 2.0004\n",
            "Epoch 223/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9867 - val_loss: 2.0015\n",
            "Epoch 224/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9865 - val_loss: 2.0008\n",
            "Epoch 225/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9865 - val_loss: 2.0010\n",
            "Epoch 226/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.9867 - val_loss: 2.0015\n",
            "Epoch 227/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9864 - val_loss: 2.0015\n",
            "Epoch 228/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.9866 - val_loss: 2.0001\n",
            "Epoch 229/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9867 - val_loss: 2.0007\n",
            "Epoch 230/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.9865 - val_loss: 2.0010\n",
            "Epoch 231/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.9864 - val_loss: 2.0011\n",
            "Epoch 232/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9863 - val_loss: 1.9995\n",
            "Epoch 233/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9867 - val_loss: 2.0000\n",
            "Epoch 234/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9865 - val_loss: 2.0004\n",
            "Epoch 235/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 1.9866 - val_loss: 2.0011\n",
            "Epoch 236/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9867 - val_loss: 1.9998\n",
            "Epoch 237/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9866 - val_loss: 2.0005\n",
            "Epoch 238/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 1.9866 - val_loss: 2.0011\n",
            "Epoch 239/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9863 - val_loss: 2.0005\n",
            "Epoch 240/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 1.9867 - val_loss: 2.0010\n",
            "Epoch 241/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.9864 - val_loss: 2.0006\n",
            "Epoch 242/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.9865 - val_loss: 2.0013\n",
            "Epoch 243/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9869 - val_loss: 1.9994\n",
            "Epoch 244/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9865 - val_loss: 2.0000\n",
            "Epoch 245/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9866 - val_loss: 2.0007\n",
            "Epoch 246/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.9865 - val_loss: 2.0003\n",
            "Epoch 247/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.9864 - val_loss: 2.0002\n",
            "Epoch 248/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.9865 - val_loss: 2.0010\n",
            "Epoch 249/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.9866 - val_loss: 2.0007\n",
            "Epoch 250/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.9867 - val_loss: 2.0004\n",
            "Epoch 251/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.9864 - val_loss: 2.0015\n",
            "Epoch 252/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9866 - val_loss: 1.9996\n",
            "Epoch 253/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9866 - val_loss: 2.0011\n",
            "Epoch 254/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9868 - val_loss: 2.0006\n",
            "Epoch 255/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9866 - val_loss: 2.0026\n",
            "Epoch 256/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9865 - val_loss: 1.9994\n",
            "Epoch 257/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.9867 - val_loss: 2.0020\n",
            "Epoch 258/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9869 - val_loss: 2.0011\n",
            "Epoch 259/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.9865 - val_loss: 2.0013\n",
            "Epoch 260/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.9866 - val_loss: 1.9997\n",
            "Epoch 261/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9865 - val_loss: 2.0012\n",
            "Epoch 262/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 1.9865 - val_loss: 2.0016\n",
            "Epoch 263/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.9865 - val_loss: 2.0003\n",
            "Epoch 264/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9865 - val_loss: 2.0007\n",
            "Epoch 265/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9866 - val_loss: 2.0000\n",
            "Epoch 266/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9865 - val_loss: 2.0014\n",
            "Epoch 267/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9865 - val_loss: 2.0021\n",
            "Epoch 268/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9864 - val_loss: 1.9993\n",
            "Epoch 269/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9867 - val_loss: 2.0004\n",
            "Epoch 270/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.9865 - val_loss: 2.0009\n",
            "Epoch 271/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9866 - val_loss: 1.9997\n",
            "Epoch 272/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9865 - val_loss: 2.0026\n",
            "Epoch 273/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.9866 - val_loss: 2.0006\n",
            "Epoch 274/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9866 - val_loss: 2.0007\n",
            "Epoch 275/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9867 - val_loss: 1.9998\n",
            "Epoch 276/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.9865 - val_loss: 2.0004\n",
            "Epoch 277/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9866 - val_loss: 2.0020\n",
            "Epoch 278/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9867 - val_loss: 2.0004\n",
            "Epoch 279/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 1.9867 - val_loss: 2.0017\n",
            "Epoch 280/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.9865 - val_loss: 2.0005\n",
            "Epoch 281/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9868 - val_loss: 2.0010\n",
            "Epoch 282/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.9867 - val_loss: 1.9999\n",
            "Epoch 283/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9865 - val_loss: 2.0017\n",
            "Epoch 284/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9866 - val_loss: 2.0012\n",
            "Epoch 285/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.9867 - val_loss: 1.9998\n",
            "Epoch 286/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9866 - val_loss: 2.0011\n",
            "Epoch 287/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9868 - val_loss: 2.0002\n",
            "Epoch 288/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.9870 - val_loss: 2.0019\n",
            "Epoch 289/500\n",
            "990/990 [==============================] - 1s 786us/step - loss: 1.9866 - val_loss: 2.0015\n",
            "Epoch 290/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9866 - val_loss: 2.0012\n",
            "Epoch 291/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9864 - val_loss: 2.0005\n",
            "Epoch 292/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.9867 - val_loss: 2.0014\n",
            "Epoch 293/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9868 - val_loss: 1.9995\n",
            "Epoch 294/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9869 - val_loss: 2.0021\n",
            "Epoch 295/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9867 - val_loss: 2.0006\n",
            "Epoch 296/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9865 - val_loss: 2.0007\n",
            "Epoch 297/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9866 - val_loss: 2.0019\n",
            "Epoch 298/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9865 - val_loss: 2.0006\n",
            "Epoch 299/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.9864 - val_loss: 2.0022\n",
            "Epoch 300/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9867 - val_loss: 2.0024\n",
            "Epoch 301/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9864 - val_loss: 2.0009\n",
            "Epoch 302/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 1.9869 - val_loss: 2.0023\n",
            "Epoch 303/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9867 - val_loss: 2.0025\n",
            "Epoch 304/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9868 - val_loss: 2.0032\n",
            "Epoch 305/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9865 - val_loss: 2.0019\n",
            "Epoch 306/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9867 - val_loss: 2.0035\n",
            "Epoch 307/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9865 - val_loss: 2.0005\n",
            "Epoch 308/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9865 - val_loss: 2.0015\n",
            "Epoch 309/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9870 - val_loss: 2.0007\n",
            "Epoch 310/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9866 - val_loss: 2.0020\n",
            "Epoch 311/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9866 - val_loss: 2.0005\n",
            "Epoch 312/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9866 - val_loss: 2.0002\n",
            "Epoch 313/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9866 - val_loss: 2.0020\n",
            "Epoch 314/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9866 - val_loss: 2.0027\n",
            "Epoch 315/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 1.9865 - val_loss: 2.0011\n",
            "Epoch 316/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9865 - val_loss: 2.0022\n",
            "Epoch 317/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 1.9865 - val_loss: 2.0026\n",
            "Epoch 318/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9865 - val_loss: 2.0011\n",
            "Epoch 319/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 1.9866 - val_loss: 2.0005\n",
            "Epoch 320/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 1.9867 - val_loss: 2.0010\n",
            "Epoch 321/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.9865 - val_loss: 2.0013\n",
            "Epoch 322/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 1.9866 - val_loss: 1.9997\n",
            "Epoch 323/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.9867 - val_loss: 2.0029\n",
            "Epoch 324/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.9867 - val_loss: 2.0037\n",
            "Epoch 325/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9869 - val_loss: 2.0031\n",
            "Epoch 326/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.9865 - val_loss: 2.0020\n",
            "Epoch 327/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 1.9866 - val_loss: 2.0028\n",
            "Epoch 328/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.9865 - val_loss: 2.0018\n",
            "Epoch 329/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9864 - val_loss: 2.0026\n",
            "Epoch 330/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.9866 - val_loss: 2.0018\n",
            "Epoch 331/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9866 - val_loss: 2.0007\n",
            "Epoch 332/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9868 - val_loss: 2.0015\n",
            "Epoch 333/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.9866 - val_loss: 2.0009\n",
            "Epoch 334/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.9867 - val_loss: 1.9997\n",
            "Epoch 335/500\n",
            "990/990 [==============================] - 1s 776us/step - loss: 1.9869 - val_loss: 1.9996\n",
            "Epoch 336/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9865 - val_loss: 1.9990\n",
            "Epoch 337/500\n",
            "990/990 [==============================] - 1s 772us/step - loss: 1.9866 - val_loss: 1.9995\n",
            "Epoch 338/500\n",
            "990/990 [==============================] - 1s 793us/step - loss: 1.9864 - val_loss: 2.0013\n",
            "Epoch 339/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9865 - val_loss: 2.0015\n",
            "Epoch 340/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.9864 - val_loss: 2.0022\n",
            "Epoch 341/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9864 - val_loss: 2.0007\n",
            "Epoch 342/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9866 - val_loss: 2.0011\n",
            "Epoch 343/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9866 - val_loss: 2.0021\n",
            "Epoch 344/500\n",
            "990/990 [==============================] - 1s 741us/step - loss: 1.9868 - val_loss: 2.0009\n",
            "Epoch 345/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9864 - val_loss: 2.0015\n",
            "Epoch 346/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.9865 - val_loss: 1.9994\n",
            "Epoch 347/500\n",
            "990/990 [==============================] - 1s 738us/step - loss: 1.9865 - val_loss: 1.9999\n",
            "Epoch 348/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9865 - val_loss: 2.0024\n",
            "Epoch 349/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.9865 - val_loss: 2.0006\n",
            "Epoch 350/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9865 - val_loss: 2.0009\n",
            "Epoch 351/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.9864 - val_loss: 2.0007\n",
            "Epoch 352/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9865 - val_loss: 1.9993\n",
            "Epoch 353/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.9864 - val_loss: 2.0003\n",
            "Epoch 354/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9870 - val_loss: 2.0008\n",
            "Epoch 355/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.9866 - val_loss: 2.0007\n",
            "Epoch 356/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9867 - val_loss: 2.0016\n",
            "Epoch 357/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.9865 - val_loss: 2.0016\n",
            "Epoch 358/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.9865 - val_loss: 2.0008\n",
            "Epoch 359/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9868 - val_loss: 2.0004\n",
            "Epoch 360/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9866 - val_loss: 2.0010\n",
            "Epoch 361/500\n",
            "990/990 [==============================] - 1s 743us/step - loss: 1.9864 - val_loss: 1.9989\n",
            "Epoch 362/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9868 - val_loss: 1.9986\n",
            "Epoch 363/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9868 - val_loss: 2.0009\n",
            "Epoch 364/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.9868 - val_loss: 2.0006\n",
            "Epoch 365/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9868 - val_loss: 2.0020\n",
            "Epoch 366/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.9871 - val_loss: 2.0015\n",
            "Epoch 367/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.9870 - val_loss: 2.0020\n",
            "Epoch 368/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9869 - val_loss: 2.0017\n",
            "Epoch 369/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.9865 - val_loss: 1.9997\n",
            "Epoch 370/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.9863 - val_loss: 2.0021\n",
            "Epoch 371/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9863 - val_loss: 2.0021\n",
            "Epoch 372/500\n",
            "990/990 [==============================] - 1s 785us/step - loss: 1.9867 - val_loss: 2.0015\n",
            "Epoch 373/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.9865 - val_loss: 2.0014\n",
            "Epoch 374/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9868 - val_loss: 2.0024\n",
            "Epoch 375/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9865 - val_loss: 2.0029\n",
            "Epoch 376/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9868 - val_loss: 2.0025\n",
            "Epoch 377/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.9866 - val_loss: 2.0014\n",
            "Epoch 378/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9863 - val_loss: 2.0010\n",
            "Epoch 379/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9865 - val_loss: 2.0018\n",
            "Epoch 380/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 1.9863 - val_loss: 1.9995\n",
            "Epoch 381/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9866 - val_loss: 2.0013\n",
            "Epoch 382/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.9865 - val_loss: 2.0003\n",
            "Epoch 383/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.9864 - val_loss: 2.0017\n",
            "Epoch 384/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.9865 - val_loss: 2.0016\n",
            "Epoch 385/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9864 - val_loss: 2.0005\n",
            "Epoch 386/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9867 - val_loss: 2.0027\n",
            "Epoch 387/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.9863 - val_loss: 2.0013\n",
            "Epoch 388/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.9866 - val_loss: 2.0018\n",
            "Epoch 389/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.9867 - val_loss: 2.0012\n",
            "Epoch 390/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9866 - val_loss: 2.0008\n",
            "Epoch 391/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.9867 - val_loss: 1.9997\n",
            "Epoch 392/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.9863 - val_loss: 2.0017\n",
            "Epoch 393/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 1.9865 - val_loss: 2.0032\n",
            "Epoch 394/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9866 - val_loss: 2.0021\n",
            "Epoch 395/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9866 - val_loss: 2.0010\n",
            "Epoch 396/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9866 - val_loss: 2.0003\n",
            "Epoch 397/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9865 - val_loss: 2.0006\n",
            "Epoch 398/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9865 - val_loss: 1.9998\n",
            "Epoch 399/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.9869 - val_loss: 2.0030\n",
            "Epoch 400/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9866 - val_loss: 2.0007\n",
            "Epoch 401/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9866 - val_loss: 2.0018\n",
            "Epoch 402/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9866 - val_loss: 2.0004\n",
            "Epoch 403/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.9865 - val_loss: 2.0018\n",
            "Epoch 404/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9867 - val_loss: 2.0012\n",
            "Epoch 405/500\n",
            "990/990 [==============================] - 1s 771us/step - loss: 1.9865 - val_loss: 2.0024\n",
            "Epoch 406/500\n",
            "990/990 [==============================] - 1s 764us/step - loss: 1.9866 - val_loss: 2.0021\n",
            "Epoch 407/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.9868 - val_loss: 2.0016\n",
            "Epoch 408/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.9868 - val_loss: 2.0009\n",
            "Epoch 409/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.9865 - val_loss: 2.0023\n",
            "Epoch 410/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.9865 - val_loss: 2.0011\n",
            "Epoch 411/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9865 - val_loss: 2.0008\n",
            "Epoch 412/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.9864 - val_loss: 2.0020\n",
            "Epoch 413/500\n",
            "990/990 [==============================] - 1s 784us/step - loss: 1.9865 - val_loss: 2.0011\n",
            "Epoch 414/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.9866 - val_loss: 2.0037\n",
            "Epoch 415/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9866 - val_loss: 2.0023\n",
            "Epoch 416/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9866 - val_loss: 2.0013\n",
            "Epoch 417/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9869 - val_loss: 2.0020\n",
            "Epoch 418/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9865 - val_loss: 2.0013\n",
            "Epoch 419/500\n",
            "990/990 [==============================] - 1s 768us/step - loss: 1.9868 - val_loss: 1.9994\n",
            "Epoch 420/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9865 - val_loss: 2.0013\n",
            "Epoch 421/500\n",
            "990/990 [==============================] - 1s 749us/step - loss: 1.9866 - val_loss: 2.0024\n",
            "Epoch 422/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9864 - val_loss: 2.0019\n",
            "Epoch 423/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9864 - val_loss: 1.9994\n",
            "Epoch 424/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9865 - val_loss: 2.0008\n",
            "Epoch 425/500\n",
            "990/990 [==============================] - 1s 760us/step - loss: 1.9865 - val_loss: 1.9994\n",
            "Epoch 426/500\n",
            "990/990 [==============================] - 1s 767us/step - loss: 1.9867 - val_loss: 2.0016\n",
            "Epoch 427/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9865 - val_loss: 2.0011\n",
            "Epoch 428/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9868 - val_loss: 2.0011\n",
            "Epoch 429/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.9868 - val_loss: 2.0009\n",
            "Epoch 430/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9868 - val_loss: 2.0022\n",
            "Epoch 431/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.9865 - val_loss: 2.0001\n",
            "Epoch 432/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.9866 - val_loss: 2.0003\n",
            "Epoch 433/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.9864 - val_loss: 2.0005\n",
            "Epoch 434/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9870 - val_loss: 2.0005\n",
            "Epoch 435/500\n",
            "990/990 [==============================] - 1s 781us/step - loss: 1.9865 - val_loss: 2.0025\n",
            "Epoch 436/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9864 - val_loss: 2.0020\n",
            "Epoch 437/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9866 - val_loss: 2.0018\n",
            "Epoch 438/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 1.9866 - val_loss: 2.0024\n",
            "Epoch 439/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9865 - val_loss: 2.0027\n",
            "Epoch 440/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9866 - val_loss: 2.0006\n",
            "Epoch 441/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9866 - val_loss: 2.0004\n",
            "Epoch 442/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9865 - val_loss: 2.0026\n",
            "Epoch 443/500\n",
            "990/990 [==============================] - 1s 778us/step - loss: 1.9870 - val_loss: 2.0001\n",
            "Epoch 444/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.9867 - val_loss: 2.0022\n",
            "Epoch 445/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.9867 - val_loss: 1.9999\n",
            "Epoch 446/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9866 - val_loss: 2.0016\n",
            "Epoch 447/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.9864 - val_loss: 2.0003\n",
            "Epoch 448/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9864 - val_loss: 2.0016\n",
            "Epoch 449/500\n",
            "990/990 [==============================] - 1s 750us/step - loss: 1.9867 - val_loss: 2.0015\n",
            "Epoch 450/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9864 - val_loss: 2.0013\n",
            "Epoch 451/500\n",
            "990/990 [==============================] - 1s 774us/step - loss: 1.9866 - val_loss: 2.0018\n",
            "Epoch 452/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.9868 - val_loss: 2.0010\n",
            "Epoch 453/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9866 - val_loss: 1.9991\n",
            "Epoch 454/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9864 - val_loss: 1.9951\n",
            "Epoch 455/500\n",
            "990/990 [==============================] - 1s 742us/step - loss: 1.9864 - val_loss: 1.9953\n",
            "Epoch 456/500\n",
            "990/990 [==============================] - 1s 747us/step - loss: 1.9864 - val_loss: 1.9961\n",
            "Epoch 457/500\n",
            "990/990 [==============================] - 1s 740us/step - loss: 1.9866 - val_loss: 1.9972\n",
            "Epoch 458/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9867 - val_loss: 1.9966\n",
            "Epoch 459/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9869 - val_loss: 1.9972\n",
            "Epoch 460/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9866 - val_loss: 1.9980\n",
            "Epoch 461/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.9866 - val_loss: 1.9969\n",
            "Epoch 462/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.9868 - val_loss: 1.9975\n",
            "Epoch 463/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9865 - val_loss: 1.9980\n",
            "Epoch 464/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.9864 - val_loss: 1.9978\n",
            "Epoch 465/500\n",
            "990/990 [==============================] - 1s 756us/step - loss: 1.9865 - val_loss: 1.9964\n",
            "Epoch 466/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9868 - val_loss: 1.9975\n",
            "Epoch 467/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.9867 - val_loss: 1.9980\n",
            "Epoch 468/500\n",
            "990/990 [==============================] - 1s 762us/step - loss: 1.9865 - val_loss: 1.9991\n",
            "Epoch 469/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.9866 - val_loss: 1.9978\n",
            "Epoch 470/500\n",
            "990/990 [==============================] - 1s 752us/step - loss: 1.9866 - val_loss: 1.9986\n",
            "Epoch 471/500\n",
            "990/990 [==============================] - 1s 757us/step - loss: 1.9865 - val_loss: 1.9982\n",
            "Epoch 472/500\n",
            "990/990 [==============================] - 1s 744us/step - loss: 1.9865 - val_loss: 1.9979\n",
            "Epoch 473/500\n",
            "990/990 [==============================] - 1s 759us/step - loss: 1.9865 - val_loss: 1.9976\n",
            "Epoch 474/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.9867 - val_loss: 1.9985\n",
            "Epoch 475/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.9865 - val_loss: 1.9965\n",
            "Epoch 476/500\n",
            "990/990 [==============================] - 1s 761us/step - loss: 1.9866 - val_loss: 1.9974\n",
            "Epoch 477/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.9864 - val_loss: 1.9983\n",
            "Epoch 478/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9864 - val_loss: 1.9973\n",
            "Epoch 479/500\n",
            "990/990 [==============================] - 1s 754us/step - loss: 1.9864 - val_loss: 1.9977\n",
            "Epoch 480/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9868 - val_loss: 1.9962\n",
            "Epoch 481/500\n",
            "990/990 [==============================] - 1s 773us/step - loss: 1.9865 - val_loss: 1.9978\n",
            "Epoch 482/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9865 - val_loss: 1.9975\n",
            "Epoch 483/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.9864 - val_loss: 1.9989\n",
            "Epoch 484/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.9865 - val_loss: 1.9989\n",
            "Epoch 485/500\n",
            "990/990 [==============================] - 1s 763us/step - loss: 1.9868 - val_loss: 1.9977\n",
            "Epoch 486/500\n",
            "990/990 [==============================] - 1s 766us/step - loss: 1.9865 - val_loss: 1.9987\n",
            "Epoch 487/500\n",
            "990/990 [==============================] - 1s 755us/step - loss: 1.9866 - val_loss: 1.9981\n",
            "Epoch 488/500\n",
            "990/990 [==============================] - 1s 770us/step - loss: 1.9865 - val_loss: 1.9979\n",
            "Epoch 489/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.9866 - val_loss: 1.9977\n",
            "Epoch 490/500\n",
            "990/990 [==============================] - 1s 751us/step - loss: 1.9865 - val_loss: 1.9971\n",
            "Epoch 491/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9867 - val_loss: 1.9983\n",
            "Epoch 492/500\n",
            "990/990 [==============================] - 1s 748us/step - loss: 1.9865 - val_loss: 1.9987\n",
            "Epoch 493/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9865 - val_loss: 1.9983\n",
            "Epoch 494/500\n",
            "990/990 [==============================] - 1s 746us/step - loss: 1.9865 - val_loss: 1.9980\n",
            "Epoch 495/500\n",
            "990/990 [==============================] - 1s 769us/step - loss: 1.9863 - val_loss: 1.9985\n",
            "Epoch 496/500\n",
            "990/990 [==============================] - 1s 753us/step - loss: 1.9864 - val_loss: 1.9982\n",
            "Epoch 497/500\n",
            "990/990 [==============================] - 1s 745us/step - loss: 1.9863 - val_loss: 1.9983\n",
            "Epoch 498/500\n",
            "990/990 [==============================] - 1s 765us/step - loss: 1.9865 - val_loss: 1.9987\n",
            "Epoch 499/500\n",
            "990/990 [==============================] - 1s 758us/step - loss: 1.9866 - val_loss: 1.9980\n",
            "Epoch 500/500\n",
            "990/990 [==============================] - 1s 777us/step - loss: 1.9867 - val_loss: 1.9979\n",
            "(lamda,Threshold) 100.0 50.0\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (1100, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 0 100.0\n",
            "The shape of N (1100, 3072)\n",
            "The minimum value of N  0.0\n",
            "The max value of N 0.0\n",
            "[INFO:] Xclean  MSE Computed shape (1100, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (1100, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0061614686, 0.0052109957, 0.0058396035, 0.005597862, 0.0055936617, 0.0052953674, 0.004712906])\n",
            "[INFO:] The anomaly threshold computed is  0.004712906\n",
            "[INFO:] The shape of input data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (1100, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (1100, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [1057, 1090, 1071, 1065, 1052, 1072, 1095, 1081, 1087, 1083, 1067, 473, 1056, 1080, 1069, 1063, 1076, 1073, 1042, 1066, 1079, 1075, 1092, 1088, 1093, 1018, 1050, 1091, 1040, 1086, 1077, 1060, 1070, 1098, 1094, 997, 1089, 1054, 1064, 1023, 1074, 1061, 1053, 1085, 1025, 1062, 1029, 991, 1034, 1068]\n",
            "[INFO:] The  Positive boundry 1050\n",
            "[INFO:] The  Negative boundry 50\n",
            "[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n",
            "img shape: (160, 320, 3)\n",
            "\n",
            "Saving Top-10 most anomalous digit: @\n",
            "=====================\n",
            "AUROC 100.0 0.9\n",
            "=======================\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([])\n",
            "========================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEVCAYAAAAM3jVmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9a887eec88>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9HMwbk9ZHFEX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The Anomalies: 100 considered**"
      ]
    },
    {
      "metadata": {
        "id": "7Qg-mXrcGB-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18346
        },
        "outputId": "9bcf00df-df3e-4794-cc19-1fa9472eccda"
      },
      "cell_type": "code",
      "source": [
        "## Obtaining the training and testing data\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from src.models.RCAE import RCAE_AD\n",
        "\n",
        "DATASET = \"gtsrb\"\n",
        "IMG_DIM= 3072\n",
        "IMG_HGT =32\n",
        "IMG_WDT=32\n",
        "IMG_CHANNEL=3\n",
        "HIDDEN_LAYER_SIZE= 32\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/gtsrb/RCAE/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/gtsrb/RCAE/\"\n",
        "PRETRAINED_WT_PATH = \"\"\n",
        "\n",
        "rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n",
        "\n",
        "# print(\"Train Data Shape: \",rcae.data._X_train.shape)\n",
        "# print(\"Train Label Shape: \",rcae.data._y_train.shape)\n",
        "# print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n",
        "# print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n",
        "# print(\"Test Data Shape: \",rcae.data._X_test.shape)\n",
        "# print(\"Test Label Shape: \",rcae.data._y_test.shape)\n",
        "print(\"===========TRAINING AND PREDICTING WITH RCAE============================\")\n",
        "rcae.fit_and_predict()\n",
        "print(\"========================================================================\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RCAE.RESULT_PATH: /content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE/\n",
            "[INFO:] Loading data...\n",
            "[INFO:] Loading adversarial data...\n",
            "[INFO:] Negative Y_test labels 100\n",
            "[INFO:] Positive Y_test labels 270\n",
            "\u001b[F\u001b[KData loaded.\n",
            "[INFO:] Assertions of memory muted\n",
            "[INFO:] Loading data...\n",
            "[INFO:] Loading adversarial data...\n",
            "[INFO:] Negative Y_test labels 100\n",
            "[INFO:] Positive Y_test labels 270\n",
            "\u001b[F\u001b[KData loaded.\n",
            "===========TRAINING AND PREDICTING WITH RCAE============================\n",
            "[INFO:]  Length of Positive data 1050\n",
            "[INFO:]  Length of Negative data 100\n",
            "[INFO:] X_train.shape (1150, 32, 32, 3)\n",
            "[INFO:] y_train.shape (1150,)\n",
            "[INFO:] X_test.shape (1150, 32, 32, 3)\n",
            "[INFO:] y_test.shape (1150,)\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 32) (256, 32)\n",
            "Train on 1035 samples, validate on 115 samples\n",
            "Epoch 1/500\n",
            "1035/1035 [==============================] - 11s 11ms/step - loss: 0.8430 - val_loss: 0.8549\n",
            "Epoch 2/500\n",
            "1035/1035 [==============================] - 1s 815us/step - loss: 0.8049 - val_loss: 0.8204\n",
            "Epoch 3/500\n",
            "1035/1035 [==============================] - 1s 802us/step - loss: 0.7943 - val_loss: 0.7976\n",
            "Epoch 4/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7892 - val_loss: 0.7934\n",
            "Epoch 5/500\n",
            "1035/1035 [==============================] - 1s 793us/step - loss: 0.7854 - val_loss: 0.7859\n",
            "Epoch 6/500\n",
            "1035/1035 [==============================] - 1s 792us/step - loss: 0.7835 - val_loss: 0.7843\n",
            "Epoch 7/500\n",
            "1035/1035 [==============================] - 1s 768us/step - loss: 0.7821 - val_loss: 0.7837\n",
            "Epoch 8/500\n",
            "1035/1035 [==============================] - 1s 805us/step - loss: 0.7800 - val_loss: 0.7820\n",
            "Epoch 9/500\n",
            "1035/1035 [==============================] - 1s 781us/step - loss: 0.7785 - val_loss: 0.7818\n",
            "Epoch 10/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7782 - val_loss: 0.7802\n",
            "Epoch 11/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7775 - val_loss: 0.7807\n",
            "Epoch 12/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7767 - val_loss: 0.7821\n",
            "Epoch 13/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7768 - val_loss: 0.7786\n",
            "Epoch 14/500\n",
            "1035/1035 [==============================] - 1s 793us/step - loss: 0.7753 - val_loss: 0.7779\n",
            "Epoch 15/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7747 - val_loss: 0.7781\n",
            "Epoch 16/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7744 - val_loss: 0.7776\n",
            "Epoch 17/500\n",
            "1035/1035 [==============================] - 1s 794us/step - loss: 0.7744 - val_loss: 0.7764\n",
            "Epoch 18/500\n",
            "1035/1035 [==============================] - 1s 822us/step - loss: 0.7742 - val_loss: 0.7781\n",
            "Epoch 19/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7742 - val_loss: 0.7782\n",
            "Epoch 20/500\n",
            "1035/1035 [==============================] - 1s 796us/step - loss: 0.7734 - val_loss: 0.7774\n",
            "Epoch 21/500\n",
            "1035/1035 [==============================] - 1s 788us/step - loss: 0.7730 - val_loss: 0.7760\n",
            "Epoch 22/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7730 - val_loss: 0.7761\n",
            "Epoch 23/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7720 - val_loss: 0.7763\n",
            "Epoch 24/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7717 - val_loss: 0.7756\n",
            "Epoch 25/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7718 - val_loss: 0.7754\n",
            "Epoch 26/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7716 - val_loss: 0.7747\n",
            "Epoch 27/500\n",
            "1035/1035 [==============================] - 1s 762us/step - loss: 0.7716 - val_loss: 0.7760\n",
            "Epoch 28/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7717 - val_loss: 0.7748\n",
            "Epoch 29/500\n",
            "1035/1035 [==============================] - 1s 793us/step - loss: 0.7713 - val_loss: 0.7747\n",
            "Epoch 30/500\n",
            "1035/1035 [==============================] - 1s 772us/step - loss: 0.7710 - val_loss: 0.7755\n",
            "Epoch 31/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7708 - val_loss: 0.7756\n",
            "Epoch 32/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7707 - val_loss: 0.7746\n",
            "Epoch 33/500\n",
            "1035/1035 [==============================] - 1s 761us/step - loss: 0.7712 - val_loss: 0.7750\n",
            "Epoch 34/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7705 - val_loss: 0.7753\n",
            "Epoch 35/500\n",
            "1035/1035 [==============================] - 1s 785us/step - loss: 0.7704 - val_loss: 0.7739\n",
            "Epoch 36/500\n",
            "1035/1035 [==============================] - 1s 789us/step - loss: 0.7696 - val_loss: 0.7744\n",
            "Epoch 37/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7699 - val_loss: 0.7743\n",
            "Epoch 38/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7693 - val_loss: 0.7735\n",
            "Epoch 39/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7698 - val_loss: 0.7744\n",
            "Epoch 40/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7695 - val_loss: 0.7737\n",
            "Epoch 41/500\n",
            "1035/1035 [==============================] - 1s 769us/step - loss: 0.7690 - val_loss: 0.7741\n",
            "Epoch 42/500\n",
            "1035/1035 [==============================] - 1s 779us/step - loss: 0.7696 - val_loss: 0.7731\n",
            "Epoch 43/500\n",
            "1035/1035 [==============================] - 1s 787us/step - loss: 0.7695 - val_loss: 0.7737\n",
            "Epoch 44/500\n",
            "1035/1035 [==============================] - 1s 765us/step - loss: 0.7691 - val_loss: 0.7746\n",
            "Epoch 45/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7694 - val_loss: 0.7743\n",
            "Epoch 46/500\n",
            "1035/1035 [==============================] - 1s 781us/step - loss: 0.7683 - val_loss: 0.7732\n",
            "Epoch 47/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7689 - val_loss: 0.7738\n",
            "Epoch 48/500\n",
            "1035/1035 [==============================] - 1s 779us/step - loss: 0.7683 - val_loss: 0.7735\n",
            "Epoch 49/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7683 - val_loss: 0.7739\n",
            "Epoch 50/500\n",
            "1035/1035 [==============================] - 1s 763us/step - loss: 0.7683 - val_loss: 0.7736\n",
            "Epoch 51/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7680 - val_loss: 0.7728\n",
            "Epoch 52/500\n",
            "1035/1035 [==============================] - 1s 757us/step - loss: 0.7682 - val_loss: 0.7735\n",
            "Epoch 53/500\n",
            "1035/1035 [==============================] - 1s 764us/step - loss: 0.7678 - val_loss: 0.7729\n",
            "Epoch 54/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7677 - val_loss: 0.7751\n",
            "Epoch 55/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7692 - val_loss: 0.7735\n",
            "Epoch 56/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7684 - val_loss: 0.7746\n",
            "Epoch 57/500\n",
            "1035/1035 [==============================] - 1s 786us/step - loss: 0.7686 - val_loss: 0.7738\n",
            "Epoch 58/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7682 - val_loss: 0.7743\n",
            "Epoch 59/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7680 - val_loss: 0.7731\n",
            "Epoch 60/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7675 - val_loss: 0.7733\n",
            "Epoch 61/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7686 - val_loss: 0.7734\n",
            "Epoch 62/500\n",
            "1035/1035 [==============================] - 1s 781us/step - loss: 0.7680 - val_loss: 0.7728\n",
            "Epoch 63/500\n",
            "1035/1035 [==============================] - 1s 792us/step - loss: 0.7672 - val_loss: 0.7730\n",
            "Epoch 64/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7676 - val_loss: 0.7731\n",
            "Epoch 65/500\n",
            "1035/1035 [==============================] - 1s 791us/step - loss: 0.7679 - val_loss: 0.7724\n",
            "Epoch 66/500\n",
            "1035/1035 [==============================] - 1s 789us/step - loss: 0.7673 - val_loss: 0.7731\n",
            "Epoch 67/500\n",
            "1035/1035 [==============================] - 1s 761us/step - loss: 0.7670 - val_loss: 0.7731\n",
            "Epoch 68/500\n",
            "1035/1035 [==============================] - 1s 765us/step - loss: 0.7673 - val_loss: 0.7728\n",
            "Epoch 69/500\n",
            "1035/1035 [==============================] - 1s 762us/step - loss: 0.7672 - val_loss: 0.7736\n",
            "Epoch 70/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7671 - val_loss: 0.7734\n",
            "Epoch 71/500\n",
            "1035/1035 [==============================] - 1s 765us/step - loss: 0.7672 - val_loss: 0.7729\n",
            "Epoch 72/500\n",
            "1035/1035 [==============================] - 1s 751us/step - loss: 0.7670 - val_loss: 0.7735\n",
            "Epoch 73/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7669 - val_loss: 0.7734\n",
            "Epoch 74/500\n",
            "1035/1035 [==============================] - 1s 789us/step - loss: 0.7664 - val_loss: 0.7740\n",
            "Epoch 75/500\n",
            "1035/1035 [==============================] - 1s 765us/step - loss: 0.7667 - val_loss: 0.7737\n",
            "Epoch 76/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7665 - val_loss: 0.7731\n",
            "Epoch 77/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7665 - val_loss: 0.7732\n",
            "Epoch 78/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7670 - val_loss: 0.7730\n",
            "Epoch 79/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7665 - val_loss: 0.7730\n",
            "Epoch 80/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7665 - val_loss: 0.7728\n",
            "Epoch 81/500\n",
            "1035/1035 [==============================] - 1s 761us/step - loss: 0.7662 - val_loss: 0.7732\n",
            "Epoch 82/500\n",
            "1035/1035 [==============================] - 1s 765us/step - loss: 0.7661 - val_loss: 0.7734\n",
            "Epoch 83/500\n",
            "1035/1035 [==============================] - 1s 750us/step - loss: 0.7661 - val_loss: 0.7734\n",
            "Epoch 84/500\n",
            "1035/1035 [==============================] - 1s 792us/step - loss: 0.7662 - val_loss: 0.7724\n",
            "Epoch 85/500\n",
            "1035/1035 [==============================] - 1s 781us/step - loss: 0.7657 - val_loss: 0.7729\n",
            "Epoch 86/500\n",
            "1035/1035 [==============================] - 1s 764us/step - loss: 0.7657 - val_loss: 0.7725\n",
            "Epoch 87/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7666 - val_loss: 0.7724\n",
            "Epoch 88/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7656 - val_loss: 0.7733\n",
            "Epoch 89/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7663 - val_loss: 0.7743\n",
            "Epoch 90/500\n",
            "1035/1035 [==============================] - 1s 768us/step - loss: 0.7663 - val_loss: 0.7722\n",
            "Epoch 91/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7655 - val_loss: 0.7727\n",
            "Epoch 92/500\n",
            "1035/1035 [==============================] - 1s 755us/step - loss: 0.7659 - val_loss: 0.7729\n",
            "Epoch 93/500\n",
            "1035/1035 [==============================] - 1s 787us/step - loss: 0.7657 - val_loss: 0.7721\n",
            "Epoch 94/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7651 - val_loss: 0.7725\n",
            "Epoch 95/500\n",
            "1035/1035 [==============================] - 1s 781us/step - loss: 0.7659 - val_loss: 0.7745\n",
            "Epoch 96/500\n",
            "1035/1035 [==============================] - 1s 772us/step - loss: 0.7663 - val_loss: 0.7723\n",
            "Epoch 97/500\n",
            "1035/1035 [==============================] - 1s 768us/step - loss: 0.7654 - val_loss: 0.7725\n",
            "Epoch 98/500\n",
            "1035/1035 [==============================] - 1s 785us/step - loss: 0.7655 - val_loss: 0.7729\n",
            "Epoch 99/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7657 - val_loss: 0.7725\n",
            "Epoch 100/500\n",
            "1035/1035 [==============================] - 1s 769us/step - loss: 0.7653 - val_loss: 0.7731\n",
            "Epoch 101/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7654 - val_loss: 0.7725\n",
            "Epoch 102/500\n",
            "1035/1035 [==============================] - 1s 781us/step - loss: 0.7655 - val_loss: 0.7723\n",
            "Epoch 103/500\n",
            "1035/1035 [==============================] - 1s 757us/step - loss: 0.7656 - val_loss: 0.7736\n",
            "Epoch 104/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7654 - val_loss: 0.7729\n",
            "Epoch 105/500\n",
            "1035/1035 [==============================] - 1s 791us/step - loss: 0.7652 - val_loss: 0.7728\n",
            "Epoch 106/500\n",
            "1035/1035 [==============================] - 1s 765us/step - loss: 0.7660 - val_loss: 0.7726\n",
            "Epoch 107/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7655 - val_loss: 0.7726\n",
            "Epoch 108/500\n",
            "1035/1035 [==============================] - 1s 781us/step - loss: 0.7656 - val_loss: 0.7742\n",
            "Epoch 109/500\n",
            "1035/1035 [==============================] - 1s 769us/step - loss: 0.7653 - val_loss: 0.7728\n",
            "Epoch 110/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7651 - val_loss: 0.7722\n",
            "Epoch 111/500\n",
            "1035/1035 [==============================] - 1s 793us/step - loss: 0.7647 - val_loss: 0.7724\n",
            "Epoch 112/500\n",
            "1035/1035 [==============================] - 1s 788us/step - loss: 0.7649 - val_loss: 0.7730\n",
            "Epoch 113/500\n",
            "1035/1035 [==============================] - 1s 779us/step - loss: 0.7651 - val_loss: 0.7724\n",
            "Epoch 114/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 0.7657 - val_loss: 0.7730\n",
            "Epoch 115/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7654 - val_loss: 0.7733\n",
            "Epoch 116/500\n",
            "1035/1035 [==============================] - 1s 764us/step - loss: 0.7647 - val_loss: 0.7725\n",
            "Epoch 117/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7646 - val_loss: 0.7731\n",
            "Epoch 118/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7654 - val_loss: 0.7733\n",
            "Epoch 119/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 0.7651 - val_loss: 0.7725\n",
            "Epoch 120/500\n",
            "1035/1035 [==============================] - 1s 747us/step - loss: 0.7646 - val_loss: 0.7723\n",
            "Epoch 121/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7647 - val_loss: 0.7720\n",
            "Epoch 122/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 0.7649 - val_loss: 0.7723\n",
            "Epoch 123/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7647 - val_loss: 0.7722\n",
            "Epoch 124/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7645 - val_loss: 0.7720\n",
            "Epoch 125/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7646 - val_loss: 0.7729\n",
            "Epoch 126/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7646 - val_loss: 0.7723\n",
            "Epoch 127/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7645 - val_loss: 0.7722\n",
            "Epoch 128/500\n",
            "1035/1035 [==============================] - 1s 768us/step - loss: 0.7642 - val_loss: 0.7728\n",
            "Epoch 129/500\n",
            "1035/1035 [==============================] - 1s 800us/step - loss: 0.7647 - val_loss: 0.7725\n",
            "Epoch 130/500\n",
            "1035/1035 [==============================] - 1s 787us/step - loss: 0.7646 - val_loss: 0.7724\n",
            "Epoch 131/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7645 - val_loss: 0.7722\n",
            "Epoch 132/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7649 - val_loss: 0.7725\n",
            "Epoch 133/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7656 - val_loss: 0.7724\n",
            "Epoch 134/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7648 - val_loss: 0.7724\n",
            "Epoch 135/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7643 - val_loss: 0.7725\n",
            "Epoch 136/500\n",
            "1035/1035 [==============================] - 1s 779us/step - loss: 0.7645 - val_loss: 0.7722\n",
            "Epoch 137/500\n",
            "1035/1035 [==============================] - 1s 762us/step - loss: 0.7641 - val_loss: 0.7723\n",
            "Epoch 138/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7643 - val_loss: 0.7736\n",
            "Epoch 139/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7643 - val_loss: 0.7724\n",
            "Epoch 140/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7645 - val_loss: 0.7726\n",
            "Epoch 141/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7644 - val_loss: 0.7728\n",
            "Epoch 142/500\n",
            "1035/1035 [==============================] - 1s 763us/step - loss: 0.7642 - val_loss: 0.7723\n",
            "Epoch 143/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7644 - val_loss: 0.7723\n",
            "Epoch 144/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7650 - val_loss: 0.7729\n",
            "Epoch 145/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 0.7643 - val_loss: 0.7718\n",
            "Epoch 146/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7640 - val_loss: 0.7722\n",
            "Epoch 147/500\n",
            "1035/1035 [==============================] - 1s 779us/step - loss: 0.7641 - val_loss: 0.7719\n",
            "Epoch 148/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7642 - val_loss: 0.7723\n",
            "Epoch 149/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7642 - val_loss: 0.7714\n",
            "Epoch 150/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7643 - val_loss: 0.7725\n",
            "Epoch 151/500\n",
            "1035/1035 [==============================] - 1s 762us/step - loss: 0.7637 - val_loss: 0.7724\n",
            "Epoch 152/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7639 - val_loss: 0.7728\n",
            "Epoch 153/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7638 - val_loss: 0.7722\n",
            "Epoch 154/500\n",
            "1035/1035 [==============================] - 1s 782us/step - loss: 0.7644 - val_loss: 0.7729\n",
            "Epoch 155/500\n",
            "1035/1035 [==============================] - 1s 782us/step - loss: 0.7641 - val_loss: 0.7733\n",
            "Epoch 156/500\n",
            "1035/1035 [==============================] - 1s 757us/step - loss: 0.7643 - val_loss: 0.7723\n",
            "Epoch 157/500\n",
            "1035/1035 [==============================] - 1s 782us/step - loss: 0.7641 - val_loss: 0.7727\n",
            "Epoch 158/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7639 - val_loss: 0.7725\n",
            "Epoch 159/500\n",
            "1035/1035 [==============================] - 1s 754us/step - loss: 0.7641 - val_loss: 0.7732\n",
            "Epoch 160/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7639 - val_loss: 0.7725\n",
            "Epoch 161/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7638 - val_loss: 0.7721\n",
            "Epoch 162/500\n",
            "1035/1035 [==============================] - 1s 755us/step - loss: 0.7636 - val_loss: 0.7722\n",
            "Epoch 163/500\n",
            "1035/1035 [==============================] - 1s 770us/step - loss: 0.7638 - val_loss: 0.7727\n",
            "Epoch 164/500\n",
            "1035/1035 [==============================] - 1s 748us/step - loss: 0.7639 - val_loss: 0.7725\n",
            "Epoch 165/500\n",
            "1035/1035 [==============================] - 1s 772us/step - loss: 0.7643 - val_loss: 0.7716\n",
            "Epoch 166/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7643 - val_loss: 0.7721\n",
            "Epoch 167/500\n",
            "1035/1035 [==============================] - 1s 772us/step - loss: 0.7640 - val_loss: 0.7724\n",
            "Epoch 168/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7639 - val_loss: 0.7720\n",
            "Epoch 169/500\n",
            "1035/1035 [==============================] - 1s 810us/step - loss: 0.7639 - val_loss: 0.7715\n",
            "Epoch 170/500\n",
            "1035/1035 [==============================] - 1s 769us/step - loss: 0.7636 - val_loss: 0.7718\n",
            "Epoch 171/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7639 - val_loss: 0.7730\n",
            "Epoch 172/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7640 - val_loss: 0.7736\n",
            "Epoch 173/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7640 - val_loss: 0.7732\n",
            "Epoch 174/500\n",
            "1035/1035 [==============================] - 1s 791us/step - loss: 0.7640 - val_loss: 0.7740\n",
            "Epoch 175/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7638 - val_loss: 0.7733\n",
            "Epoch 176/500\n",
            "1035/1035 [==============================] - 1s 769us/step - loss: 0.7635 - val_loss: 0.7728\n",
            "Epoch 177/500\n",
            "1035/1035 [==============================] - 1s 788us/step - loss: 0.7635 - val_loss: 0.7722\n",
            "Epoch 178/500\n",
            "1035/1035 [==============================] - 1s 795us/step - loss: 0.7634 - val_loss: 0.7715\n",
            "Epoch 179/500\n",
            "1035/1035 [==============================] - 1s 770us/step - loss: 0.7635 - val_loss: 0.7722\n",
            "Epoch 180/500\n",
            "1035/1035 [==============================] - 1s 804us/step - loss: 0.7634 - val_loss: 0.7720\n",
            "Epoch 181/500\n",
            "1035/1035 [==============================] - 1s 792us/step - loss: 0.7636 - val_loss: 0.7724\n",
            "Epoch 182/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7634 - val_loss: 0.7734\n",
            "Epoch 183/500\n",
            "1035/1035 [==============================] - 1s 789us/step - loss: 0.7636 - val_loss: 0.7726\n",
            "Epoch 184/500\n",
            "1035/1035 [==============================] - 1s 786us/step - loss: 0.7634 - val_loss: 0.7725\n",
            "Epoch 185/500\n",
            "1035/1035 [==============================] - 1s 801us/step - loss: 0.7638 - val_loss: 0.7734\n",
            "Epoch 186/500\n",
            "1035/1035 [==============================] - 1s 806us/step - loss: 0.7636 - val_loss: 0.7726\n",
            "Epoch 187/500\n",
            "1035/1035 [==============================] - 1s 797us/step - loss: 0.7633 - val_loss: 0.7723\n",
            "Epoch 188/500\n",
            "1035/1035 [==============================] - 1s 799us/step - loss: 0.7635 - val_loss: 0.7720\n",
            "Epoch 189/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7633 - val_loss: 0.7728\n",
            "Epoch 190/500\n",
            "1035/1035 [==============================] - 1s 763us/step - loss: 0.7636 - val_loss: 0.7724\n",
            "Epoch 191/500\n",
            "1035/1035 [==============================] - 1s 779us/step - loss: 0.7635 - val_loss: 0.7718\n",
            "Epoch 192/500\n",
            "1035/1035 [==============================] - 1s 795us/step - loss: 0.7635 - val_loss: 0.7725\n",
            "Epoch 193/500\n",
            "1035/1035 [==============================] - 1s 772us/step - loss: 0.7637 - val_loss: 0.7722\n",
            "Epoch 194/500\n",
            "1035/1035 [==============================] - 1s 786us/step - loss: 0.7637 - val_loss: 0.7717\n",
            "Epoch 195/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7633 - val_loss: 0.7723\n",
            "Epoch 196/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7641 - val_loss: 0.7723\n",
            "Epoch 197/500\n",
            "1035/1035 [==============================] - 1s 781us/step - loss: 0.7636 - val_loss: 0.7720\n",
            "Epoch 198/500\n",
            "1035/1035 [==============================] - 1s 796us/step - loss: 0.7634 - val_loss: 0.7728\n",
            "Epoch 199/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7633 - val_loss: 0.7714\n",
            "Epoch 200/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7633 - val_loss: 0.7723\n",
            "Epoch 201/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7626 - val_loss: 0.7724\n",
            "Epoch 202/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7632 - val_loss: 0.7723\n",
            "Epoch 203/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7637 - val_loss: 0.7723\n",
            "Epoch 204/500\n",
            "1035/1035 [==============================] - 1s 763us/step - loss: 0.7639 - val_loss: 0.7738\n",
            "Epoch 205/500\n",
            "1035/1035 [==============================] - 1s 782us/step - loss: 0.7636 - val_loss: 0.7722\n",
            "Epoch 206/500\n",
            "1035/1035 [==============================] - 1s 770us/step - loss: 0.7634 - val_loss: 0.7724\n",
            "Epoch 207/500\n",
            "1035/1035 [==============================] - 1s 795us/step - loss: 0.7636 - val_loss: 0.7723\n",
            "Epoch 208/500\n",
            "1035/1035 [==============================] - 1s 805us/step - loss: 0.7637 - val_loss: 0.7727\n",
            "Epoch 209/500\n",
            "1035/1035 [==============================] - 1s 813us/step - loss: 0.7635 - val_loss: 0.7726\n",
            "Epoch 210/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7631 - val_loss: 0.7725\n",
            "Epoch 211/500\n",
            "1035/1035 [==============================] - 1s 819us/step - loss: 0.7632 - val_loss: 0.7721\n",
            "Epoch 212/500\n",
            "1035/1035 [==============================] - 1s 794us/step - loss: 0.7629 - val_loss: 0.7724\n",
            "Epoch 213/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7633 - val_loss: 0.7726\n",
            "Epoch 214/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7633 - val_loss: 0.7726\n",
            "Epoch 215/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7631 - val_loss: 0.7724\n",
            "Epoch 216/500\n",
            "1035/1035 [==============================] - 1s 760us/step - loss: 0.7630 - val_loss: 0.7710\n",
            "Epoch 217/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7628 - val_loss: 0.7727\n",
            "Epoch 218/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7635 - val_loss: 0.7717\n",
            "Epoch 219/500\n",
            "1035/1035 [==============================] - 1s 770us/step - loss: 0.7628 - val_loss: 0.7712\n",
            "Epoch 220/500\n",
            "1035/1035 [==============================] - 1s 787us/step - loss: 0.7630 - val_loss: 0.7721\n",
            "Epoch 221/500\n",
            "1035/1035 [==============================] - 1s 782us/step - loss: 0.7630 - val_loss: 0.7724\n",
            "Epoch 222/500\n",
            "1035/1035 [==============================] - 1s 762us/step - loss: 0.7631 - val_loss: 0.7719\n",
            "Epoch 223/500\n",
            "1035/1035 [==============================] - 1s 787us/step - loss: 0.7633 - val_loss: 0.7720\n",
            "Epoch 224/500\n",
            "1035/1035 [==============================] - 1s 768us/step - loss: 0.7636 - val_loss: 0.7724\n",
            "Epoch 225/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7628 - val_loss: 0.7723\n",
            "Epoch 226/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7628 - val_loss: 0.7730\n",
            "Epoch 227/500\n",
            "1035/1035 [==============================] - 1s 755us/step - loss: 0.7631 - val_loss: 0.7725\n",
            "Epoch 228/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7629 - val_loss: 0.7717\n",
            "Epoch 229/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7626 - val_loss: 0.7719\n",
            "Epoch 230/500\n",
            "1035/1035 [==============================] - 1s 770us/step - loss: 0.7626 - val_loss: 0.7716\n",
            "Epoch 231/500\n",
            "1035/1035 [==============================] - 1s 798us/step - loss: 0.7627 - val_loss: 0.7726\n",
            "Epoch 232/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7636 - val_loss: 0.7730\n",
            "Epoch 233/500\n",
            "1035/1035 [==============================] - 1s 764us/step - loss: 0.7631 - val_loss: 0.7723\n",
            "Epoch 234/500\n",
            "1035/1035 [==============================] - 1s 789us/step - loss: 0.7630 - val_loss: 0.7719\n",
            "Epoch 235/500\n",
            "1035/1035 [==============================] - 1s 791us/step - loss: 0.7634 - val_loss: 0.7724\n",
            "Epoch 236/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7626 - val_loss: 0.7716\n",
            "Epoch 237/500\n",
            "1035/1035 [==============================] - 1s 795us/step - loss: 0.7628 - val_loss: 0.7718\n",
            "Epoch 238/500\n",
            "1035/1035 [==============================] - 1s 800us/step - loss: 0.7626 - val_loss: 0.7718\n",
            "Epoch 239/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7625 - val_loss: 0.7721\n",
            "Epoch 240/500\n",
            "1035/1035 [==============================] - 1s 799us/step - loss: 0.7628 - val_loss: 0.7718\n",
            "Epoch 241/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7628 - val_loss: 0.7713\n",
            "Epoch 242/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7627 - val_loss: 0.7717\n",
            "Epoch 243/500\n",
            "1035/1035 [==============================] - 1s 804us/step - loss: 0.7627 - val_loss: 0.7725\n",
            "Epoch 244/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7629 - val_loss: 0.7714\n",
            "Epoch 245/500\n",
            "1035/1035 [==============================] - 1s 779us/step - loss: 0.7628 - val_loss: 0.7731\n",
            "Epoch 246/500\n",
            "1035/1035 [==============================] - 1s 793us/step - loss: 0.7630 - val_loss: 0.7719\n",
            "Epoch 247/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7627 - val_loss: 0.7725\n",
            "Epoch 248/500\n",
            "1035/1035 [==============================] - 1s 791us/step - loss: 0.7628 - val_loss: 0.7719\n",
            "Epoch 249/500\n",
            "1035/1035 [==============================] - 1s 798us/step - loss: 0.7630 - val_loss: 0.7722\n",
            "Epoch 250/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7630 - val_loss: 0.7723\n",
            "Epoch 251/500\n",
            "1035/1035 [==============================] - 1s 787us/step - loss: 0.7628 - val_loss: 0.7723\n",
            "Epoch 252/500\n",
            "1035/1035 [==============================] - 1s 797us/step - loss: 0.7632 - val_loss: 0.7727\n",
            "Epoch 253/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7630 - val_loss: 0.7723\n",
            "Epoch 254/500\n",
            "1035/1035 [==============================] - 1s 789us/step - loss: 0.7628 - val_loss: 0.7725\n",
            "Epoch 255/500\n",
            "1035/1035 [==============================] - 1s 798us/step - loss: 0.7626 - val_loss: 0.7720\n",
            "Epoch 256/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7627 - val_loss: 0.7725\n",
            "Epoch 257/500\n",
            "1035/1035 [==============================] - 1s 786us/step - loss: 0.7626 - val_loss: 0.7725\n",
            "Epoch 258/500\n",
            "1035/1035 [==============================] - 1s 788us/step - loss: 0.7630 - val_loss: 0.7717\n",
            "Epoch 259/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7628 - val_loss: 0.7728\n",
            "Epoch 260/500\n",
            "1035/1035 [==============================] - 1s 797us/step - loss: 0.7626 - val_loss: 0.7722\n",
            "Epoch 261/500\n",
            "1035/1035 [==============================] - 1s 793us/step - loss: 0.7628 - val_loss: 0.7724\n",
            "Epoch 262/500\n",
            "1035/1035 [==============================] - 1s 770us/step - loss: 0.7628 - val_loss: 0.7723\n",
            "Epoch 263/500\n",
            "1035/1035 [==============================] - 1s 779us/step - loss: 0.7630 - val_loss: 0.7715\n",
            "Epoch 264/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7630 - val_loss: 0.7723\n",
            "Epoch 265/500\n",
            "1035/1035 [==============================] - 1s 794us/step - loss: 0.7625 - val_loss: 0.7713\n",
            "Epoch 266/500\n",
            "1035/1035 [==============================] - 1s 797us/step - loss: 0.7631 - val_loss: 0.7715\n",
            "Epoch 267/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7625 - val_loss: 0.7724\n",
            "Epoch 268/500\n",
            "1035/1035 [==============================] - 1s 804us/step - loss: 0.7627 - val_loss: 0.7713\n",
            "Epoch 269/500\n",
            "1035/1035 [==============================] - 1s 769us/step - loss: 0.7627 - val_loss: 0.7720\n",
            "Epoch 270/500\n",
            "1035/1035 [==============================] - 1s 765us/step - loss: 0.7626 - val_loss: 0.7724\n",
            "Epoch 271/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7626 - val_loss: 0.7724\n",
            "Epoch 272/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7625 - val_loss: 0.7718\n",
            "Epoch 273/500\n",
            "1035/1035 [==============================] - 1s 763us/step - loss: 0.7625 - val_loss: 0.7724\n",
            "Epoch 274/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7628 - val_loss: 0.7708\n",
            "Epoch 275/500\n",
            "1035/1035 [==============================] - 1s 805us/step - loss: 0.7624 - val_loss: 0.7728\n",
            "Epoch 276/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7624 - val_loss: 0.7718\n",
            "Epoch 277/500\n",
            "1035/1035 [==============================] - 1s 785us/step - loss: 0.7624 - val_loss: 0.7726\n",
            "Epoch 278/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7626 - val_loss: 0.7714\n",
            "Epoch 279/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7628 - val_loss: 0.7718\n",
            "Epoch 280/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7626 - val_loss: 0.7721\n",
            "Epoch 281/500\n",
            "1035/1035 [==============================] - 1s 787us/step - loss: 0.7623 - val_loss: 0.7723\n",
            "Epoch 282/500\n",
            "1035/1035 [==============================] - 1s 782us/step - loss: 0.7625 - val_loss: 0.7725\n",
            "Epoch 283/500\n",
            "1035/1035 [==============================] - 1s 793us/step - loss: 0.7628 - val_loss: 0.7728\n",
            "Epoch 284/500\n",
            "1035/1035 [==============================] - 1s 787us/step - loss: 0.7626 - val_loss: 0.7729\n",
            "Epoch 285/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7622 - val_loss: 0.7725\n",
            "Epoch 286/500\n",
            "1035/1035 [==============================] - 1s 792us/step - loss: 0.7622 - val_loss: 0.7718\n",
            "Epoch 287/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7623 - val_loss: 0.7720\n",
            "Epoch 288/500\n",
            "1035/1035 [==============================] - 1s 844us/step - loss: 0.7627 - val_loss: 0.7729\n",
            "Epoch 289/500\n",
            "1035/1035 [==============================] - 1s 795us/step - loss: 0.7623 - val_loss: 0.7731\n",
            "Epoch 290/500\n",
            "1035/1035 [==============================] - 1s 782us/step - loss: 0.7626 - val_loss: 0.7717\n",
            "Epoch 291/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7629 - val_loss: 0.7722\n",
            "Epoch 292/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7625 - val_loss: 0.7715\n",
            "Epoch 293/500\n",
            "1035/1035 [==============================] - 1s 762us/step - loss: 0.7626 - val_loss: 0.7722\n",
            "Epoch 294/500\n",
            "1035/1035 [==============================] - 1s 799us/step - loss: 0.7623 - val_loss: 0.7716\n",
            "Epoch 295/500\n",
            "1035/1035 [==============================] - 1s 796us/step - loss: 0.7622 - val_loss: 0.7721\n",
            "Epoch 296/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7621 - val_loss: 0.7722\n",
            "Epoch 297/500\n",
            "1035/1035 [==============================] - 1s 787us/step - loss: 0.7623 - val_loss: 0.7724\n",
            "Epoch 298/500\n",
            "1035/1035 [==============================] - 1s 794us/step - loss: 0.7624 - val_loss: 0.7722\n",
            "Epoch 299/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7624 - val_loss: 0.7719\n",
            "Epoch 300/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7621 - val_loss: 0.7709\n",
            "Epoch 301/500\n",
            "1035/1035 [==============================] - 1s 796us/step - loss: 0.7624 - val_loss: 0.7709\n",
            "Epoch 302/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7625 - val_loss: 0.7712\n",
            "Epoch 303/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7623 - val_loss: 0.7727\n",
            "Epoch 304/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7625 - val_loss: 0.7721\n",
            "Epoch 305/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7627 - val_loss: 0.7727\n",
            "Epoch 306/500\n",
            "1035/1035 [==============================] - 1s 789us/step - loss: 0.7625 - val_loss: 0.7726\n",
            "Epoch 307/500\n",
            "1035/1035 [==============================] - 1s 753us/step - loss: 0.7621 - val_loss: 0.7715\n",
            "Epoch 308/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7625 - val_loss: 0.7723\n",
            "Epoch 309/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7626 - val_loss: 0.7721\n",
            "Epoch 310/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7623 - val_loss: 0.7714\n",
            "Epoch 311/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7621 - val_loss: 0.7710\n",
            "Epoch 312/500\n",
            "1035/1035 [==============================] - 1s 779us/step - loss: 0.7619 - val_loss: 0.7716\n",
            "Epoch 313/500\n",
            "1035/1035 [==============================] - 1s 762us/step - loss: 0.7624 - val_loss: 0.7717\n",
            "Epoch 314/500\n",
            "1035/1035 [==============================] - 1s 781us/step - loss: 0.7624 - val_loss: 0.7723\n",
            "Epoch 315/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7629 - val_loss: 0.7726\n",
            "Epoch 316/500\n",
            "1035/1035 [==============================] - 1s 756us/step - loss: 0.7622 - val_loss: 0.7720\n",
            "Epoch 317/500\n",
            "1035/1035 [==============================] - 1s 769us/step - loss: 0.7623 - val_loss: 0.7729\n",
            "Epoch 318/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7626 - val_loss: 0.7728\n",
            "Epoch 319/500\n",
            "1035/1035 [==============================] - 1s 759us/step - loss: 0.7623 - val_loss: 0.7733\n",
            "Epoch 320/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7620 - val_loss: 0.7718\n",
            "Epoch 321/500\n",
            "1035/1035 [==============================] - 1s 768us/step - loss: 0.7620 - val_loss: 0.7727\n",
            "Epoch 322/500\n",
            "1035/1035 [==============================] - 1s 789us/step - loss: 0.7624 - val_loss: 0.7718\n",
            "Epoch 323/500\n",
            "1035/1035 [==============================] - 1s 785us/step - loss: 0.7623 - val_loss: 0.7705\n",
            "Epoch 324/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7623 - val_loss: 0.7710\n",
            "Epoch 325/500\n",
            "1035/1035 [==============================] - 1s 768us/step - loss: 0.7625 - val_loss: 0.7719\n",
            "Epoch 326/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7626 - val_loss: 0.7726\n",
            "Epoch 327/500\n",
            "1035/1035 [==============================] - 1s 796us/step - loss: 0.7622 - val_loss: 0.7717\n",
            "Epoch 328/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7622 - val_loss: 0.7725\n",
            "Epoch 329/500\n",
            "1035/1035 [==============================] - 1s 785us/step - loss: 0.7621 - val_loss: 0.7732\n",
            "Epoch 330/500\n",
            "1035/1035 [==============================] - 1s 764us/step - loss: 0.7617 - val_loss: 0.7724\n",
            "Epoch 331/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7618 - val_loss: 0.7750\n",
            "Epoch 332/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7623 - val_loss: 0.7734\n",
            "Epoch 333/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7620 - val_loss: 0.7723\n",
            "Epoch 334/500\n",
            "1035/1035 [==============================] - 1s 789us/step - loss: 0.7620 - val_loss: 0.7728\n",
            "Epoch 335/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7618 - val_loss: 0.7716\n",
            "Epoch 336/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7617 - val_loss: 0.7719\n",
            "Epoch 337/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7618 - val_loss: 0.7712\n",
            "Epoch 338/500\n",
            "1035/1035 [==============================] - 1s 764us/step - loss: 0.7617 - val_loss: 0.7714\n",
            "Epoch 339/500\n",
            "1035/1035 [==============================] - 1s 768us/step - loss: 0.7623 - val_loss: 0.7723\n",
            "Epoch 340/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7618 - val_loss: 0.7723\n",
            "Epoch 341/500\n",
            "1035/1035 [==============================] - 1s 749us/step - loss: 0.7616 - val_loss: 0.7730\n",
            "Epoch 342/500\n",
            "1035/1035 [==============================] - 1s 762us/step - loss: 0.7622 - val_loss: 0.7720\n",
            "Epoch 343/500\n",
            "1035/1035 [==============================] - 1s 761us/step - loss: 0.7619 - val_loss: 0.7719\n",
            "Epoch 344/500\n",
            "1035/1035 [==============================] - 1s 755us/step - loss: 0.7622 - val_loss: 0.7726\n",
            "Epoch 345/500\n",
            "1035/1035 [==============================] - 1s 762us/step - loss: 0.7620 - val_loss: 0.7729\n",
            "Epoch 346/500\n",
            "1035/1035 [==============================] - 1s 756us/step - loss: 0.7621 - val_loss: 0.7720\n",
            "Epoch 347/500\n",
            "1035/1035 [==============================] - 1s 764us/step - loss: 0.7617 - val_loss: 0.7715\n",
            "Epoch 348/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7623 - val_loss: 0.7718\n",
            "Epoch 349/500\n",
            "1035/1035 [==============================] - 1s 755us/step - loss: 0.7616 - val_loss: 0.7712\n",
            "Epoch 350/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7620 - val_loss: 0.7721\n",
            "Epoch 351/500\n",
            "1035/1035 [==============================] - 1s 765us/step - loss: 0.7620 - val_loss: 0.7722\n",
            "Epoch 352/500\n",
            "1035/1035 [==============================] - 1s 741us/step - loss: 0.7618 - val_loss: 0.7714\n",
            "Epoch 353/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7625 - val_loss: 0.7719\n",
            "Epoch 354/500\n",
            "1035/1035 [==============================] - 1s 768us/step - loss: 0.7624 - val_loss: 0.7723\n",
            "Epoch 355/500\n",
            "1035/1035 [==============================] - 1s 748us/step - loss: 0.7619 - val_loss: 0.7719\n",
            "Epoch 356/500\n",
            "1035/1035 [==============================] - 1s 762us/step - loss: 0.7622 - val_loss: 0.7712\n",
            "Epoch 357/500\n",
            "1035/1035 [==============================] - 1s 754us/step - loss: 0.7617 - val_loss: 0.7724\n",
            "Epoch 358/500\n",
            "1035/1035 [==============================] - 1s 755us/step - loss: 0.7618 - val_loss: 0.7721\n",
            "Epoch 359/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 0.7614 - val_loss: 0.7711\n",
            "Epoch 360/500\n",
            "1035/1035 [==============================] - 1s 746us/step - loss: 0.7618 - val_loss: 0.7727\n",
            "Epoch 361/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7623 - val_loss: 0.7727\n",
            "Epoch 362/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7618 - val_loss: 0.7725\n",
            "Epoch 363/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7618 - val_loss: 0.7721\n",
            "Epoch 364/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7617 - val_loss: 0.7722\n",
            "Epoch 365/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7621 - val_loss: 0.7726\n",
            "Epoch 366/500\n",
            "1035/1035 [==============================] - 1s 754us/step - loss: 0.7617 - val_loss: 0.7725\n",
            "Epoch 367/500\n",
            "1035/1035 [==============================] - 1s 824us/step - loss: 0.7622 - val_loss: 0.7728\n",
            "Epoch 368/500\n",
            "1035/1035 [==============================] - 1s 770us/step - loss: 0.7620 - val_loss: 0.7718\n",
            "Epoch 369/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7620 - val_loss: 0.7714\n",
            "Epoch 370/500\n",
            "1035/1035 [==============================] - 1s 788us/step - loss: 0.7619 - val_loss: 0.7722\n",
            "Epoch 371/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7621 - val_loss: 0.7728\n",
            "Epoch 372/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7620 - val_loss: 0.7718\n",
            "Epoch 373/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7618 - val_loss: 0.7724\n",
            "Epoch 374/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 0.7619 - val_loss: 0.7730\n",
            "Epoch 375/500\n",
            "1035/1035 [==============================] - 1s 779us/step - loss: 0.7621 - val_loss: 0.7722\n",
            "Epoch 376/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 0.7620 - val_loss: 0.7729\n",
            "Epoch 377/500\n",
            "1035/1035 [==============================] - 1s 772us/step - loss: 0.7618 - val_loss: 0.7728\n",
            "Epoch 378/500\n",
            "1035/1035 [==============================] - 1s 779us/step - loss: 0.7615 - val_loss: 0.7722\n",
            "Epoch 379/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7615 - val_loss: 0.7728\n",
            "Epoch 380/500\n",
            "1035/1035 [==============================] - 1s 763us/step - loss: 0.7614 - val_loss: 0.7728\n",
            "Epoch 381/500\n",
            "1035/1035 [==============================] - 1s 782us/step - loss: 0.7616 - val_loss: 0.7719\n",
            "Epoch 382/500\n",
            "1035/1035 [==============================] - 1s 753us/step - loss: 0.7618 - val_loss: 0.7724\n",
            "Epoch 383/500\n",
            "1035/1035 [==============================] - 1s 793us/step - loss: 0.7619 - val_loss: 0.7715\n",
            "Epoch 384/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7620 - val_loss: 0.7726\n",
            "Epoch 385/500\n",
            "1035/1035 [==============================] - 1s 769us/step - loss: 0.7616 - val_loss: 0.7724\n",
            "Epoch 386/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7621 - val_loss: 0.7727\n",
            "Epoch 387/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7620 - val_loss: 0.7721\n",
            "Epoch 388/500\n",
            "1035/1035 [==============================] - 1s 760us/step - loss: 0.7618 - val_loss: 0.7712\n",
            "Epoch 389/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7614 - val_loss: 0.7728\n",
            "Epoch 390/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7617 - val_loss: 0.7719\n",
            "Epoch 391/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7620 - val_loss: 0.7719\n",
            "Epoch 392/500\n",
            "1035/1035 [==============================] - 1s 790us/step - loss: 0.7618 - val_loss: 0.7715\n",
            "Epoch 393/500\n",
            "1035/1035 [==============================] - 1s 807us/step - loss: 0.7617 - val_loss: 0.7726\n",
            "Epoch 394/500\n",
            "1035/1035 [==============================] - 1s 760us/step - loss: 0.7616 - val_loss: 0.7718\n",
            "Epoch 395/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7616 - val_loss: 0.7728\n",
            "Epoch 396/500\n",
            "1035/1035 [==============================] - 1s 760us/step - loss: 0.7619 - val_loss: 0.7735\n",
            "Epoch 397/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7618 - val_loss: 0.7721\n",
            "Epoch 398/500\n",
            "1035/1035 [==============================] - 1s 764us/step - loss: 0.7617 - val_loss: 0.7719\n",
            "Epoch 399/500\n",
            "1035/1035 [==============================] - 1s 751us/step - loss: 0.7618 - val_loss: 0.7723\n",
            "Epoch 400/500\n",
            "1035/1035 [==============================] - 1s 763us/step - loss: 0.7616 - val_loss: 0.7721\n",
            "Epoch 401/500\n",
            "1035/1035 [==============================] - 1s 758us/step - loss: 0.7616 - val_loss: 0.7714\n",
            "Epoch 402/500\n",
            "1035/1035 [==============================] - 1s 748us/step - loss: 0.7616 - val_loss: 0.7744\n",
            "Epoch 403/500\n",
            "1035/1035 [==============================] - 1s 763us/step - loss: 0.7617 - val_loss: 0.7722\n",
            "Epoch 404/500\n",
            "1035/1035 [==============================] - 1s 747us/step - loss: 0.7617 - val_loss: 0.7732\n",
            "Epoch 405/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7615 - val_loss: 0.7714\n",
            "Epoch 406/500\n",
            "1035/1035 [==============================] - 1s 795us/step - loss: 0.7613 - val_loss: 0.7721\n",
            "Epoch 407/500\n",
            "1035/1035 [==============================] - 1s 760us/step - loss: 0.7618 - val_loss: 0.7720\n",
            "Epoch 408/500\n",
            "1035/1035 [==============================] - 1s 796us/step - loss: 0.7614 - val_loss: 0.7716\n",
            "Epoch 409/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7618 - val_loss: 0.7722\n",
            "Epoch 410/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7613 - val_loss: 0.7719\n",
            "Epoch 411/500\n",
            "1035/1035 [==============================] - 1s 782us/step - loss: 0.7615 - val_loss: 0.7716\n",
            "Epoch 412/500\n",
            "1035/1035 [==============================] - 1s 787us/step - loss: 0.7620 - val_loss: 0.7733\n",
            "Epoch 413/500\n",
            "1035/1035 [==============================] - 1s 768us/step - loss: 0.7614 - val_loss: 0.7734\n",
            "Epoch 414/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7613 - val_loss: 0.7730\n",
            "Epoch 415/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7615 - val_loss: 0.7722\n",
            "Epoch 416/500\n",
            "1035/1035 [==============================] - 1s 770us/step - loss: 0.7623 - val_loss: 0.7749\n",
            "Epoch 417/500\n",
            "1035/1035 [==============================] - 1s 770us/step - loss: 0.7618 - val_loss: 0.7733\n",
            "Epoch 418/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7617 - val_loss: 0.7732\n",
            "Epoch 419/500\n",
            "1035/1035 [==============================] - 1s 772us/step - loss: 0.7618 - val_loss: 0.7731\n",
            "Epoch 420/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7614 - val_loss: 0.7735\n",
            "Epoch 421/500\n",
            "1035/1035 [==============================] - 1s 765us/step - loss: 0.7615 - val_loss: 0.7738\n",
            "Epoch 422/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7615 - val_loss: 0.7729\n",
            "Epoch 423/500\n",
            "1035/1035 [==============================] - 1s 782us/step - loss: 0.7614 - val_loss: 0.7739\n",
            "Epoch 424/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 0.7615 - val_loss: 0.7728\n",
            "Epoch 425/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7614 - val_loss: 0.7731\n",
            "Epoch 426/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7614 - val_loss: 0.7723\n",
            "Epoch 427/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 0.7619 - val_loss: 0.7733\n",
            "Epoch 428/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7614 - val_loss: 0.7733\n",
            "Epoch 429/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7615 - val_loss: 0.7723\n",
            "Epoch 430/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 0.7620 - val_loss: 0.7748\n",
            "Epoch 431/500\n",
            "1035/1035 [==============================] - 1s 779us/step - loss: 0.7618 - val_loss: 0.7734\n",
            "Epoch 432/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7615 - val_loss: 0.7728\n",
            "Epoch 433/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7611 - val_loss: 0.7731\n",
            "Epoch 434/500\n",
            "1035/1035 [==============================] - 1s 777us/step - loss: 0.7614 - val_loss: 0.7726\n",
            "Epoch 435/500\n",
            "1035/1035 [==============================] - 1s 769us/step - loss: 0.7614 - val_loss: 0.7734\n",
            "Epoch 436/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 0.7613 - val_loss: 0.7736\n",
            "Epoch 437/500\n",
            "1035/1035 [==============================] - 1s 765us/step - loss: 0.7613 - val_loss: 0.7728\n",
            "Epoch 438/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 0.7613 - val_loss: 0.7734\n",
            "Epoch 439/500\n",
            "1035/1035 [==============================] - 1s 772us/step - loss: 0.7614 - val_loss: 0.7749\n",
            "Epoch 440/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7619 - val_loss: 0.7739\n",
            "Epoch 441/500\n",
            "1035/1035 [==============================] - 1s 748us/step - loss: 0.7612 - val_loss: 0.7739\n",
            "Epoch 442/500\n",
            "1035/1035 [==============================] - 1s 763us/step - loss: 0.7614 - val_loss: 0.7738\n",
            "Epoch 443/500\n",
            "1035/1035 [==============================] - 1s 749us/step - loss: 0.7619 - val_loss: 0.7734\n",
            "Epoch 444/500\n",
            "1035/1035 [==============================] - 1s 746us/step - loss: 0.7611 - val_loss: 0.7739\n",
            "Epoch 445/500\n",
            "1035/1035 [==============================] - 1s 760us/step - loss: 0.7613 - val_loss: 0.7731\n",
            "Epoch 446/500\n",
            "1035/1035 [==============================] - 1s 761us/step - loss: 0.7614 - val_loss: 0.7730\n",
            "Epoch 447/500\n",
            "1035/1035 [==============================] - 1s 752us/step - loss: 0.7621 - val_loss: 0.7743\n",
            "Epoch 448/500\n",
            "1035/1035 [==============================] - 1s 770us/step - loss: 0.7616 - val_loss: 0.7732\n",
            "Epoch 449/500\n",
            "1035/1035 [==============================] - 1s 760us/step - loss: 0.7616 - val_loss: 0.7751\n",
            "Epoch 450/500\n",
            "1035/1035 [==============================] - 1s 763us/step - loss: 0.7614 - val_loss: 0.7738\n",
            "Epoch 451/500\n",
            "1035/1035 [==============================] - 1s 759us/step - loss: 0.7620 - val_loss: 0.7753\n",
            "Epoch 452/500\n",
            "1035/1035 [==============================] - 1s 751us/step - loss: 0.7615 - val_loss: 0.7734\n",
            "Epoch 453/500\n",
            "1035/1035 [==============================] - 1s 769us/step - loss: 0.7613 - val_loss: 0.7745\n",
            "Epoch 454/500\n",
            "1035/1035 [==============================] - 1s 745us/step - loss: 0.7612 - val_loss: 0.7733\n",
            "Epoch 455/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7611 - val_loss: 0.7727\n",
            "Epoch 456/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 0.7610 - val_loss: 0.7731\n",
            "Epoch 457/500\n",
            "1035/1035 [==============================] - 1s 761us/step - loss: 0.7615 - val_loss: 0.7739\n",
            "Epoch 458/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7613 - val_loss: 0.7741\n",
            "Epoch 459/500\n",
            "1035/1035 [==============================] - 1s 768us/step - loss: 0.7619 - val_loss: 0.7738\n",
            "Epoch 460/500\n",
            "1035/1035 [==============================] - 1s 768us/step - loss: 0.7620 - val_loss: 0.7747\n",
            "Epoch 461/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 0.7612 - val_loss: 0.7740\n",
            "Epoch 462/500\n",
            "1035/1035 [==============================] - 1s 764us/step - loss: 0.7615 - val_loss: 0.7730\n",
            "Epoch 463/500\n",
            "1035/1035 [==============================] - 1s 765us/step - loss: 0.7613 - val_loss: 0.7734\n",
            "Epoch 464/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7615 - val_loss: 0.7726\n",
            "Epoch 465/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7615 - val_loss: 0.7735\n",
            "Epoch 466/500\n",
            "1035/1035 [==============================] - 1s 782us/step - loss: 0.7618 - val_loss: 0.7736\n",
            "Epoch 467/500\n",
            "1035/1035 [==============================] - 1s 774us/step - loss: 0.7615 - val_loss: 0.7747\n",
            "Epoch 468/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7613 - val_loss: 0.7752\n",
            "Epoch 469/500\n",
            "1035/1035 [==============================] - 1s 788us/step - loss: 0.7611 - val_loss: 0.7748\n",
            "Epoch 470/500\n",
            "1035/1035 [==============================] - 1s 786us/step - loss: 0.7610 - val_loss: 0.7753\n",
            "Epoch 471/500\n",
            "1035/1035 [==============================] - 1s 787us/step - loss: 0.7612 - val_loss: 0.7746\n",
            "Epoch 472/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7614 - val_loss: 0.7738\n",
            "Epoch 473/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7612 - val_loss: 0.7754\n",
            "Epoch 474/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 0.7615 - val_loss: 0.7751\n",
            "Epoch 475/500\n",
            "1035/1035 [==============================] - 1s 765us/step - loss: 0.7613 - val_loss: 0.7755\n",
            "Epoch 476/500\n",
            "1035/1035 [==============================] - 1s 795us/step - loss: 0.7614 - val_loss: 0.7747\n",
            "Epoch 477/500\n",
            "1035/1035 [==============================] - 1s 745us/step - loss: 0.7614 - val_loss: 0.7744\n",
            "Epoch 478/500\n",
            "1035/1035 [==============================] - 1s 778us/step - loss: 0.7616 - val_loss: 0.7734\n",
            "Epoch 479/500\n",
            "1035/1035 [==============================] - 1s 764us/step - loss: 0.7611 - val_loss: 0.7742\n",
            "Epoch 480/500\n",
            "1035/1035 [==============================] - 1s 772us/step - loss: 0.7611 - val_loss: 0.7752\n",
            "Epoch 481/500\n",
            "1035/1035 [==============================] - 1s 781us/step - loss: 0.7611 - val_loss: 0.7731\n",
            "Epoch 482/500\n",
            "1035/1035 [==============================] - 1s 756us/step - loss: 0.7612 - val_loss: 0.7723\n",
            "Epoch 483/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7612 - val_loss: 0.7732\n",
            "Epoch 484/500\n",
            "1035/1035 [==============================] - 1s 813us/step - loss: 0.7617 - val_loss: 0.7726\n",
            "Epoch 485/500\n",
            "1035/1035 [==============================] - 1s 809us/step - loss: 0.7616 - val_loss: 0.7717\n",
            "Epoch 486/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7614 - val_loss: 0.7736\n",
            "Epoch 487/500\n",
            "1035/1035 [==============================] - 1s 773us/step - loss: 0.7611 - val_loss: 0.7735\n",
            "Epoch 488/500\n",
            "1035/1035 [==============================] - 1s 762us/step - loss: 0.7613 - val_loss: 0.7721\n",
            "Epoch 489/500\n",
            "1035/1035 [==============================] - 1s 783us/step - loss: 0.7611 - val_loss: 0.7724\n",
            "Epoch 490/500\n",
            "1035/1035 [==============================] - 1s 784us/step - loss: 0.7612 - val_loss: 0.7724\n",
            "Epoch 491/500\n",
            "1035/1035 [==============================] - 1s 764us/step - loss: 0.7614 - val_loss: 0.7715\n",
            "Epoch 492/500\n",
            "1035/1035 [==============================] - 1s 755us/step - loss: 0.7614 - val_loss: 0.7731\n",
            "Epoch 493/500\n",
            "1035/1035 [==============================] - 1s 754us/step - loss: 0.7614 - val_loss: 0.7722\n",
            "Epoch 494/500\n",
            "1035/1035 [==============================] - 1s 762us/step - loss: 0.7613 - val_loss: 0.7733\n",
            "Epoch 495/500\n",
            "1035/1035 [==============================] - 1s 771us/step - loss: 0.7613 - val_loss: 0.7739\n",
            "Epoch 496/500\n",
            "1035/1035 [==============================] - 1s 754us/step - loss: 0.7611 - val_loss: 0.7729\n",
            "Epoch 497/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7609 - val_loss: 0.7722\n",
            "Epoch 498/500\n",
            "1035/1035 [==============================] - 1s 767us/step - loss: 0.7610 - val_loss: 0.7729\n",
            "Epoch 499/500\n",
            "1035/1035 [==============================] - 1s 751us/step - loss: 0.7609 - val_loss: 0.7726\n",
            "Epoch 500/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 0.7610 - val_loss: 0.7731\n",
            "(lamda,Threshold) 0.0 0.0\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (1150, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 3532800 0.0\n",
            "The shape of N (1150, 3072)\n",
            "The minimum value of N  -0.88844883\n",
            "The max value of N 0.8830899\n",
            "[INFO:] Xclean  MSE Computed shape (1150, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (1150, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.006914293])\n",
            "[INFO:] The anomaly threshold computed is  0.006914293\n",
            "[INFO:] The shape of input data   (1150, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (1150, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (1150, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/one_class_neural_networks//reports/figures/gtsrb/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [1090, 1130, 1088, 1060, 1124, 1081, 1083, 1111, 1123, 1052, 1057, 1063, 1118, 1067, 1120, 1087, 1142, 1125, 1115, 1126, 1079, 1071, 1095, 1065, 1069, 1138, 1127, 1080, 1116, 1056, 1042, 1146, 1141, 1106, 1066, 1086, 1108, 1117, 1073, 1050, 1149, 1040, 1132, 1145, 512, 988, 1093, 1072, 1133, 1121, 1054, 1139, 1075, 1100, 1091, 1112, 1076, 1061, 1140, 1107, 1018, 1094, 1136, 552, 1122, 1134, 1092, 1089, 356, 1109, 472, 381, 1103, 1070, 1114, 397, 1098, 426, 951, 1064, 105, 885, 359, 584, 522, 1128, 1104, 1085, 410, 1077, 605, 467, 986, 1101, 473, 403, 661, 230, 1110, 1113]\n",
            "[INFO:] The  Positive boundry 1050\n",
            "[INFO:] The  Negative boundry 100\n",
            "[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n",
            "img shape: (160, 320, 3)\n",
            "\n",
            "Saving Top-10 most anomalous digit: @\n",
            "=====================\n",
            "AUROC 0.0 0.875\n",
            "=======================\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 32) (256, 32)\n",
            "Train on 1035 samples, validate on 115 samples\n",
            "Epoch 1/500\n",
            "1035/1035 [==============================] - 11s 11ms/step - loss: 1.0446 - val_loss: 1.0569\n",
            "Epoch 2/500\n",
            "1035/1035 [==============================] - 1s 797us/step - loss: 1.0445 - val_loss: 1.0560\n",
            "Epoch 3/500\n",
            "1035/1035 [==============================] - 1s 802us/step - loss: 1.0448 - val_loss: 1.0555\n",
            "Epoch 4/500\n",
            "1035/1035 [==============================] - 1s 780us/step - loss: 1.0444 - val_loss: 1.0541\n",
            "Epoch 5/500\n",
            "1035/1035 [==============================] - 1s 776us/step - loss: 1.0441 - val_loss: 1.0563\n",
            "Epoch 6/500\n",
            "1035/1035 [==============================] - 1s 775us/step - loss: 1.0442 - val_loss: 1.0559\n",
            "Epoch 7/500\n",
            "1035/1035 [==============================] - 1s 766us/step - loss: 1.0443 - val_loss: 1.0562\n",
            "Epoch 8/500\n",
            "1035/1035 [==============================] - 1s 785us/step - loss: 1.0444 - val_loss: 1.0558\n",
            "Epoch 9/500\n",
            " 192/1035 [====>.........................] - ETA: 0s - loss: 1.0451"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YbzSYpADbNPR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pretrain Autoencoder"
      ]
    },
    {
      "metadata": {
        "id": "aM_sbMGrbNPS",
        "colab_type": "code",
        "colab": {},
        "outputId": "3203a2d9-9007-4ec4-c5fc-28769f42c376"
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "DATASET = \"MNIST\"\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_CHANNEL=1\n",
        "HIDDEN_LAYER_SIZE= 128\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/Deep_SVDD/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/Deep_SVDD/\"\n",
        "PRETRAINED_WT_PATH = \"\"\n",
        "\n",
        "## Prepare the data for pretraining CAE\n",
        "x_train = trainX.reshape((len(trainX), 28, 28, 1))\n",
        "x_trainForWtInit= x_train\n",
        "\n",
        "test_ones = test_ones.reshape((len(test_ones), 28, 28, 1))\n",
        "test_sevens = test_sevens.reshape((len(test_sevens), 28, 28, 1))\n",
        "x_test = np.concatenate((test_ones,test_sevens))\n",
        "\n",
        "print(\"Reshaped Training samples for CAE\",x_train.shape)\n",
        "print(\"Reshaped Testing samples for CAE\",x_test.shape)\n",
        "\n",
        "from src.models.Deep_SVDD import Deep_SVDD\n",
        "deep_svdd =   Deep_SVDD(DATASET,x_trainForWtInit,IMG_DIM,HIDDEN_LAYER_SIZE,IMG_HGT,IMG_WDT,IMG_CHANNEL,MODEL_SAVE_PATH,REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainX' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-7deea1159869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m## Prepare the data for pretraining CAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mx_trainForWtInit\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainX' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "kEnNSUJQbNPV",
        "colab_type": "code",
        "colab": {},
        "outputId": "520f1bed-597e-46ae-a7bb-4f24b201852b"
      },
      "cell_type": "code",
      "source": [
        "Deep_SVDD()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "CdlqJ_4AbNPY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F2EKpZ78bNPa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1HYntufnbNPc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YCigNvEJbNPe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BqIyZSRNbNPg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train and Test  FF_NN Model Supervised Model"
      ]
    },
    {
      "metadata": {
        "id": "jSTEBas9bNPh",
        "colab_type": "code",
        "colab": {},
        "outputId": "528454c6-82d3-4e8d-b57e-f7da17c68216"
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_DEPTH=1\n",
        "HIDDEN_LAYER_SIZE=196\n",
        "\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/FF_NN/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/FF_NN/\"\n",
        "\n",
        "print(\"[INFO]\",train_Anomaly_X.shape[0],\"Anomalous Samples Appended to training set\")\n",
        "data_train = np.concatenate((trainX,train_Anomaly_X),axis=0)\n",
        "data_train_label = np.concatenate((trainY,train_Anomaly_Y),axis=0)\n",
        "print(\"[INFO]\",data_train.shape[0],\"Training Samples Contains both 1's and 7s\")\n",
        "nClass =2\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "clf_FF_NN =  FF_NN(IMG_DIM,HIDDEN_LAYER_SIZE,IMG_HGT,IMG_WDT,MODEL_SAVE_PATH,REPORT_SAVE_PATH)\n",
        "clf_FF_NN.fit(data_train,data_train_label,NUM_EPOCHS,IMG_HGT,IMG_WDT,IMG_DEPTH,nClass)\n",
        "\n",
        "## Predict the scores \n",
        "auc_FF_NN = clf_FF_NN.score(test_ones,label_ones,test_sevens,label_sevens)\n",
        "print(\"===========\")\n",
        "print(\"AUC: \",auc_FF_NN)\n",
        "print(\"===========\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_Anomaly_X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-cee4a7dae6a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mREPORT_SAVE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPROJECT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/reports/figures/MNIST/FF_NN/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Anomaly_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Anomalous Samples Appended to training set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Anomaly_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdata_train_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Anomaly_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_Anomaly_X' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "9P6cnpdSbNPj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## FakeNoise FF_NN Model"
      ]
    },
    {
      "metadata": {
        "id": "_XcgWlBObNPk",
        "colab_type": "code",
        "colab": {},
        "outputId": "38ab74f6-ad64-4e87-dc4e-5c139342f81f"
      },
      "cell_type": "code",
      "source": [
        "## Fake Noise data to be generated which will be added to the training set before training\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_DEPTH=1\n",
        "HIDDEN_LAYER_SIZE=196\n",
        "\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/FAKE_NOISE_FF_NN/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/FAKE_NOISE_FF_NN/\"\n",
        "\n",
        "\n",
        "from src.models.Fake_Noise_FF_NN import Fake_Noise_FF_NN\n",
        "## Remove the Anomalous data and instead add Noise\n",
        "X_Noise,X_NoiseLabel = createData.get_FAKE_Noise_MNIST_TrainingData(trainX)\n",
        "print(\"[INFO]\",X_Noise.shape[0],\"Noise Samples Appended for training set\")\n",
        "data_train = np.concatenate((trainX,X_Noise),axis=0)\n",
        "data_train_label = np.concatenate((trainY,X_NoiseLabel),axis=0)\n",
        "\n",
        "\n",
        "clf_FakeNoise_FF_NN =   Fake_Noise_FF_NN(IMG_DIM,HIDDEN_LAYER_SIZE,IMG_HGT,IMG_WDT,MODEL_SAVE_PATH,REPORT_SAVE_PATH)\n",
        "clf_FakeNoise_FF_NN.fit(data_train,data_train_label,NUM_EPOCHS,IMG_HGT,IMG_WDT,IMG_DEPTH,nClass)\n",
        "# Predict the scores \n",
        "\n",
        "auc_FAKENOISE_FF_NN = clf_FakeNoise_FF_NN.score(test_ones,label_ones,test_sevens,label_sevens)\n",
        "print(\"===========\")\n",
        "print(\"AUC: \",auc_FAKENOISE_FF_NN)\n",
        "print(\"===========\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] 5000 Noise Samples Appended for training set\n",
            "[INFO] compiling model...\n",
            "[INFO] training network...\n",
            "[INFO] serializing network...\n",
            "[INFO] loading network...\n",
            "5050 Actual test samples\n",
            "5050 Predicted test samples\n",
            "===================================\n",
            "auccary_score: 0.8998019801980198\n",
            "roc_auc_score: 0.5534\n",
            "y_true [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "y_pred [1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0]\n",
            "===================================\n",
            "===========\n",
            "AUC:  0.5534\n",
            "===========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "QbTzfZFwbNPn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SKlearn OCSVM"
      ]
    },
    {
      "metadata": {
        "id": "D1HEw6K4bNPn",
        "colab_type": "code",
        "colab": {},
        "outputId": "f8dc75c8-3ca7-41fb-897a-811b388b3ab0"
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "##create the classifier\n",
        "from src.models.ocsvmSklearn import OCSVM\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "ocsvm = OCSVM(IMG_HGT,IMG_WDT)\n",
        "nu= 0.01\n",
        "kernel = 'linear'\n",
        "clf = ocsvm.fit(trainX,nu,kernel)\n",
        "res = ocsvm.score(clf,test_ones,test_sevens)\n",
        "auc_OCSVM_linear = res\n",
        "print(\"=\"*35)\n",
        "print(\"AUC:\",res)\n",
        "print(\"=\"*35)\n",
        "\n",
        "kernel = 'rbf'\n",
        "clf = ocsvm.fit(trainX,nu,kernel)\n",
        "res = ocsvm.score(clf,test_ones,test_sevens)\n",
        "auc_OCSVM_rbf = res\n",
        "print(\"=\"*35)\n",
        "print(\"AUC:\",res)\n",
        "print(\"=\"*35)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the OCSVM classifier.....\n",
            "===================================\n",
            "AUC: 0.7869240000000001\n",
            "===================================\n",
            "Training the OCSVM classifier.....\n",
            "===================================\n",
            "AUC: 0.9705560000000001\n",
            "===================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "maveTr_JbNPp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## OC-NN"
      ]
    },
    {
      "metadata": {
        "id": "XVNt7sZwbNPq",
        "colab_type": "code",
        "colab": {},
        "outputId": "3e8538c9-c5e6-4d59-c307-69f8efc8d2db"
      },
      "cell_type": "code",
      "source": [
        "##create the classifier\n",
        "## Instantiate the object and call the function\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "DATASET= \"MNIST\"\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_DEPTH=1\n",
        "HIDDEN_LAYER_SIZE=196\n",
        "nClass=2\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/OC_NN/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/OC_NN/\"\n",
        "PRE_TRAINED_WT_PATH = PROJECT_DIR +\"/models/MNIST/FF_NN/\"\n",
        "\n",
        "from src.models.OC_NN import OC_NN\n",
        "import keras\n",
        "\n",
        "ocnn = OC_NN(DATASET,IMG_DIM,HIDDEN_LAYER_SIZE,IMG_HGT,IMG_WDT,MODEL_SAVE_PATH,REPORT_SAVE_PATH,PRE_TRAINED_WT_PATH)\n",
        "\n",
        "\n",
        "nu= 0.01\n",
        "NUM_EPOCHS = 100\n",
        "ocnn.fit(trainX,nu,NUM_EPOCHS,IMG_HGT,IMG_WDT,IMG_DEPTH,nClass)\n",
        "res = ocnn.score(test_ones,test_sevens) \n",
        "auc_OCNN = res\n",
        "\n",
        "print(\"=\"*35)\n",
        "print(\"AUC:\",res)\n",
        "print(\"=\"*35)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] compiling model...\n",
            "[INFO]  (196,) input  --> hidden layer weights shape ...\n",
            "[INFO]  (2,) hidden --> output layer weights shape ...\n",
            "[INFO] training network...\n",
            "Train on 4500 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "4500/4500 [==============================] - 0s 63us/step - loss: 44.2036 - val_loss: 43.6791\n",
            "evaluation for epoch: 0\n",
            "output: Tensor(\"Print:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 2/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.1708 - val_loss: 43.7103\n",
            "evaluation for epoch: 1\n",
            "output: Tensor(\"Print_1:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 3/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.1486 - val_loss: 43.7392\n",
            "evaluation for epoch: 2\n",
            "output: Tensor(\"Print_2:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 4/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.1274 - val_loss: 43.7661\n",
            "evaluation for epoch: 3\n",
            "output: Tensor(\"Print_3:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 5/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.1070 - val_loss: 43.7910\n",
            "evaluation for epoch: 4\n",
            "output: Tensor(\"Print_4:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 6/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 44.0875 - val_loss: 43.8141\n",
            "evaluation for epoch: 5\n",
            "output: Tensor(\"Print_5:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 7/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.0683 - val_loss: 43.8355\n",
            "evaluation for epoch: 6\n",
            "output: Tensor(\"Print_6:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 8/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.0497 - val_loss: 43.8560\n",
            "evaluation for epoch: 7\n",
            "output: Tensor(\"Print_7:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 9/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.0315 - val_loss: 43.8742\n",
            "evaluation for epoch: 8\n",
            "output: Tensor(\"Print_8:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 10/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.0135 - val_loss: 43.8920\n",
            "evaluation for epoch: 9\n",
            "output: Tensor(\"Print_9:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 11/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.9958 - val_loss: 43.9087\n",
            "evaluation for epoch: 10\n",
            "output: Tensor(\"Print_10:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 12/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9783 - val_loss: 43.9235\n",
            "evaluation for epoch: 11\n",
            "output: Tensor(\"Print_11:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 13/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9610 - val_loss: 43.9383\n",
            "evaluation for epoch: 12\n",
            "output: Tensor(\"Print_12:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 14/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9438 - val_loss: 43.9518\n",
            "evaluation for epoch: 13\n",
            "output: Tensor(\"Print_13:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 15/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9268 - val_loss: 43.9646\n",
            "evaluation for epoch: 14\n",
            "output: Tensor(\"Print_14:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 16/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9098 - val_loss: 43.9768\n",
            "evaluation for epoch: 15\n",
            "output: Tensor(\"Print_15:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 17/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.8930 - val_loss: 43.9876\n",
            "evaluation for epoch: 16\n",
            "output: Tensor(\"Print_16:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 18/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.8763 - val_loss: 43.9983\n",
            "evaluation for epoch: 17\n",
            "output: Tensor(\"Print_17:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 19/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.8597 - val_loss: 44.0087\n",
            "evaluation for epoch: 18\n",
            "output: Tensor(\"Print_18:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 20/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.8431 - val_loss: 44.0186\n",
            "evaluation for epoch: 19\n",
            "output: Tensor(\"Print_19:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 21/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.8266 - val_loss: 44.0276\n",
            "evaluation for epoch: 20\n",
            "output: Tensor(\"Print_20:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 22/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.8102 - val_loss: 44.0363\n",
            "evaluation for epoch: 21\n",
            "output: Tensor(\"Print_21:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 23/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.7938 - val_loss: 44.0446\n",
            "evaluation for epoch: 22\n",
            "output: Tensor(\"Print_22:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 24/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.7775 - val_loss: 44.0524\n",
            "evaluation for epoch: 23\n",
            "output: Tensor(\"Print_23:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 25/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.7613 - val_loss: 44.0602\n",
            "evaluation for epoch: 24\n",
            "output: Tensor(\"Print_24:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 26/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 43.7450 - val_loss: 44.0676\n",
            "evaluation for epoch: 25\n",
            "output: Tensor(\"Print_25:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 27/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.7290 - val_loss: 44.0743\n",
            "evaluation for epoch: 26\n",
            "output: Tensor(\"Print_26:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 28/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.7129 - val_loss: 44.0810\n",
            "evaluation for epoch: 27\n",
            "output: Tensor(\"Print_27:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 29/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6969 - val_loss: 44.0873\n",
            "evaluation for epoch: 28\n",
            "output: Tensor(\"Print_28:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 30/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6810 - val_loss: 44.0933\n",
            "evaluation for epoch: 29\n",
            "output: Tensor(\"Print_29:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 31/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6651 - val_loss: 44.0989\n",
            "evaluation for epoch: 30\n",
            "output: Tensor(\"Print_30:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 32/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6492 - val_loss: 44.1047\n",
            "evaluation for epoch: 31\n",
            "output: Tensor(\"Print_31:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 33/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6334 - val_loss: 44.1100\n",
            "evaluation for epoch: 32\n",
            "output: Tensor(\"Print_32:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 34/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6176 - val_loss: 44.1152\n",
            "evaluation for epoch: 33\n",
            "output: Tensor(\"Print_33:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 35/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6019 - val_loss: 44.1201\n",
            "evaluation for epoch: 34\n",
            "output: Tensor(\"Print_34:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 36/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.5862 - val_loss: 44.1254\n",
            "evaluation for epoch: 35\n",
            "output: Tensor(\"Print_35:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 37/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.5705 - val_loss: 44.1299\n",
            "evaluation for epoch: 36\n",
            "output: Tensor(\"Print_36:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 38/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.5548 - val_loss: 44.1340\n",
            "evaluation for epoch: 37\n",
            "output: Tensor(\"Print_37:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 39/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.5392 - val_loss: 44.1384\n",
            "evaluation for epoch: 38\n",
            "output: Tensor(\"Print_38:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 40/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.5235 - val_loss: 44.1428\n",
            "evaluation for epoch: 39\n",
            "output: Tensor(\"Print_39:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 41/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.5081 - val_loss: 44.1472\n",
            "evaluation for epoch: 40\n",
            "output: Tensor(\"Print_40:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 42/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.4925 - val_loss: 44.1508\n",
            "evaluation for epoch: 41\n",
            "output: Tensor(\"Print_41:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 43/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 43.4770 - val_loss: 44.1549\n",
            "evaluation for epoch: 42\n",
            "output: Tensor(\"Print_42:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 44/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.4615 - val_loss: 44.1586\n",
            "evaluation for epoch: 43\n",
            "output: Tensor(\"Print_43:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 45/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.4460 - val_loss: 44.1624\n",
            "evaluation for epoch: 44\n",
            "output: Tensor(\"Print_44:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 46/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.4305 - val_loss: 44.1662\n",
            "evaluation for epoch: 45\n",
            "output: Tensor(\"Print_45:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 47/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.4151 - val_loss: 44.1692\n",
            "evaluation for epoch: 46\n",
            "output: Tensor(\"Print_46:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 48/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.3998 - val_loss: 44.1723\n",
            "evaluation for epoch: 47\n",
            "output: Tensor(\"Print_47:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 49/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.3843 - val_loss: 44.1755\n",
            "evaluation for epoch: 48\n",
            "output: Tensor(\"Print_48:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 50/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.3690 - val_loss: 44.1785\n",
            "evaluation for epoch: 49\n",
            "output: Tensor(\"Print_49:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 51/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.3536 - val_loss: 44.1808\n",
            "evaluation for epoch: 50\n",
            "output: Tensor(\"Print_50:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 52/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.3383 - val_loss: 44.1833\n",
            "evaluation for epoch: 51\n",
            "output: Tensor(\"Print_51:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 53/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.3230 - val_loss: 44.1867\n",
            "evaluation for epoch: 52\n",
            "output: Tensor(\"Print_52:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 54/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.3077 - val_loss: 44.1901\n",
            "evaluation for epoch: 53\n",
            "output: Tensor(\"Print_53:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 55/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.2924 - val_loss: 44.1927\n",
            "evaluation for epoch: 54\n",
            "output: Tensor(\"Print_54:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 56/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.2772 - val_loss: 44.1949\n",
            "evaluation for epoch: 55\n",
            "output: Tensor(\"Print_55:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 57/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.2620 - val_loss: 44.1971\n",
            "evaluation for epoch: 56\n",
            "output: Tensor(\"Print_56:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 58/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.2467 - val_loss: 44.1995\n",
            "evaluation for epoch: 57\n",
            "output: Tensor(\"Print_57:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 59/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.2315 - val_loss: 44.2022\n",
            "evaluation for epoch: 58\n",
            "output: Tensor(\"Print_58:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 60/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.2163 - val_loss: 44.2043\n",
            "evaluation for epoch: 59\n",
            "output: Tensor(\"Print_59:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 61/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.2012 - val_loss: 44.2068\n",
            "evaluation for epoch: 60\n",
            "output: Tensor(\"Print_60:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 62/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.1860 - val_loss: 44.2088\n",
            "evaluation for epoch: 61\n",
            "output: Tensor(\"Print_61:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 63/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1708 - val_loss: 44.2115\n",
            "evaluation for epoch: 62\n",
            "output: Tensor(\"Print_62:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 64/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1557 - val_loss: 44.2135\n",
            "evaluation for epoch: 63\n",
            "output: Tensor(\"Print_63:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 65/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1406 - val_loss: 44.2155\n",
            "evaluation for epoch: 64\n",
            "output: Tensor(\"Print_64:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 66/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1255 - val_loss: 44.2175\n",
            "evaluation for epoch: 65\n",
            "output: Tensor(\"Print_65:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 67/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1105 - val_loss: 44.2191\n",
            "evaluation for epoch: 66\n",
            "output: Tensor(\"Print_66:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 68/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.0953 - val_loss: 44.2209\n",
            "evaluation for epoch: 67\n",
            "output: Tensor(\"Print_67:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 69/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.0802 - val_loss: 44.2227\n",
            "evaluation for epoch: 68\n",
            "output: Tensor(\"Print_68:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 70/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.0652 - val_loss: 44.2248\n",
            "evaluation for epoch: 69\n",
            "output: Tensor(\"Print_69:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 71/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.0502 - val_loss: 44.2263\n",
            "evaluation for epoch: 70\n",
            "output: Tensor(\"Print_70:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 72/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.0350 - val_loss: 44.2279\n",
            "evaluation for epoch: 71\n",
            "output: Tensor(\"Print_71:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 73/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.0201 - val_loss: 44.2291\n",
            "evaluation for epoch: 72\n",
            "output: Tensor(\"Print_72:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 74/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.0051 - val_loss: 44.2304\n",
            "evaluation for epoch: 73\n",
            "output: Tensor(\"Print_73:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 75/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9900 - val_loss: 44.2318\n",
            "evaluation for epoch: 74\n",
            "output: Tensor(\"Print_74:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 76/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9750 - val_loss: 44.2337\n",
            "evaluation for epoch: 75\n",
            "output: Tensor(\"Print_75:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 77/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9601 - val_loss: 44.2353\n",
            "evaluation for epoch: 76\n",
            "output: Tensor(\"Print_76:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 78/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 42.9451 - val_loss: 44.2366\n",
            "evaluation for epoch: 77\n",
            "output: Tensor(\"Print_77:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 79/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9301 - val_loss: 44.2378\n",
            "evaluation for epoch: 78\n",
            "output: Tensor(\"Print_78:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 80/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.9153 - val_loss: 44.2391\n",
            "evaluation for epoch: 79\n",
            "output: Tensor(\"Print_79:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 81/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9004 - val_loss: 44.2408\n",
            "evaluation for epoch: 80\n",
            "output: Tensor(\"Print_80:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 82/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 42.8855 - val_loss: 44.2420\n",
            "evaluation for epoch: 81\n",
            "output: Tensor(\"Print_81:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 83/100\n",
            "4500/4500 [==============================] - 0s 32us/step - loss: 42.8707 - val_loss: 44.2429\n",
            "evaluation for epoch: 82\n",
            "output: Tensor(\"Print_82:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 84/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 42.8558 - val_loss: 44.2441\n",
            "evaluation for epoch: 83\n",
            "output: Tensor(\"Print_83:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 85/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.8410 - val_loss: 44.2451\n",
            "evaluation for epoch: 84\n",
            "output: Tensor(\"Print_84:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 86/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4500/4500 [==============================] - 0s 31us/step - loss: 42.8262 - val_loss: 44.2463\n",
            "evaluation for epoch: 85\n",
            "output: Tensor(\"Print_85:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 87/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.8114 - val_loss: 44.2472\n",
            "evaluation for epoch: 86\n",
            "output: Tensor(\"Print_86:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 88/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.7966 - val_loss: 44.2481\n",
            "evaluation for epoch: 87\n",
            "output: Tensor(\"Print_87:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 89/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.7819 - val_loss: 44.2487\n",
            "evaluation for epoch: 88\n",
            "output: Tensor(\"Print_88:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 90/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.7671 - val_loss: 44.2499\n",
            "evaluation for epoch: 89\n",
            "output: Tensor(\"Print_89:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 91/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.7523 - val_loss: 44.2506\n",
            "evaluation for epoch: 90\n",
            "output: Tensor(\"Print_90:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 92/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.7376 - val_loss: 44.2513\n",
            "evaluation for epoch: 91\n",
            "output: Tensor(\"Print_91:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 93/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.7229 - val_loss: 44.2522\n",
            "evaluation for epoch: 92\n",
            "output: Tensor(\"Print_92:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 94/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.7082 - val_loss: 44.2533\n",
            "evaluation for epoch: 93\n",
            "output: Tensor(\"Print_93:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 95/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.6934 - val_loss: 44.2529\n",
            "evaluation for epoch: 94\n",
            "output: Tensor(\"Print_94:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 96/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6788 - val_loss: 44.2538\n",
            "evaluation for epoch: 95\n",
            "output: Tensor(\"Print_95:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 97/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6641 - val_loss: 44.2544\n",
            "evaluation for epoch: 96\n",
            "output: Tensor(\"Print_96:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 98/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6494 - val_loss: 44.2557\n",
            "evaluation for epoch: 97\n",
            "output: Tensor(\"Print_97:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 99/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6347 - val_loss: 44.2564\n",
            "evaluation for epoch: 98\n",
            "output: Tensor(\"Print_98:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 100/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6200 - val_loss: 44.2568\n",
            "evaluation for epoch: 99\n",
            "output: Tensor(\"Print_99:0\", shape=(?, 2), dtype=float32)\n",
            "[INFO] serializing network and saving trained weights...\n",
            "[INFO] Saving model layer weights...\n",
            "[INFO] loading network...\n",
            "5050 Actual test samples\n",
            "===================================\n",
            "auccary_score: 0.9984158415841584\n",
            "roc_auc_score: 0.9199999999999999\n",
            "y_true [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "y_pred [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0]\n",
            "===================================\n",
            "===================================\n",
            "AUC: 0.9199999999999999\n",
            "===================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/raghav/envPython3/lib/python3.6/site-packages/keras/engine/sequential.py:252: UserWarning: Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
            "  warnings.warn('Network returning invalid probability values. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "3FEq4W6JbNPs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Comparing  AUC scores of various methods"
      ]
    },
    {
      "metadata": {
        "id": "0wXWe_83bNPt",
        "colab_type": "code",
        "colab": {},
        "outputId": "9e164537-99a0-4f45-9294-92ffdc30d7f7"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "auc = np.zeros((1,5))\n",
        "auc[0][0] = auc_FF_NN\n",
        "auc[0][1] = auc_FAKENOISE_FF_NN\n",
        "auc[0][2] = auc_OCSVM_linear\n",
        "auc[0][3] = auc_OCSVM_rbf\n",
        "auc[0][4] = auc_OCNN\n",
        "\n",
        "\n",
        "aucList = [auc_FF_NN,auc_FAKENOISE_FF_NN, auc_OCSVM_linear,auc_OCSVM_rbf, auc_OCNN]\n",
        "\n",
        "index = ['FF_NN', 'Fake_NN', 'OCSVM_L','OCSVM_rbf','OCNN']\n",
        "df = pd.DataFrame({'auc': aucList}, index=index)\n",
        "ax = df.plot.bar(rot=0)\n",
        "\n",
        "plt.ylabel('AUC')\n",
        "plt.xlabel('Methods')\n",
        "plt.title('AUC Comparision for MNIST Dataset ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'AUC Comparision for MNIST Dataset ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "GT0KNEqKbNPv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}