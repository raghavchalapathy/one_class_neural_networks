{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCAE_MNIST_Experiments--5_Vs_All_98pc.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/raghavchalapathy/one_class_neural_networks/blob/master/RCAE_MNIST_Experiments_5_Vs_All_98pc.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "izSliiGYb5NE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6bf063f-4f7a-4dbf-8a3d-03a2357a68b3"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# !pip install fuel\n",
        "# !pip install picklable_itertools"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dzqnWOhKbdZt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5k3HKHTabdZw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "PROJECT_DIR = \"/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/\"\n",
        "import sys,os\n",
        "import numpy as np\n",
        "sys.path.append(PROJECT_DIR)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VxmHxqV97ain",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Produce Embeddings for RCAE: Used as pretrained Autoencoder**"
      ]
    },
    {
      "metadata": {
        "id": "do_jek9Q7W5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65662
        },
        "outputId": "de43a51c-f0ca-4a91-ca28-82ff76d9f17d"
      },
      "cell_type": "code",
      "source": [
        "## Obtaining the training and testing data\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from src.models.RCAE import RCAE_AD\n",
        "\n",
        "DATASET = \"mnist\"\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_CHANNEL=1\n",
        "HIDDEN_LAYER_SIZE= 32\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n",
        "PRETRAINED_WT_PATH = \"\"\n",
        "\n",
        "rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n",
        "\n",
        "print(\"Train Data Shape: \",rcae.data._X_train.shape)\n",
        "print(\"Train Label Shape: \",rcae.data._y_train.shape)\n",
        "print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n",
        "print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n",
        "\n",
        "\n",
        "print(\"Test Data Shape: \",rcae.data._X_test.shape)\n",
        "print(\"Test Label Shape: \",rcae.data._y_test.shape)\n",
        "print(\"===========TRAINING AND PREDICTING WITH RCAE============================\")\n",
        "rcae.fit_and_predict()\n",
        "print(\"========================================================================\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n",
            "This call to matplotlib.use() has no effect because the backend has already\n",
            "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
            "or matplotlib.backends is imported for the first time.\n",
            "\n",
            "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
            "    ioloop.IOLoop.instance().start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
            "    super(ZMQIOLoop, self).start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n",
            "    handler_func(fd_obj, events)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
            "    self._handle_recv()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
            "    self._run_callback(callback, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
            "    callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
            "    return self.dispatch_shell(stream, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
            "    handler(stream, idents, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
            "    user_expressions, allow_stdin)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n",
            "    interactivity=interactivity, compiler=compiler, result=result)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n",
            "    if self.run_code(code, result):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-e96966cecab3>\", line 1, in <module>\n",
            "    get_ipython().magic('matplotlib inline')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
            "    return self.run_line_magic(magic_name, magic_arg_s)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
            "    result = fn(*args,**kwargs)\n",
            "  File \"<decorator-gen-105>\", line 2, in matplotlib\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
            "    call = lambda f, *a, **k: f(*a, **k)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n",
            "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
            "    pt.activate_matplotlib(backend)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n",
            "    matplotlib.pyplot.switch_backend(backend)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
            "    matplotlib.use(newbackend, warn=False, force=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
            "    reload(sys.modules['matplotlib.backends'])\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
            "    line for line in traceback.format_stack()\n",
            "\n",
            "\n",
            "  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n",
            "This call to matplotlib.use() has no effect because the backend has already\n",
            "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
            "or matplotlib.backends is imported for the first time.\n",
            "\n",
            "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
            "    ioloop.IOLoop.instance().start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
            "    super(ZMQIOLoop, self).start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n",
            "    handler_func(fd_obj, events)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
            "    self._handle_recv()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
            "    self._run_callback(callback, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
            "    callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
            "    return self.dispatch_shell(stream, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
            "    handler(stream, idents, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
            "    user_expressions, allow_stdin)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n",
            "    interactivity=interactivity, compiler=compiler, result=result)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n",
            "    if self.run_code(code, result):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-e96966cecab3>\", line 1, in <module>\n",
            "    get_ipython().magic('matplotlib inline')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
            "    return self.run_line_magic(magic_name, magic_arg_s)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
            "    result = fn(*args,**kwargs)\n",
            "  File \"<decorator-gen-105>\", line 2, in matplotlib\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
            "    call = lambda f, *a, **k: f(*a, **k)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n",
            "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
            "    pt.activate_matplotlib(backend)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n",
            "    matplotlib.pyplot.switch_backend(backend)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
            "    matplotlib.use(newbackend, warn=False, force=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
            "    reload(sys.modules['matplotlib.backends'])\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
            "    line for line in traceback.format_stack()\n",
            "\n",
            "\n",
            "  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n",
            "Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE/\n",
            "[INFO: ] Loading data...\n",
            "[INFO:] THe shape of X is  (60000, 28, 28, 1)\n",
            "[INFO:] THe shape of y is  (60000,)\n",
            "[INFO] : The idx_normal is:  [ True False False ...  True False False]\n",
            "[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n",
            "[INFO] : The shape of X is:  (60000, 28, 28, 1)\n",
            "[INFO] : The shape of y is:  (60000,)\n",
            "[INFO:] THe shape of X is  (10000, 1, 28, 28)\n",
            "[INFO:] THe shape of y is  (10000,)\n",
            "[INFO] : The idx_normal is:  [False False False ... False  True False]\n",
            "[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n",
            "[INFO] : The shape of X is:  (10000, 1, 28, 28)\n",
            "[INFO] : The shape of y is:  (10000,)\n",
            "[INFO: ] Data loaded.\n",
            "[INFO:] Assertions of memory muted\n",
            "Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE/\n",
            "[INFO: ] Loading data...\n",
            "[INFO:] THe shape of X is  (60000, 28, 28, 1)\n",
            "[INFO:] THe shape of y is  (60000,)\n",
            "[INFO] : The idx_normal is:  [ True False False ...  True False False]\n",
            "[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n",
            "[INFO] : The shape of X is:  (60000, 28, 28, 1)\n",
            "[INFO] : The shape of y is:  (60000,)\n",
            "[INFO:] THe shape of X is  (10000, 1, 28, 28)\n",
            "[INFO:] THe shape of y is  (10000,)\n",
            "[INFO] : The idx_normal is:  [False False False ... False  True False]\n",
            "[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n",
            "[INFO] : The shape of X is:  (10000, 1, 28, 28)\n",
            "[INFO] : The shape of y is:  (10000,)\n",
            "[INFO: ] Data loaded.\n",
            "Train Data Shape:  (4564, 28, 28, 1)\n",
            "Train Label Shape:  (4564,)\n",
            "Validation Data Shape:  (912, 28, 28, 1)\n",
            "Validation Label Shape:  (912,)\n",
            "Test Data Shape:  (10000, 1, 28, 28)\n",
            "Test Label Shape:  (10000,)\n",
            "===========TRAINING AND PREDICTING WITH RCAE============================\n",
            "[INFO:]  Length of Positive data 4518\n",
            "[INFO:]  Length of Negative data 46\n",
            "[INFO:] X_test.shape (4564, 28, 28, 1)\n",
            "[INFO:] y_test.shape [ 1.  1.  1. ... -1. -1. -1.]\n",
            "[INFO:] y_train.shape (4564,)\n",
            "[INFO:] y_train.shape [ 1.  1.  1. ... -1. -1. -1.]\n",
            "[INFO] compiling model...\n",
            "Train on 4518 samples, validate on 46 samples\n",
            "Epoch 1/350\n",
            "4518/4518 [==============================] - 4s 908us/step - loss: 5.0795 - val_loss: 5.1374\n",
            "Epoch 2/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 5.0088 - val_loss: 5.0415\n",
            "Epoch 3/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9918 - val_loss: 5.0192\n",
            "Epoch 4/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9832 - val_loss: 5.0096\n",
            "Epoch 5/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9762 - val_loss: 5.0124\n",
            "Epoch 6/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9726 - val_loss: 5.0094\n",
            "Epoch 7/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9700 - val_loss: 5.0130\n",
            "Epoch 8/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 4.9685 - val_loss: 5.0023\n",
            "Epoch 9/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 4.9666 - val_loss: 5.0043\n",
            "Epoch 10/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9664 - val_loss: 5.0014\n",
            "Epoch 11/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9679 - val_loss: 5.0094\n",
            "Epoch 12/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9646 - val_loss: 5.0006\n",
            "Epoch 13/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9632 - val_loss: 4.9946\n",
            "Epoch 14/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9625 - val_loss: 4.9901\n",
            "Epoch 15/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9619 - val_loss: 4.9920\n",
            "Epoch 16/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9611 - val_loss: 4.9909\n",
            "Epoch 17/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9609 - val_loss: 4.9916\n",
            "Epoch 18/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9608 - val_loss: 4.9891\n",
            "Epoch 19/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9601 - val_loss: 4.9852\n",
            "Epoch 20/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9596 - val_loss: 4.9850\n",
            "Epoch 21/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9592 - val_loss: 4.9843\n",
            "Epoch 22/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9595 - val_loss: 4.9901\n",
            "Epoch 23/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 4.9601 - val_loss: 4.9883\n",
            "Epoch 24/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9587 - val_loss: 4.9823\n",
            "Epoch 25/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9584 - val_loss: 4.9820\n",
            "Epoch 26/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9580 - val_loss: 4.9805\n",
            "Epoch 27/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9581 - val_loss: 4.9920\n",
            "Epoch 28/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9578 - val_loss: 4.9833\n",
            "Epoch 29/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9573 - val_loss: 4.9832\n",
            "Epoch 30/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 4.9595 - val_loss: 4.9821\n",
            "Epoch 31/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 4.9583 - val_loss: 4.9785\n",
            "Epoch 32/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9580 - val_loss: 4.9774\n",
            "Epoch 33/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9575 - val_loss: 4.9792\n",
            "Epoch 34/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9571 - val_loss: 4.9766\n",
            "Epoch 35/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9572 - val_loss: 4.9780\n",
            "Epoch 36/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9568 - val_loss: 4.9775\n",
            "Epoch 37/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9564 - val_loss: 4.9762\n",
            "Epoch 38/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9564 - val_loss: 4.9765\n",
            "Epoch 39/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9562 - val_loss: 4.9762\n",
            "Epoch 40/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9563 - val_loss: 4.9781\n",
            "Epoch 41/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9562 - val_loss: 4.9750\n",
            "Epoch 42/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9561 - val_loss: 4.9755\n",
            "Epoch 43/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9558 - val_loss: 4.9755\n",
            "Epoch 44/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9558 - val_loss: 4.9749\n",
            "Epoch 45/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9556 - val_loss: 4.9744\n",
            "Epoch 46/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9555 - val_loss: 4.9745\n",
            "Epoch 47/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9558 - val_loss: 4.9750\n",
            "Epoch 48/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9557 - val_loss: 4.9747\n",
            "Epoch 49/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9558 - val_loss: 4.9751\n",
            "Epoch 50/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9558 - val_loss: 4.9787\n",
            "Epoch 51/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9560 - val_loss: 4.9780\n",
            "Epoch 52/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9556 - val_loss: 4.9744\n",
            "Epoch 53/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9554 - val_loss: 4.9745\n",
            "Epoch 54/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9553 - val_loss: 4.9752\n",
            "Epoch 55/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9570 - val_loss: 4.9750\n",
            "Epoch 56/350\n",
            "4518/4518 [==============================] - 1s 317us/step - loss: 4.9559 - val_loss: 4.9736\n",
            "Epoch 57/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9554 - val_loss: 4.9733\n",
            "Epoch 58/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9552 - val_loss: 4.9728\n",
            "Epoch 59/350\n",
            "4518/4518 [==============================] - 1s 317us/step - loss: 4.9551 - val_loss: 4.9728\n",
            "Epoch 60/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9551 - val_loss: 4.9722\n",
            "Epoch 61/350\n",
            "4518/4518 [==============================] - 1s 317us/step - loss: 4.9550 - val_loss: 4.9725\n",
            "Epoch 62/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9550 - val_loss: 4.9740\n",
            "Epoch 63/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9550 - val_loss: 4.9728\n",
            "Epoch 64/350\n",
            "4518/4518 [==============================] - 1s 317us/step - loss: 4.9548 - val_loss: 4.9726\n",
            "Epoch 65/350\n",
            "4518/4518 [==============================] - 1s 317us/step - loss: 4.9546 - val_loss: 4.9723\n",
            "Epoch 66/350\n",
            "4518/4518 [==============================] - 1s 318us/step - loss: 4.9545 - val_loss: 4.9716\n",
            "Epoch 67/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9545 - val_loss: 4.9716\n",
            "Epoch 68/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9546 - val_loss: 4.9720\n",
            "Epoch 69/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9546 - val_loss: 4.9721\n",
            "Epoch 70/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 4.9546 - val_loss: 4.9714\n",
            "Epoch 71/350\n",
            "4518/4518 [==============================] - 1s 317us/step - loss: 4.9545 - val_loss: 4.9719\n",
            "Epoch 72/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9544 - val_loss: 4.9718\n",
            "Epoch 73/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9544 - val_loss: 4.9712\n",
            "Epoch 74/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9544 - val_loss: 4.9710\n",
            "Epoch 75/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9544 - val_loss: 4.9713\n",
            "Epoch 76/350\n",
            "4518/4518 [==============================] - 1s 317us/step - loss: 4.9543 - val_loss: 4.9715\n",
            "Epoch 77/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9542 - val_loss: 4.9712\n",
            "Epoch 78/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9556 - val_loss: 4.9911\n",
            "Epoch 79/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9558 - val_loss: 4.9792\n",
            "Epoch 80/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9550 - val_loss: 4.9735\n",
            "Epoch 81/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9547 - val_loss: 4.9718\n",
            "Epoch 82/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9546 - val_loss: 4.9712\n",
            "Epoch 83/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9545 - val_loss: 4.9713\n",
            "Epoch 84/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9544 - val_loss: 4.9712\n",
            "Epoch 85/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9543 - val_loss: 4.9713\n",
            "Epoch 86/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9542 - val_loss: 4.9706\n",
            "Epoch 87/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9545 - val_loss: 4.9708\n",
            "Epoch 88/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9542 - val_loss: 4.9706\n",
            "Epoch 89/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9541 - val_loss: 4.9705\n",
            "Epoch 90/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9543 - val_loss: 4.9706\n",
            "Epoch 91/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9542 - val_loss: 4.9708\n",
            "Epoch 92/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9541 - val_loss: 4.9703\n",
            "Epoch 93/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9540 - val_loss: 4.9706\n",
            "Epoch 94/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9541 - val_loss: 4.9705\n",
            "Epoch 95/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9541 - val_loss: 4.9699\n",
            "Epoch 96/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9539 - val_loss: 4.9703\n",
            "Epoch 97/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9540 - val_loss: 4.9700\n",
            "Epoch 98/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9540 - val_loss: 4.9706\n",
            "Epoch 99/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9539 - val_loss: 4.9706\n",
            "Epoch 100/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9538 - val_loss: 4.9705\n",
            "Epoch 101/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9538 - val_loss: 4.9707\n",
            "Epoch 102/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9538 - val_loss: 4.9702\n",
            "Epoch 103/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9539 - val_loss: 4.9706\n",
            "Epoch 104/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9539 - val_loss: 4.9708\n",
            "Epoch 105/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9538 - val_loss: 4.9705\n",
            "Epoch 106/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9537 - val_loss: 4.9705\n",
            "Epoch 107/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9537 - val_loss: 4.9707\n",
            "Epoch 108/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9536 - val_loss: 4.9710\n",
            "Epoch 109/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9537 - val_loss: 4.9704\n",
            "Epoch 110/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9538 - val_loss: 4.9705\n",
            "Epoch 111/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9536 - val_loss: 4.9702\n",
            "Epoch 112/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9536 - val_loss: 4.9706\n",
            "Epoch 113/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9537 - val_loss: 4.9706\n",
            "Epoch 114/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9537 - val_loss: 4.9707\n",
            "Epoch 115/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9536 - val_loss: 4.9707\n",
            "Epoch 116/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9537 - val_loss: 4.9706\n",
            "Epoch 117/350\n",
            "4518/4518 [==============================] - 1s 318us/step - loss: 4.9537 - val_loss: 4.9705\n",
            "Epoch 118/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 4.9536 - val_loss: 4.9706\n",
            "Epoch 119/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9535 - val_loss: 4.9719\n",
            "Epoch 120/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9536 - val_loss: 4.9709\n",
            "Epoch 121/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9535 - val_loss: 4.9710\n",
            "Epoch 122/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 4.9537 - val_loss: 4.9714\n",
            "Epoch 123/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9535 - val_loss: 4.9709\n",
            "Epoch 124/350\n",
            "4518/4518 [==============================] - 1s 317us/step - loss: 4.9535 - val_loss: 4.9713\n",
            "Epoch 125/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9535 - val_loss: 4.9704\n",
            "Epoch 126/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9534 - val_loss: 4.9705\n",
            "Epoch 127/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9534 - val_loss: 4.9704\n",
            "Epoch 128/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9535 - val_loss: 4.9709\n",
            "Epoch 129/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9535 - val_loss: 4.9702\n",
            "Epoch 130/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9534 - val_loss: 4.9706\n",
            "Epoch 131/350\n",
            "4518/4518 [==============================] - 1s 317us/step - loss: 4.9534 - val_loss: 4.9706\n",
            "Epoch 132/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9533 - val_loss: 4.9704\n",
            "Epoch 133/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9533 - val_loss: 4.9717\n",
            "Epoch 134/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9533 - val_loss: 4.9702\n",
            "Epoch 135/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9532 - val_loss: 4.9705\n",
            "Epoch 136/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9531 - val_loss: 4.9703\n",
            "Epoch 137/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9532 - val_loss: 4.9707\n",
            "Epoch 138/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9532 - val_loss: 4.9705\n",
            "Epoch 139/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9533 - val_loss: 4.9708\n",
            "Epoch 140/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9533 - val_loss: 4.9708\n",
            "Epoch 141/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9532 - val_loss: 4.9704\n",
            "Epoch 142/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9533 - val_loss: 4.9704\n",
            "Epoch 143/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9534 - val_loss: 4.9711\n",
            "Epoch 144/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9535 - val_loss: 4.9704\n",
            "Epoch 145/350\n",
            "4518/4518 [==============================] - 1s 318us/step - loss: 4.9534 - val_loss: 4.9706\n",
            "Epoch 146/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 4.9532 - val_loss: 4.9705\n",
            "Epoch 147/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9531 - val_loss: 4.9705\n",
            "Epoch 148/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9532 - val_loss: 4.9704\n",
            "Epoch 149/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9532 - val_loss: 4.9707\n",
            "Epoch 150/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9532 - val_loss: 4.9702\n",
            "Epoch 151/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9532 - val_loss: 4.9709\n",
            "Epoch 152/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9531 - val_loss: 4.9705\n",
            "Epoch 153/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9531 - val_loss: 4.9713\n",
            "Epoch 154/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9531 - val_loss: 4.9715\n",
            "Epoch 155/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9531 - val_loss: 4.9705\n",
            "Epoch 156/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9531 - val_loss: 4.9704\n",
            "Epoch 157/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9530 - val_loss: 4.9705\n",
            "Epoch 158/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9531 - val_loss: 4.9703\n",
            "Epoch 159/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9530 - val_loss: 4.9700\n",
            "Epoch 160/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9530 - val_loss: 4.9706\n",
            "Epoch 161/350\n",
            "4518/4518 [==============================] - 1s 318us/step - loss: 4.9532 - val_loss: 4.9709\n",
            "Epoch 162/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9532 - val_loss: 4.9702\n",
            "Epoch 163/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9532 - val_loss: 4.9704\n",
            "Epoch 164/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9532 - val_loss: 4.9701\n",
            "Epoch 165/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9530 - val_loss: 4.9709\n",
            "Epoch 166/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9531 - val_loss: 4.9708\n",
            "Epoch 167/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9531 - val_loss: 4.9707\n",
            "Epoch 168/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 4.9531 - val_loss: 4.9706\n",
            "Epoch 169/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9529 - val_loss: 4.9711\n",
            "Epoch 170/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9531 - val_loss: 4.9708\n",
            "Epoch 171/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9530 - val_loss: 4.9710\n",
            "Epoch 172/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9530 - val_loss: 4.9716\n",
            "Epoch 173/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9531 - val_loss: 4.9708\n",
            "Epoch 174/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9530 - val_loss: 4.9712\n",
            "Epoch 175/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9529 - val_loss: 4.9710\n",
            "Epoch 176/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9530 - val_loss: 4.9704\n",
            "Epoch 177/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9531 - val_loss: 4.9711\n",
            "Epoch 178/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9531 - val_loss: 4.9708\n",
            "Epoch 179/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9532 - val_loss: 4.9711\n",
            "Epoch 180/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9530 - val_loss: 4.9708\n",
            "Epoch 181/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9529 - val_loss: 4.9706\n",
            "Epoch 182/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 4.9529 - val_loss: 4.9702\n",
            "Epoch 183/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9529 - val_loss: 4.9705\n",
            "Epoch 184/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9528 - val_loss: 4.9700\n",
            "Epoch 185/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9531 - val_loss: 4.9709\n",
            "Epoch 186/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9529 - val_loss: 4.9703\n",
            "Epoch 187/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9528 - val_loss: 4.9706\n",
            "Epoch 188/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9529 - val_loss: 4.9708\n",
            "Epoch 189/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9528 - val_loss: 4.9707\n",
            "Epoch 190/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9530 - val_loss: 4.9705\n",
            "Epoch 191/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9529 - val_loss: 4.9709\n",
            "Epoch 192/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9529 - val_loss: 4.9704\n",
            "Epoch 193/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9529 - val_loss: 4.9713\n",
            "Epoch 194/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9527 - val_loss: 4.9713\n",
            "Epoch 195/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 4.9528 - val_loss: 4.9706\n",
            "Epoch 196/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 4.9528 - val_loss: 4.9721\n",
            "Epoch 197/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9529 - val_loss: 4.9709\n",
            "Epoch 198/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 4.9527 - val_loss: 4.9706\n",
            "Epoch 199/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9528 - val_loss: 4.9708\n",
            "Epoch 200/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9529 - val_loss: 4.9714\n",
            "Epoch 201/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9530 - val_loss: 4.9711\n",
            "Epoch 202/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 4.9528 - val_loss: 4.9712\n",
            "Epoch 203/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9528 - val_loss: 4.9708\n",
            "Epoch 204/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9528 - val_loss: 4.9706\n",
            "Epoch 205/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9528 - val_loss: 4.9706\n",
            "Epoch 206/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9529 - val_loss: 4.9712\n",
            "Epoch 207/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9527 - val_loss: 4.9715\n",
            "Epoch 208/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9529 - val_loss: 4.9704\n",
            "Epoch 209/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9529 - val_loss: 4.9705\n",
            "Epoch 210/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9528 - val_loss: 4.9703\n",
            "Epoch 211/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9529 - val_loss: 4.9727\n",
            "Epoch 212/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9530 - val_loss: 4.9713\n",
            "Epoch 213/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9529 - val_loss: 4.9710\n",
            "Epoch 214/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9529 - val_loss: 4.9711\n",
            "Epoch 215/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9528 - val_loss: 4.9714\n",
            "Epoch 216/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9526 - val_loss: 4.9707\n",
            "Epoch 217/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9530 - val_loss: 4.9709\n",
            "Epoch 218/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9531 - val_loss: 4.9710\n",
            "Epoch 219/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9533 - val_loss: 4.9713\n",
            "Epoch 220/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9532 - val_loss: 4.9713\n",
            "Epoch 221/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9530 - val_loss: 4.9702\n",
            "Epoch 222/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 4.9530 - val_loss: 4.9706\n",
            "Epoch 223/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9529 - val_loss: 4.9703\n",
            "Epoch 224/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9527 - val_loss: 4.9709\n",
            "Epoch 225/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9528 - val_loss: 4.9703\n",
            "Epoch 226/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9527 - val_loss: 4.9702\n",
            "Epoch 227/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9527 - val_loss: 4.9699\n",
            "Epoch 228/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9527 - val_loss: 4.9702\n",
            "Epoch 229/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9527 - val_loss: 4.9706\n",
            "Epoch 230/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9527 - val_loss: 4.9707\n",
            "Epoch 231/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 4.9527 - val_loss: 4.9700\n",
            "Epoch 232/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9527 - val_loss: 4.9708\n",
            "Epoch 233/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9527 - val_loss: 4.9707\n",
            "Epoch 234/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9526 - val_loss: 4.9707\n",
            "Epoch 235/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9526 - val_loss: 4.9705\n",
            "Epoch 236/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9526 - val_loss: 4.9709\n",
            "Epoch 237/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9526 - val_loss: 4.9711\n",
            "Epoch 238/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9526 - val_loss: 4.9705\n",
            "Epoch 239/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9525 - val_loss: 4.9709\n",
            "Epoch 240/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9526 - val_loss: 4.9709\n",
            "Epoch 241/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9527 - val_loss: 4.9701\n",
            "Epoch 242/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9527 - val_loss: 4.9708\n",
            "Epoch 243/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 4.9527 - val_loss: 4.9713\n",
            "Epoch 244/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 4.9526 - val_loss: 4.9704\n",
            "Epoch 245/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9526 - val_loss: 4.9710\n",
            "Epoch 246/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9525 - val_loss: 4.9707\n",
            "Epoch 247/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9526 - val_loss: 4.9715\n",
            "Epoch 248/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9526 - val_loss: 4.9708\n",
            "Epoch 249/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9526 - val_loss: 4.9716\n",
            "Epoch 250/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9525 - val_loss: 4.9713\n",
            "Epoch 251/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9525 - val_loss: 4.9710\n",
            "Epoch 252/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9526 - val_loss: 4.9707\n",
            "Epoch 253/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9527 - val_loss: 4.9705\n",
            "Epoch 254/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9526 - val_loss: 4.9710\n",
            "Epoch 255/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9526 - val_loss: 4.9711\n",
            "Epoch 256/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9526 - val_loss: 4.9710\n",
            "Epoch 257/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9525 - val_loss: 4.9712\n",
            "Epoch 258/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9527 - val_loss: 4.9708\n",
            "Epoch 259/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9527 - val_loss: 4.9717\n",
            "Epoch 260/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9527 - val_loss: 4.9712\n",
            "Epoch 261/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9525 - val_loss: 4.9712\n",
            "Epoch 262/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9526 - val_loss: 4.9708\n",
            "Epoch 263/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9524 - val_loss: 4.9710\n",
            "Epoch 264/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9526 - val_loss: 4.9706\n",
            "Epoch 265/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9525 - val_loss: 4.9704\n",
            "Epoch 266/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9524 - val_loss: 4.9707\n",
            "Epoch 267/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9525 - val_loss: 4.9708\n",
            "Epoch 268/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9526 - val_loss: 4.9703\n",
            "Epoch 269/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9525 - val_loss: 4.9707\n",
            "Epoch 270/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 4.9525 - val_loss: 4.9708\n",
            "Epoch 271/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9525 - val_loss: 4.9706\n",
            "Epoch 272/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9525 - val_loss: 4.9710\n",
            "Epoch 273/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9525 - val_loss: 4.9706\n",
            "Epoch 274/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 4.9525 - val_loss: 4.9713\n",
            "Epoch 275/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9524 - val_loss: 4.9712\n",
            "Epoch 276/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9524 - val_loss: 4.9707\n",
            "Epoch 277/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 4.9524 - val_loss: 4.9709\n",
            "Epoch 278/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9525 - val_loss: 4.9710\n",
            "Epoch 279/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 4.9524 - val_loss: 4.9716\n",
            "Epoch 280/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9525 - val_loss: 4.9712\n",
            "Epoch 281/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9525 - val_loss: 4.9710\n",
            "Epoch 282/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9524 - val_loss: 4.9719\n",
            "Epoch 283/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9524 - val_loss: 4.9710\n",
            "Epoch 284/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9525 - val_loss: 4.9706\n",
            "Epoch 285/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 4.9524 - val_loss: 4.9707\n",
            "Epoch 286/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9524 - val_loss: 4.9710\n",
            "Epoch 287/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9523 - val_loss: 4.9715\n",
            "Epoch 288/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9524 - val_loss: 4.9709\n",
            "Epoch 289/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9525 - val_loss: 4.9710\n",
            "Epoch 290/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9524 - val_loss: 4.9710\n",
            "Epoch 291/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9524 - val_loss: 4.9710\n",
            "Epoch 292/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9524 - val_loss: 4.9714\n",
            "Epoch 293/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9524 - val_loss: 4.9710\n",
            "Epoch 294/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9524 - val_loss: 4.9714\n",
            "Epoch 295/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 4.9524 - val_loss: 4.9717\n",
            "Epoch 296/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9524 - val_loss: 4.9706\n",
            "Epoch 297/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9535 - val_loss: 5.0666\n",
            "Epoch 298/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9541 - val_loss: 4.9841\n",
            "Epoch 299/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9533 - val_loss: 4.9737\n",
            "Epoch 300/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9529 - val_loss: 4.9725\n",
            "Epoch 301/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9528 - val_loss: 4.9722\n",
            "Epoch 302/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 4.9527 - val_loss: 4.9722\n",
            "Epoch 303/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9526 - val_loss: 4.9730\n",
            "Epoch 304/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9528 - val_loss: 4.9730\n",
            "Epoch 305/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9526 - val_loss: 4.9726\n",
            "Epoch 306/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9526 - val_loss: 4.9723\n",
            "Epoch 307/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9526 - val_loss: 4.9718\n",
            "Epoch 308/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9525 - val_loss: 4.9724\n",
            "Epoch 309/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9525 - val_loss: 4.9721\n",
            "Epoch 310/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9524 - val_loss: 4.9721\n",
            "Epoch 311/350\n",
            "4518/4518 [==============================] - 1s 317us/step - loss: 4.9524 - val_loss: 4.9719\n",
            "Epoch 312/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9524 - val_loss: 4.9719\n",
            "Epoch 313/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9526 - val_loss: 4.9726\n",
            "Epoch 314/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9525 - val_loss: 4.9722\n",
            "Epoch 315/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9525 - val_loss: 4.9723\n",
            "Epoch 316/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9524 - val_loss: 4.9723\n",
            "Epoch 317/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9529 - val_loss: 4.9741\n",
            "Epoch 318/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9529 - val_loss: 4.9728\n",
            "Epoch 319/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9526 - val_loss: 4.9729\n",
            "Epoch 320/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 4.9526 - val_loss: 4.9719\n",
            "Epoch 321/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9524 - val_loss: 4.9717\n",
            "Epoch 322/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9524 - val_loss: 4.9727\n",
            "Epoch 323/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 4.9526 - val_loss: 4.9726\n",
            "Epoch 324/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9524 - val_loss: 4.9723\n",
            "Epoch 325/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9525 - val_loss: 4.9722\n",
            "Epoch 326/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9524 - val_loss: 4.9724\n",
            "Epoch 327/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9524 - val_loss: 4.9728\n",
            "Epoch 328/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9524 - val_loss: 4.9726\n",
            "Epoch 329/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9524 - val_loss: 4.9728\n",
            "Epoch 330/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9523 - val_loss: 4.9727\n",
            "Epoch 331/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 4.9523 - val_loss: 4.9728\n",
            "Epoch 332/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9525 - val_loss: 4.9731\n",
            "Epoch 333/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9523 - val_loss: 4.9729\n",
            "Epoch 334/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 4.9524 - val_loss: 4.9710\n",
            "Epoch 335/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9525 - val_loss: 4.9718\n",
            "Epoch 336/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9523 - val_loss: 4.9713\n",
            "Epoch 337/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 4.9523 - val_loss: 4.9715\n",
            "Epoch 338/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9523 - val_loss: 4.9723\n",
            "Epoch 339/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9523 - val_loss: 4.9713\n",
            "Epoch 340/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9523 - val_loss: 4.9719\n",
            "Epoch 341/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 4.9523 - val_loss: 4.9718\n",
            "Epoch 342/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9522 - val_loss: 4.9717\n",
            "Epoch 343/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 4.9522 - val_loss: 4.9712\n",
            "Epoch 344/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9522 - val_loss: 4.9714\n",
            "Epoch 345/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 4.9523 - val_loss: 4.9716\n",
            "Epoch 346/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9523 - val_loss: 4.9718\n",
            "Epoch 347/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9522 - val_loss: 4.9716\n",
            "Epoch 348/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 4.9522 - val_loss: 4.9710\n",
            "Epoch 349/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 4.9522 - val_loss: 4.9714\n",
            "Epoch 350/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 4.9523 - val_loss: 4.9706\n",
            "(lamda,Threshold) 0.0 0.0\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (4564, 784) 784\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 3578172 0.0\n",
            "The shape of N (4564, 784)\n",
            "The minimum value of N  -0.99815136\n",
            "The max value of N 0.999942\n",
            "[INFO:] Xclean  MSE Computed shape (4564, 784)\n",
            "[INFO:]Xdecoded  Computed shape (4564, 784)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0022015534])\n",
            "[INFO:] The anomaly threshold computed is  0.0022015534\n",
            "side: 28\n",
            "channel: 1\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [4539, 4547, 4561, 4524, 4522, 4550, 4563, 4526, 4559, 4518, 4536, 4532, 4542, 4521, 4562, 4533, 4523, 4520, 4531, 4529, 4538, 4551, 4545, 4525, 4535, 4548, 4527, 4530, 4537, 4519, 4528, 4549, 4554, 4541, 4553, 4557, 4552, 4558, 4543, 4540, 4556, 1906, 3527, 1761, 4544, 676]\n",
            "[INFO:] The worstreconstructed_Top200index index are  [4539, 4547, 4561, 4524, 4522, 4550, 4563, 4526, 4559, 4518, 4536, 4532, 4542, 4521, 4562, 4533, 4523, 4520, 4531, 4529, 4538, 4551, 4545, 4525, 4535, 4548, 4527, 4530, 4537, 4519, 4528, 4549, 4554, 4541, 4553, 4557, 4552, 4558, 4543, 4540, 4556, 1906, 3527, 1761, 4544, 676, 1131, 277, 4534, 4315, 2677, 449, 3229, 1193, 2391, 602, 3723, 263, 4555, 691, 3972, 4546, 2853, 4560, 274, 803, 4163, 1493, 591, 3054, 2764, 826, 4443, 1050, 3828, 1503, 89, 2790, 737, 4451, 1760, 3844, 2086, 4306, 3068, 2887, 4483, 569, 3855, 4345, 470, 2593, 1543, 3381, 3419, 875, 203, 4368, 3034, 4423, 4213, 1452, 7, 4229, 2307, 4067, 1144, 1048, 433, 478, 3672, 221, 377, 4339, 2832, 1227, 3542, 624, 521, 3412, 64, 3996, 502, 1978, 2879, 2776, 2179, 3113, 381, 1373, 1371, 3077, 1368, 1953, 1519, 3873, 1395, 4416, 318, 3247, 2951, 422, 4153, 1309, 4015, 2813, 4407, 657, 3874, 1035, 816, 2359, 2699, 2390, 4186, 1129, 185, 1477, 4350, 72, 3935, 270, 4049, 1997, 3674, 3191, 2037, 1891, 200, 3275, 4009, 105, 2338, 4394, 1091, 2016, 2237, 1387, 1307, 2484, 247, 3862, 4076, 2480, 2556, 1064, 3613, 4476, 2473, 71, 3321, 2684, 3555, 3356, 4019, 1329, 1041, 832, 4239, 2892]\n",
            "=====================\n",
            "AUROC 0.0 0.9565217391304348\n",
            "=======================\n",
            "Saved model to disk....\n",
            "[INFO] compiling model...\n",
            "Train on 4518 samples, validate on 46 samples\n",
            "Epoch 1/350\n",
            "4518/4518 [==============================] - 4s 791us/step - loss: 7.9034 - val_loss: 7.9232\n",
            "Epoch 2/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9032 - val_loss: 7.9220\n",
            "Epoch 3/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9033 - val_loss: 7.9219\n",
            "Epoch 4/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 7.9033 - val_loss: 7.9227\n",
            "Epoch 5/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9033 - val_loss: 7.9234\n",
            "Epoch 6/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9034 - val_loss: 7.9225\n",
            "Epoch 7/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9033 - val_loss: 7.9233\n",
            "Epoch 8/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9034 - val_loss: 7.9225\n",
            "Epoch 9/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9033 - val_loss: 7.9221\n",
            "Epoch 10/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9033 - val_loss: 7.9221\n",
            "Epoch 11/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9033 - val_loss: 7.9223\n",
            "Epoch 12/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9032 - val_loss: 7.9219\n",
            "Epoch 13/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9032 - val_loss: 7.9222\n",
            "Epoch 14/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9032 - val_loss: 7.9229\n",
            "Epoch 15/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9034 - val_loss: 7.9219\n",
            "Epoch 16/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9031 - val_loss: 7.9224\n",
            "Epoch 17/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9032 - val_loss: 7.9221\n",
            "Epoch 18/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9033 - val_loss: 7.9224\n",
            "Epoch 19/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9031 - val_loss: 7.9227\n",
            "Epoch 20/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9032 - val_loss: 7.9228\n",
            "Epoch 21/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9032 - val_loss: 7.9227\n",
            "Epoch 22/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9033 - val_loss: 7.9224\n",
            "Epoch 23/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9033 - val_loss: 7.9230\n",
            "Epoch 24/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9032 - val_loss: 7.9227\n",
            "Epoch 25/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9031 - val_loss: 7.9220\n",
            "Epoch 26/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9033 - val_loss: 7.9227\n",
            "Epoch 27/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 7.9032 - val_loss: 7.9234\n",
            "Epoch 28/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9032 - val_loss: 7.9225\n",
            "Epoch 29/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9031 - val_loss: 7.9225\n",
            "Epoch 30/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9033 - val_loss: 7.9222\n",
            "Epoch 31/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9032 - val_loss: 7.9221\n",
            "Epoch 32/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9032 - val_loss: 7.9226\n",
            "Epoch 33/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9032 - val_loss: 7.9221\n",
            "Epoch 34/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9031 - val_loss: 7.9223\n",
            "Epoch 35/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 7.9031 - val_loss: 7.9227\n",
            "Epoch 36/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9030 - val_loss: 7.9227\n",
            "Epoch 37/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9032 - val_loss: 7.9227\n",
            "Epoch 38/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9031 - val_loss: 7.9236\n",
            "Epoch 39/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9031 - val_loss: 7.9226\n",
            "Epoch 40/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9031 - val_loss: 7.9233\n",
            "Epoch 41/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9031 - val_loss: 7.9231\n",
            "Epoch 42/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9031 - val_loss: 7.9237\n",
            "Epoch 43/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9032 - val_loss: 7.9220\n",
            "Epoch 44/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9031 - val_loss: 7.9233\n",
            "Epoch 45/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9032 - val_loss: 7.9232\n",
            "Epoch 46/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9033 - val_loss: 7.9222\n",
            "Epoch 47/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9032 - val_loss: 7.9222\n",
            "Epoch 48/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9031 - val_loss: 7.9218\n",
            "Epoch 49/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9031 - val_loss: 7.9226\n",
            "Epoch 50/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9032 - val_loss: 7.9220\n",
            "Epoch 51/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9031 - val_loss: 7.9219\n",
            "Epoch 52/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9031 - val_loss: 7.9223\n",
            "Epoch 53/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9030 - val_loss: 7.9222\n",
            "Epoch 54/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9031 - val_loss: 7.9226\n",
            "Epoch 55/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9031 - val_loss: 7.9236\n",
            "Epoch 56/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9032 - val_loss: 7.9230\n",
            "Epoch 57/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9031 - val_loss: 7.9225\n",
            "Epoch 58/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9032 - val_loss: 7.9226\n",
            "Epoch 59/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9030 - val_loss: 7.9219\n",
            "Epoch 60/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9031 - val_loss: 7.9221\n",
            "Epoch 61/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 7.9030 - val_loss: 7.9223\n",
            "Epoch 62/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 7.9030 - val_loss: 7.9219\n",
            "Epoch 63/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 7.9031 - val_loss: 7.9225\n",
            "Epoch 64/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 7.9031 - val_loss: 7.9224\n",
            "Epoch 65/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9030 - val_loss: 7.9219\n",
            "Epoch 66/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9031 - val_loss: 7.9299\n",
            "Epoch 67/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9032 - val_loss: 7.9236\n",
            "Epoch 68/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9031 - val_loss: 7.9231\n",
            "Epoch 69/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 7.9030 - val_loss: 7.9233\n",
            "Epoch 70/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9030 - val_loss: 7.9237\n",
            "Epoch 71/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9032 - val_loss: 7.9228\n",
            "Epoch 72/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9031 - val_loss: 7.9232\n",
            "Epoch 73/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9031 - val_loss: 7.9230\n",
            "Epoch 74/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9031 - val_loss: 7.9223\n",
            "Epoch 75/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9031 - val_loss: 7.9226\n",
            "Epoch 76/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9030 - val_loss: 7.9225\n",
            "Epoch 77/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9031 - val_loss: 7.9231\n",
            "Epoch 78/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9031 - val_loss: 7.9224\n",
            "Epoch 79/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9042 - val_loss: 7.9244\n",
            "Epoch 80/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9040 - val_loss: 7.9229\n",
            "Epoch 81/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9036 - val_loss: 7.9225\n",
            "Epoch 82/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9034 - val_loss: 7.9223\n",
            "Epoch 83/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9033 - val_loss: 7.9222\n",
            "Epoch 84/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9032 - val_loss: 7.9218\n",
            "Epoch 85/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9031 - val_loss: 7.9228\n",
            "Epoch 86/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9031 - val_loss: 7.9236\n",
            "Epoch 87/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 7.9031 - val_loss: 7.9239\n",
            "Epoch 88/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9031 - val_loss: 7.9235\n",
            "Epoch 89/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9031 - val_loss: 7.9242\n",
            "Epoch 90/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9031 - val_loss: 7.9247\n",
            "Epoch 91/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9030 - val_loss: 7.9242\n",
            "Epoch 92/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9032 - val_loss: 7.9241\n",
            "Epoch 93/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9030 - val_loss: 7.9243\n",
            "Epoch 94/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9031 - val_loss: 7.9240\n",
            "Epoch 95/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9030 - val_loss: 7.9237\n",
            "Epoch 96/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9246\n",
            "Epoch 97/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9030 - val_loss: 7.9247\n",
            "Epoch 98/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9030 - val_loss: 7.9254\n",
            "Epoch 99/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9031 - val_loss: 7.9256\n",
            "Epoch 100/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9031 - val_loss: 7.9252\n",
            "Epoch 101/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9030 - val_loss: 7.9238\n",
            "Epoch 102/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9030 - val_loss: 7.9241\n",
            "Epoch 103/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9030 - val_loss: 7.9243\n",
            "Epoch 104/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9030 - val_loss: 7.9247\n",
            "Epoch 105/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9030 - val_loss: 7.9243\n",
            "Epoch 106/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9029 - val_loss: 7.9237\n",
            "Epoch 107/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9029 - val_loss: 7.9239\n",
            "Epoch 108/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9030 - val_loss: 7.9238\n",
            "Epoch 109/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9030 - val_loss: 7.9251\n",
            "Epoch 110/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9030 - val_loss: 7.9249\n",
            "Epoch 111/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9030 - val_loss: 7.9257\n",
            "Epoch 112/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9030 - val_loss: 7.9249\n",
            "Epoch 113/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9249\n",
            "Epoch 114/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9029 - val_loss: 7.9259\n",
            "Epoch 115/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9030 - val_loss: 7.9247\n",
            "Epoch 116/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9029 - val_loss: 7.9243\n",
            "Epoch 117/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9030 - val_loss: 7.9249\n",
            "Epoch 118/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9029 - val_loss: 7.9248\n",
            "Epoch 119/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9029 - val_loss: 7.9244\n",
            "Epoch 120/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9030 - val_loss: 7.9256\n",
            "Epoch 121/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9030 - val_loss: 7.9254\n",
            "Epoch 122/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9030 - val_loss: 7.9239\n",
            "Epoch 123/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9030 - val_loss: 7.9249\n",
            "Epoch 124/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9029 - val_loss: 7.9240\n",
            "Epoch 125/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9030 - val_loss: 7.9251\n",
            "Epoch 126/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9030 - val_loss: 7.9257\n",
            "Epoch 127/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9028 - val_loss: 7.9250\n",
            "Epoch 128/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9029 - val_loss: 7.9259\n",
            "Epoch 129/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9030 - val_loss: 7.9263\n",
            "Epoch 130/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9030 - val_loss: 7.9242\n",
            "Epoch 131/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9029 - val_loss: 7.9238\n",
            "Epoch 132/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9029 - val_loss: 7.9240\n",
            "Epoch 133/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9029 - val_loss: 7.9242\n",
            "Epoch 134/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9030 - val_loss: 7.9241\n",
            "Epoch 135/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9029 - val_loss: 7.9235\n",
            "Epoch 136/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9030 - val_loss: 7.9237\n",
            "Epoch 137/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9030 - val_loss: 7.9237\n",
            "Epoch 138/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9238\n",
            "Epoch 139/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9239\n",
            "Epoch 140/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9030 - val_loss: 7.9239\n",
            "Epoch 141/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9029 - val_loss: 7.9241\n",
            "Epoch 142/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9029 - val_loss: 7.9240\n",
            "Epoch 143/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9028 - val_loss: 7.9236\n",
            "Epoch 144/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9245\n",
            "Epoch 145/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9029 - val_loss: 7.9248\n",
            "Epoch 146/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9029 - val_loss: 7.9237\n",
            "Epoch 147/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9029 - val_loss: 7.9236\n",
            "Epoch 148/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9030 - val_loss: 7.9245\n",
            "Epoch 149/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9030 - val_loss: 7.9238\n",
            "Epoch 150/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9029 - val_loss: 7.9234\n",
            "Epoch 151/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9029 - val_loss: 7.9239\n",
            "Epoch 152/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9029 - val_loss: 7.9246\n",
            "Epoch 153/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9239\n",
            "Epoch 154/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9029 - val_loss: 7.9237\n",
            "Epoch 155/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9029 - val_loss: 7.9250\n",
            "Epoch 156/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9029 - val_loss: 7.9244\n",
            "Epoch 157/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9251\n",
            "Epoch 158/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9252\n",
            "Epoch 159/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9029 - val_loss: 7.9251\n",
            "Epoch 160/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9028 - val_loss: 7.9244\n",
            "Epoch 161/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9028 - val_loss: 7.9243\n",
            "Epoch 162/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9259\n",
            "Epoch 163/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9257\n",
            "Epoch 164/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9029 - val_loss: 7.9246\n",
            "Epoch 165/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9029 - val_loss: 7.9242\n",
            "Epoch 166/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9028 - val_loss: 7.9240\n",
            "Epoch 167/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9243\n",
            "Epoch 168/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9029 - val_loss: 7.9247\n",
            "Epoch 169/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9244\n",
            "Epoch 170/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9029 - val_loss: 7.9244\n",
            "Epoch 171/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9028 - val_loss: 7.9246\n",
            "Epoch 172/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9028 - val_loss: 7.9243\n",
            "Epoch 173/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9029 - val_loss: 7.9252\n",
            "Epoch 174/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9028 - val_loss: 7.9247\n",
            "Epoch 175/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9249\n",
            "Epoch 176/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9251\n",
            "Epoch 177/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9028 - val_loss: 7.9240\n",
            "Epoch 178/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9028 - val_loss: 7.9251\n",
            "Epoch 179/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9029 - val_loss: 7.9253\n",
            "Epoch 180/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 7.9028 - val_loss: 7.9247\n",
            "Epoch 181/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9028 - val_loss: 7.9244\n",
            "Epoch 182/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9029 - val_loss: 7.9347\n",
            "Epoch 183/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9029 - val_loss: 7.9250\n",
            "Epoch 184/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9242\n",
            "Epoch 185/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9029 - val_loss: 7.9243\n",
            "Epoch 186/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9029 - val_loss: 7.9251\n",
            "Epoch 187/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9252\n",
            "Epoch 188/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9028 - val_loss: 7.9260\n",
            "Epoch 189/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9029 - val_loss: 7.9255\n",
            "Epoch 190/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9260\n",
            "Epoch 191/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9029 - val_loss: 7.9256\n",
            "Epoch 192/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9267\n",
            "Epoch 193/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9029 - val_loss: 7.9264\n",
            "Epoch 194/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9028 - val_loss: 7.9266\n",
            "Epoch 195/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9267\n",
            "Epoch 196/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9029 - val_loss: 7.9282\n",
            "Epoch 197/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9029 - val_loss: 7.9254\n",
            "Epoch 198/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9028 - val_loss: 7.9256\n",
            "Epoch 199/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9253\n",
            "Epoch 200/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9028 - val_loss: 7.9252\n",
            "Epoch 201/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9028 - val_loss: 7.9257\n",
            "Epoch 202/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9027 - val_loss: 7.9257\n",
            "Epoch 203/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9028 - val_loss: 7.9249\n",
            "Epoch 204/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9250\n",
            "Epoch 205/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9255\n",
            "Epoch 206/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9252\n",
            "Epoch 207/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9249\n",
            "Epoch 208/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9028 - val_loss: 7.9258\n",
            "Epoch 209/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9256\n",
            "Epoch 210/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9260\n",
            "Epoch 211/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9028 - val_loss: 7.9258\n",
            "Epoch 212/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9262\n",
            "Epoch 213/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9256\n",
            "Epoch 214/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9029 - val_loss: 7.9253\n",
            "Epoch 215/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9028 - val_loss: 7.9256\n",
            "Epoch 216/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9028 - val_loss: 7.9259\n",
            "Epoch 217/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9255\n",
            "Epoch 218/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9263\n",
            "Epoch 219/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9027 - val_loss: 7.9264\n",
            "Epoch 220/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9028 - val_loss: 7.9252\n",
            "Epoch 221/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9255\n",
            "Epoch 222/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9028 - val_loss: 7.9253\n",
            "Epoch 223/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9252\n",
            "Epoch 224/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9252\n",
            "Epoch 225/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9027 - val_loss: 7.9250\n",
            "Epoch 226/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9257\n",
            "Epoch 227/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9253\n",
            "Epoch 228/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9254\n",
            "Epoch 229/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9028 - val_loss: 7.9241\n",
            "Epoch 230/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9251\n",
            "Epoch 231/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9028 - val_loss: 7.9257\n",
            "Epoch 232/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9027 - val_loss: 7.9249\n",
            "Epoch 233/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9253\n",
            "Epoch 234/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9028 - val_loss: 7.9260\n",
            "Epoch 235/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9253\n",
            "Epoch 236/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9028 - val_loss: 7.9260\n",
            "Epoch 237/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9029 - val_loss: 7.9257\n",
            "Epoch 238/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9029 - val_loss: 7.9266\n",
            "Epoch 239/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9027 - val_loss: 7.9253\n",
            "Epoch 240/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9028 - val_loss: 7.9249\n",
            "Epoch 241/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9253\n",
            "Epoch 242/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9027 - val_loss: 7.9249\n",
            "Epoch 243/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9027 - val_loss: 7.9253\n",
            "Epoch 244/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9029 - val_loss: 7.9260\n",
            "Epoch 245/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9028 - val_loss: 7.9254\n",
            "Epoch 246/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9028 - val_loss: 7.9267\n",
            "Epoch 247/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9261\n",
            "Epoch 248/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9028 - val_loss: 7.9268\n",
            "Epoch 249/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9028 - val_loss: 7.9280\n",
            "Epoch 250/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9028 - val_loss: 7.9262\n",
            "Epoch 251/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9027 - val_loss: 7.9266\n",
            "Epoch 252/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9027 - val_loss: 7.9258\n",
            "Epoch 253/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9028 - val_loss: 7.9261\n",
            "Epoch 254/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9027 - val_loss: 7.9259\n",
            "Epoch 255/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9028 - val_loss: 7.9271\n",
            "Epoch 256/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9028 - val_loss: 7.9266\n",
            "Epoch 257/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9028 - val_loss: 7.9279\n",
            "Epoch 258/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9257\n",
            "Epoch 259/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9028 - val_loss: 7.9276\n",
            "Epoch 260/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9027 - val_loss: 7.9277\n",
            "Epoch 261/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9258\n",
            "Epoch 262/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9028 - val_loss: 7.9263\n",
            "Epoch 263/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9273\n",
            "Epoch 264/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9027 - val_loss: 7.9265\n",
            "Epoch 265/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9027 - val_loss: 7.9269\n",
            "Epoch 266/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9028 - val_loss: 7.9266\n",
            "Epoch 267/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9028 - val_loss: 7.9263\n",
            "Epoch 268/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9027 - val_loss: 7.9262\n",
            "Epoch 269/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9028 - val_loss: 7.9269\n",
            "Epoch 270/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9027 - val_loss: 7.9271\n",
            "Epoch 271/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9028 - val_loss: 7.9281\n",
            "Epoch 272/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9267\n",
            "Epoch 273/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9027 - val_loss: 7.9273\n",
            "Epoch 274/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9027 - val_loss: 7.9271\n",
            "Epoch 275/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9271\n",
            "Epoch 276/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9257\n",
            "Epoch 277/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9027 - val_loss: 7.9268\n",
            "Epoch 278/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9027 - val_loss: 7.9260\n",
            "Epoch 279/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9027 - val_loss: 7.9263\n",
            "Epoch 280/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9027 - val_loss: 7.9284\n",
            "Epoch 281/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 7.9027 - val_loss: 7.9271\n",
            "Epoch 282/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9028 - val_loss: 7.9268\n",
            "Epoch 283/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9028 - val_loss: 7.9273\n",
            "Epoch 284/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9027 - val_loss: 7.9262\n",
            "Epoch 285/350\n",
            "4518/4518 [==============================] - 1s 304us/step - loss: 7.9027 - val_loss: 7.9278\n",
            "Epoch 286/350\n",
            "4518/4518 [==============================] - 1s 303us/step - loss: 7.9028 - val_loss: 7.9271\n",
            "Epoch 287/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 7.9028 - val_loss: 7.9280\n",
            "Epoch 288/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9028 - val_loss: 7.9273\n",
            "Epoch 289/350\n",
            "4518/4518 [==============================] - 1s 304us/step - loss: 7.9028 - val_loss: 7.9280\n",
            "Epoch 290/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 7.9027 - val_loss: 7.9255\n",
            "Epoch 291/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9027 - val_loss: 7.9255\n",
            "Epoch 292/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9263\n",
            "Epoch 293/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9027 - val_loss: 7.9269\n",
            "Epoch 294/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9027 - val_loss: 7.9269\n",
            "Epoch 295/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9028 - val_loss: 7.9274\n",
            "Epoch 296/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9028 - val_loss: 7.9276\n",
            "Epoch 297/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9027 - val_loss: 7.9265\n",
            "Epoch 298/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9027 - val_loss: 7.9266\n",
            "Epoch 299/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9027 - val_loss: 7.9266\n",
            "Epoch 300/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9260\n",
            "Epoch 301/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9028 - val_loss: 7.9271\n",
            "Epoch 302/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9257\n",
            "Epoch 303/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9267\n",
            "Epoch 304/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9279\n",
            "Epoch 305/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9027 - val_loss: 7.9273\n",
            "Epoch 306/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9026 - val_loss: 7.9265\n",
            "Epoch 307/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9282\n",
            "Epoch 308/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9027 - val_loss: 7.9272\n",
            "Epoch 309/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9283\n",
            "Epoch 310/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9027 - val_loss: 7.9274\n",
            "Epoch 311/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9026 - val_loss: 7.9280\n",
            "Epoch 312/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9026 - val_loss: 7.9292\n",
            "Epoch 313/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9027 - val_loss: 7.9290\n",
            "Epoch 314/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 7.9027 - val_loss: 7.9271\n",
            "Epoch 315/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9026 - val_loss: 7.9266\n",
            "Epoch 316/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9027 - val_loss: 7.9265\n",
            "Epoch 317/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9280\n",
            "Epoch 318/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9026 - val_loss: 7.9293\n",
            "Epoch 319/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9265\n",
            "Epoch 320/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9027 - val_loss: 7.9273\n",
            "Epoch 321/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 7.9027 - val_loss: 7.9277\n",
            "Epoch 322/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9026 - val_loss: 7.9292\n",
            "Epoch 323/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9290\n",
            "Epoch 324/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9027 - val_loss: 7.9266\n",
            "Epoch 325/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9027 - val_loss: 7.9263\n",
            "Epoch 326/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9027 - val_loss: 7.9275\n",
            "Epoch 327/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9026 - val_loss: 7.9277\n",
            "Epoch 328/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9027 - val_loss: 7.9273\n",
            "Epoch 329/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 7.9026 - val_loss: 7.9276\n",
            "Epoch 330/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9026 - val_loss: 7.9278\n",
            "Epoch 331/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9027 - val_loss: 7.9270\n",
            "Epoch 332/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 7.9027 - val_loss: 7.9280\n",
            "Epoch 333/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9027 - val_loss: 7.9286\n",
            "Epoch 334/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9027 - val_loss: 7.9290\n",
            "Epoch 335/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9026 - val_loss: 7.9296\n",
            "Epoch 336/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9027 - val_loss: 7.9281\n",
            "Epoch 337/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9027 - val_loss: 7.9274\n",
            "Epoch 338/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9027 - val_loss: 7.9295\n",
            "Epoch 339/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9026 - val_loss: 7.9288\n",
            "Epoch 340/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9026 - val_loss: 7.9283\n",
            "Epoch 341/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 7.9027 - val_loss: 7.9297\n",
            "Epoch 342/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9027 - val_loss: 7.9313\n",
            "Epoch 343/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9026 - val_loss: 7.9287\n",
            "Epoch 344/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9027 - val_loss: 7.9293\n",
            "Epoch 345/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 7.9026 - val_loss: 7.9317\n",
            "Epoch 346/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9027 - val_loss: 7.9308\n",
            "Epoch 347/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 7.9027 - val_loss: 7.9300\n",
            "Epoch 348/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 7.9026 - val_loss: 7.9298\n",
            "Epoch 349/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 7.9026 - val_loss: 7.9300\n",
            "Epoch 350/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 7.9026 - val_loss: 7.9321\n",
            "(lamda,Threshold) 0.5 0.25\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (4564, 784) 784\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 10704 0.5\n",
            "The shape of N (4564, 784)\n",
            "The minimum value of N  -0.75\n",
            "The max value of N 0.7499740123748779\n",
            "[INFO:] Xclean  MSE Computed shape (4564, 784)\n",
            "[INFO:]Xdecoded  Computed shape (4564, 784)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0022015534, 0.0016917054])\n",
            "[INFO:] The anomaly threshold computed is  0.0016917054\n",
            "side: 28\n",
            "channel: 1\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [4547, 4533, 4532, 4539, 4550, 4524, 4522, 4561, 4518, 4526, 4559, 4536, 4563, 4542, 4545, 4551, 4562, 4554, 4523, 4521, 4525, 4529, 4538, 4535, 4520, 4548, 4541, 4527, 4519, 4530, 4549, 4531, 4528, 4558, 4552, 4557, 4540, 4553, 4537, 4543, 4544, 4560, 4556, 4534, 1906, 4546]\n",
            "[INFO:] The worstreconstructed_Top200index index are  [4547, 4533, 4532, 4539, 4550, 4524, 4522, 4561, 4518, 4526, 4559, 4536, 4563, 4542, 4545, 4551, 4562, 4554, 4523, 4521, 4525, 4529, 4538, 4535, 4520, 4548, 4541, 4527, 4519, 4530, 4549, 4531, 4528, 4558, 4552, 4557, 4540, 4553, 4537, 4543, 4544, 4560, 4556, 4534, 1906, 4546, 4555, 1761, 3527, 277, 2677, 449, 569, 591, 3229, 2593, 89, 4213, 1373, 826, 3723, 3068, 2790, 2764, 3542, 3844, 803, 4423, 237, 422, 691, 2086, 2832, 4162, 263, 247, 478, 667, 1144, 1452, 1953, 2237, 2179, 433, 64, 676, 2916, 2887, 2951, 1760, 3034, 4015, 3855, 4339, 3828, 1080, 4315, 318, 7, 4368, 2450, 3054, 657, 2894, 2232, 3153, 2085, 3996, 3862, 2776, 4394, 4350, 4076, 1503, 221, 521, 4143, 1849, 875, 1831, 4345, 3419, 3381, 4451, 4067, 1309, 3491, 3674, 2359, 218, 1459, 2619, 4239, 4483, 3553, 3247, 3472, 190, 1091, 185, 4443, 3759, 602, 2129, 4075, 4408, 3191, 4221, 1477, 2813, 3275, 903, 2484, 2684, 4130, 4476, 4407, 1193, 3258, 814, 3901, 223, 2365, 3874, 4229, 2120, 2220, 2480, 470, 3960, 1048, 1298, 2743, 2190, 189, 3433, 962, 895, 624, 1171, 3364, 525, 3672, 3077, 1368, 2037, 3817, 1387, 1538, 1065, 3236, 203, 178, 1585, 3196, 2408, 965, 544, 4009, 4441]\n",
            "=====================\n",
            "AUROC 0.5 0.9891304347826088\n",
            "=======================\n",
            "Saved model to disk....\n",
            "[INFO] compiling model...\n",
            "Train on 4518 samples, validate on 46 samples\n",
            "Epoch 1/350\n",
            "4518/4518 [==============================] - 4s 830us/step - loss: 16.8092 - val_loss: 16.8366\n",
            "Epoch 2/350\n",
            "4518/4518 [==============================] - 1s 319us/step - loss: 16.8092 - val_loss: 16.8406\n",
            "Epoch 3/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8092 - val_loss: 16.8390\n",
            "Epoch 4/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8092 - val_loss: 16.8405\n",
            "Epoch 5/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8399\n",
            "Epoch 6/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8091 - val_loss: 16.8382\n",
            "Epoch 7/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8409\n",
            "Epoch 8/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8092 - val_loss: 16.8393\n",
            "Epoch 9/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8092 - val_loss: 16.8385\n",
            "Epoch 10/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8398\n",
            "Epoch 11/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8364\n",
            "Epoch 12/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8091 - val_loss: 16.8363\n",
            "Epoch 13/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8092 - val_loss: 16.8400\n",
            "Epoch 14/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8091 - val_loss: 16.8400\n",
            "Epoch 15/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8386\n",
            "Epoch 16/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8092 - val_loss: 16.8394\n",
            "Epoch 17/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8091 - val_loss: 16.8378\n",
            "Epoch 18/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8091 - val_loss: 16.8404\n",
            "Epoch 19/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 16.8092 - val_loss: 16.8336\n",
            "Epoch 20/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8093 - val_loss: 16.8355\n",
            "Epoch 21/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8092 - val_loss: 16.8370\n",
            "Epoch 22/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8397\n",
            "Epoch 23/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8388\n",
            "Epoch 24/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8091 - val_loss: 16.8387\n",
            "Epoch 25/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8091 - val_loss: 16.8376\n",
            "Epoch 26/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8091 - val_loss: 16.8400\n",
            "Epoch 27/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8091 - val_loss: 16.8401\n",
            "Epoch 28/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8419\n",
            "Epoch 29/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8446\n",
            "Epoch 30/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8092 - val_loss: 16.8378\n",
            "Epoch 31/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8421\n",
            "Epoch 32/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8091 - val_loss: 16.8386\n",
            "Epoch 33/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8091 - val_loss: 16.8403\n",
            "Epoch 34/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8398\n",
            "Epoch 35/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8091 - val_loss: 16.8444\n",
            "Epoch 36/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8441\n",
            "Epoch 37/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8411\n",
            "Epoch 38/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8410\n",
            "Epoch 39/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8430\n",
            "Epoch 40/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8482\n",
            "Epoch 41/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8398\n",
            "Epoch 42/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8453\n",
            "Epoch 43/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8413\n",
            "Epoch 44/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8091 - val_loss: 16.8446\n",
            "Epoch 45/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8457\n",
            "Epoch 46/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8428\n",
            "Epoch 47/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8420\n",
            "Epoch 48/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8431\n",
            "Epoch 49/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8434\n",
            "Epoch 50/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8479\n",
            "Epoch 51/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8092 - val_loss: 16.8456\n",
            "Epoch 52/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8091 - val_loss: 16.8556\n",
            "Epoch 53/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8091 - val_loss: 16.8513\n",
            "Epoch 54/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8530\n",
            "Epoch 55/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8527\n",
            "Epoch 56/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8091 - val_loss: 16.8531\n",
            "Epoch 57/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8091 - val_loss: 16.8527\n",
            "Epoch 58/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8091 - val_loss: 16.8498\n",
            "Epoch 59/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8504\n",
            "Epoch 60/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8091 - val_loss: 16.8500\n",
            "Epoch 61/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8091 - val_loss: 16.8524\n",
            "Epoch 62/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8091 - val_loss: 16.8515\n",
            "Epoch 63/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8509\n",
            "Epoch 64/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8507\n",
            "Epoch 65/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8517\n",
            "Epoch 66/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8522\n",
            "Epoch 67/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8521\n",
            "Epoch 68/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8466\n",
            "Epoch 69/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8091 - val_loss: 16.8465\n",
            "Epoch 70/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8091 - val_loss: 16.8483\n",
            "Epoch 71/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8485\n",
            "Epoch 72/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8091 - val_loss: 16.8478\n",
            "Epoch 73/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8484\n",
            "Epoch 74/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8091 - val_loss: 16.8494\n",
            "Epoch 75/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8499\n",
            "Epoch 76/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8488\n",
            "Epoch 77/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8091 - val_loss: 16.8500\n",
            "Epoch 78/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8493\n",
            "Epoch 79/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8491\n",
            "Epoch 80/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8502\n",
            "Epoch 81/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8505\n",
            "Epoch 82/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8491\n",
            "Epoch 83/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8514\n",
            "Epoch 84/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8510\n",
            "Epoch 85/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8518\n",
            "Epoch 86/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8512\n",
            "Epoch 87/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8090 - val_loss: 16.8504\n",
            "Epoch 88/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8544\n",
            "Epoch 89/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8091 - val_loss: 16.8550\n",
            "Epoch 90/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8539\n",
            "Epoch 91/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8091 - val_loss: 16.8533\n",
            "Epoch 92/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8540\n",
            "Epoch 93/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8549\n",
            "Epoch 94/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8544\n",
            "Epoch 95/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8090 - val_loss: 16.8532\n",
            "Epoch 96/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 16.8090 - val_loss: 16.8574\n",
            "Epoch 97/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 16.8090 - val_loss: 16.8559\n",
            "Epoch 98/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8541\n",
            "Epoch 99/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8091 - val_loss: 16.8550\n",
            "Epoch 100/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8091 - val_loss: 16.8571\n",
            "Epoch 101/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8544\n",
            "Epoch 102/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8529\n",
            "Epoch 103/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8546\n",
            "Epoch 104/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8091 - val_loss: 16.8564\n",
            "Epoch 105/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8091 - val_loss: 16.8534\n",
            "Epoch 106/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8404\n",
            "Epoch 107/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8407\n",
            "Epoch 108/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8508\n",
            "Epoch 109/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8525\n",
            "Epoch 110/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8521\n",
            "Epoch 111/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8091 - val_loss: 16.8528\n",
            "Epoch 112/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 16.8091 - val_loss: 16.8528\n",
            "Epoch 113/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8522\n",
            "Epoch 114/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8529\n",
            "Epoch 115/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8530\n",
            "Epoch 116/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8090 - val_loss: 16.8532\n",
            "Epoch 117/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8090 - val_loss: 16.8527\n",
            "Epoch 118/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8090 - val_loss: 16.8526\n",
            "Epoch 119/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8511\n",
            "Epoch 120/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8091 - val_loss: 16.8532\n",
            "Epoch 121/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8538\n",
            "Epoch 122/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8532\n",
            "Epoch 123/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8528\n",
            "Epoch 124/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8523\n",
            "Epoch 125/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8530\n",
            "Epoch 126/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8539\n",
            "Epoch 127/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8089 - val_loss: 16.8539\n",
            "Epoch 128/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8555\n",
            "Epoch 129/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8548\n",
            "Epoch 130/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8548\n",
            "Epoch 131/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8550\n",
            "Epoch 132/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8091 - val_loss: 16.8559\n",
            "Epoch 133/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8091 - val_loss: 16.8533\n",
            "Epoch 134/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8538\n",
            "Epoch 135/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8090 - val_loss: 16.8515\n",
            "Epoch 136/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8526\n",
            "Epoch 137/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8090 - val_loss: 16.8551\n",
            "Epoch 138/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8543\n",
            "Epoch 139/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8517\n",
            "Epoch 140/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8538\n",
            "Epoch 141/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8573\n",
            "Epoch 142/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8089 - val_loss: 16.8564\n",
            "Epoch 143/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8090 - val_loss: 16.8563\n",
            "Epoch 144/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8574\n",
            "Epoch 145/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8569\n",
            "Epoch 146/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8570\n",
            "Epoch 147/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 16.8090 - val_loss: 16.8554\n",
            "Epoch 148/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8574\n",
            "Epoch 149/350\n",
            "4518/4518 [==============================] - 1s 304us/step - loss: 16.8090 - val_loss: 16.8577\n",
            "Epoch 150/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 16.8090 - val_loss: 16.8575\n",
            "Epoch 151/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8587\n",
            "Epoch 152/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8567\n",
            "Epoch 153/350\n",
            "4518/4518 [==============================] - 1s 304us/step - loss: 16.8090 - val_loss: 16.8560\n",
            "Epoch 154/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8570\n",
            "Epoch 155/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 16.8090 - val_loss: 16.8581\n",
            "Epoch 156/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8576\n",
            "Epoch 157/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 16.8089 - val_loss: 16.8573\n",
            "Epoch 158/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8090 - val_loss: 16.8578\n",
            "Epoch 159/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8575\n",
            "Epoch 160/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8557\n",
            "Epoch 161/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8570\n",
            "Epoch 162/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8559\n",
            "Epoch 163/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8556\n",
            "Epoch 164/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8564\n",
            "Epoch 165/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8563\n",
            "Epoch 166/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 16.8090 - val_loss: 16.8556\n",
            "Epoch 167/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8555\n",
            "Epoch 168/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8562\n",
            "Epoch 169/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 16.8090 - val_loss: 16.8557\n",
            "Epoch 170/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8560\n",
            "Epoch 171/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8553\n",
            "Epoch 172/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8565\n",
            "Epoch 173/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8585\n",
            "Epoch 174/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8091 - val_loss: 16.8558\n",
            "Epoch 175/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8568\n",
            "Epoch 176/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8566\n",
            "Epoch 177/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8561\n",
            "Epoch 178/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8540\n",
            "Epoch 179/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8550\n",
            "Epoch 180/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8545\n",
            "Epoch 181/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8532\n",
            "Epoch 182/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8553\n",
            "Epoch 183/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8567\n",
            "Epoch 184/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8559\n",
            "Epoch 185/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8562\n",
            "Epoch 186/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8555\n",
            "Epoch 187/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8530\n",
            "Epoch 188/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8534\n",
            "Epoch 189/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8535\n",
            "Epoch 190/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8090 - val_loss: 16.8576\n",
            "Epoch 191/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8596\n",
            "Epoch 192/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8547\n",
            "Epoch 193/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8089 - val_loss: 16.8568\n",
            "Epoch 194/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8090 - val_loss: 16.8555\n",
            "Epoch 195/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8553\n",
            "Epoch 196/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8558\n",
            "Epoch 197/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8594\n",
            "Epoch 198/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8574\n",
            "Epoch 199/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8591\n",
            "Epoch 200/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8577\n",
            "Epoch 201/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8575\n",
            "Epoch 202/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8574\n",
            "Epoch 203/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8590\n",
            "Epoch 204/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8595\n",
            "Epoch 205/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8089 - val_loss: 16.8565\n",
            "Epoch 206/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8552\n",
            "Epoch 207/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8089 - val_loss: 16.8575\n",
            "Epoch 208/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8090 - val_loss: 16.8589\n",
            "Epoch 209/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 16.8090 - val_loss: 16.8561\n",
            "Epoch 210/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 16.8089 - val_loss: 16.8563\n",
            "Epoch 211/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8566\n",
            "Epoch 212/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8578\n",
            "Epoch 213/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8558\n",
            "Epoch 214/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8089 - val_loss: 16.8566\n",
            "Epoch 215/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8570\n",
            "Epoch 216/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8091 - val_loss: 16.8564\n",
            "Epoch 217/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8597\n",
            "Epoch 218/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8090 - val_loss: 16.8590\n",
            "Epoch 219/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8574\n",
            "Epoch 220/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8089 - val_loss: 16.8598\n",
            "Epoch 221/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8607\n",
            "Epoch 222/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8608\n",
            "Epoch 223/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8089 - val_loss: 16.8591\n",
            "Epoch 224/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8602\n",
            "Epoch 225/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8586\n",
            "Epoch 226/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8602\n",
            "Epoch 227/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8601\n",
            "Epoch 228/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8583\n",
            "Epoch 229/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8090 - val_loss: 16.8616\n",
            "Epoch 230/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8089 - val_loss: 16.8597\n",
            "Epoch 231/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8089 - val_loss: 16.8609\n",
            "Epoch 232/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8579\n",
            "Epoch 233/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8603\n",
            "Epoch 234/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 16.8090 - val_loss: 16.8609\n",
            "Epoch 235/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 16.8089 - val_loss: 16.8612\n",
            "Epoch 236/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8593\n",
            "Epoch 237/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8596\n",
            "Epoch 238/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8089 - val_loss: 16.8581\n",
            "Epoch 239/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8089 - val_loss: 16.8576\n",
            "Epoch 240/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8542\n",
            "Epoch 241/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8593\n",
            "Epoch 242/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8588\n",
            "Epoch 243/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8506\n",
            "Epoch 244/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8555\n",
            "Epoch 245/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8571\n",
            "Epoch 246/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8500\n",
            "Epoch 247/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8585\n",
            "Epoch 248/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8091 - val_loss: 16.8560\n",
            "Epoch 249/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8593\n",
            "Epoch 250/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8540\n",
            "Epoch 251/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8539\n",
            "Epoch 252/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8089 - val_loss: 16.8576\n",
            "Epoch 253/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8553\n",
            "Epoch 254/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8089 - val_loss: 16.8547\n",
            "Epoch 255/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8587\n",
            "Epoch 256/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8089 - val_loss: 16.8568\n",
            "Epoch 257/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8089 - val_loss: 16.8587\n",
            "Epoch 258/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8603\n",
            "Epoch 259/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8587\n",
            "Epoch 260/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8557\n",
            "Epoch 261/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8588\n",
            "Epoch 262/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8575\n",
            "Epoch 263/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8601\n",
            "Epoch 264/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8089 - val_loss: 16.8594\n",
            "Epoch 265/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8572\n",
            "Epoch 266/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8596\n",
            "Epoch 267/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8090 - val_loss: 16.8587\n",
            "Epoch 268/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8568\n",
            "Epoch 269/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8089 - val_loss: 16.8563\n",
            "Epoch 270/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8566\n",
            "Epoch 271/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8089 - val_loss: 16.8549\n",
            "Epoch 272/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8528\n",
            "Epoch 273/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8559\n",
            "Epoch 274/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8089 - val_loss: 16.8568\n",
            "Epoch 275/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 16.8089 - val_loss: 16.8517\n",
            "Epoch 276/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8575\n",
            "Epoch 277/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8556\n",
            "Epoch 278/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8563\n",
            "Epoch 279/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8578\n",
            "Epoch 280/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8576\n",
            "Epoch 281/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8596\n",
            "Epoch 282/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8595\n",
            "Epoch 283/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8089 - val_loss: 16.8603\n",
            "Epoch 284/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8089 - val_loss: 16.8564\n",
            "Epoch 285/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8090 - val_loss: 16.8562\n",
            "Epoch 286/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8557\n",
            "Epoch 287/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8582\n",
            "Epoch 288/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8594\n",
            "Epoch 289/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8596\n",
            "Epoch 290/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8575\n",
            "Epoch 291/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8591\n",
            "Epoch 292/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8089 - val_loss: 16.8573\n",
            "Epoch 293/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8504\n",
            "Epoch 294/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8593\n",
            "Epoch 295/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8560\n",
            "Epoch 296/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8089 - val_loss: 16.8584\n",
            "Epoch 297/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8588\n",
            "Epoch 298/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8593\n",
            "Epoch 299/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8578\n",
            "Epoch 300/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8598\n",
            "Epoch 301/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8589\n",
            "Epoch 302/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8090 - val_loss: 16.8583\n",
            "Epoch 303/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8090 - val_loss: 16.8548\n",
            "Epoch 304/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8592\n",
            "Epoch 305/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8089 - val_loss: 16.8605\n",
            "Epoch 306/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 16.8090 - val_loss: 16.8602\n",
            "Epoch 307/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8601\n",
            "Epoch 308/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8600\n",
            "Epoch 309/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8597\n",
            "Epoch 310/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8600\n",
            "Epoch 311/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8598\n",
            "Epoch 312/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8090 - val_loss: 16.8545\n",
            "Epoch 313/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8577\n",
            "Epoch 314/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8089 - val_loss: 16.8594\n",
            "Epoch 315/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8089 - val_loss: 16.8600\n",
            "Epoch 316/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8487\n",
            "Epoch 317/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 16.8089 - val_loss: 16.8548\n",
            "Epoch 318/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8555\n",
            "Epoch 319/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8089 - val_loss: 16.8596\n",
            "Epoch 320/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8089 - val_loss: 16.8719\n",
            "Epoch 321/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8472\n",
            "Epoch 322/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8089 - val_loss: 16.8458\n",
            "Epoch 323/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8089 - val_loss: 16.8461\n",
            "Epoch 324/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8088 - val_loss: 16.8458\n",
            "Epoch 325/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8089 - val_loss: 16.8455\n",
            "Epoch 326/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8440\n",
            "Epoch 327/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8447\n",
            "Epoch 328/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8089 - val_loss: 16.8435\n",
            "Epoch 329/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8466\n",
            "Epoch 330/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8465\n",
            "Epoch 331/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 16.8089 - val_loss: 16.8461\n",
            "Epoch 332/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8453\n",
            "Epoch 333/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8090 - val_loss: 16.8468\n",
            "Epoch 334/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8468\n",
            "Epoch 335/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8446\n",
            "Epoch 336/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8456\n",
            "Epoch 337/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 16.8089 - val_loss: 16.8445\n",
            "Epoch 338/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8089 - val_loss: 16.8463\n",
            "Epoch 339/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 16.8088 - val_loss: 16.8444\n",
            "Epoch 340/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8457\n",
            "Epoch 341/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8090 - val_loss: 16.8465\n",
            "Epoch 342/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8429\n",
            "Epoch 343/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8423\n",
            "Epoch 344/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8463\n",
            "Epoch 345/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8089 - val_loss: 16.8434\n",
            "Epoch 346/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 16.8089 - val_loss: 16.8446\n",
            "Epoch 347/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8089 - val_loss: 16.8453\n",
            "Epoch 348/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 16.8089 - val_loss: 16.8452\n",
            "Epoch 349/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 16.8089 - val_loss: 16.8455\n",
            "Epoch 350/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 16.8089 - val_loss: 16.8449\n",
            "(lamda,Threshold) 1.0 0.5\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (4564, 784) 784\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 1725 1.0\n",
            "The shape of N (4564, 784)\n",
            "The minimum value of N  -0.5\n",
            "The max value of N 0.4999919533729553\n",
            "[INFO:] Xclean  MSE Computed shape (4564, 784)\n",
            "[INFO:]Xdecoded  Computed shape (4564, 784)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0022015534, 0.0016917054, 0.0014983624])\n",
            "[INFO:] The anomaly threshold computed is  0.0014983624\n",
            "side: 28\n",
            "channel: 1\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [4547, 4563, 4539, 4532, 4524, 4550, 4522, 4559, 4518, 4526, 4536, 4561, 4542, 4521, 4533, 4551, 4523, 4525, 4562, 4535, 4554, 4529, 4538, 4520, 4541, 4545, 4548, 4531, 4549, 4530, 4519, 4552, 4527, 4528, 4558, 4540, 4557, 4553, 2758, 4537, 4556, 4543, 4560, 4546, 4544, 1906]\n",
            "[INFO:] The worstreconstructed_Top200index index are  [4547, 4563, 4539, 4532, 4524, 4550, 4522, 4559, 4518, 4526, 4536, 4561, 4542, 4521, 4533, 4551, 4523, 4525, 4562, 4535, 4554, 4529, 4538, 4520, 4541, 4545, 4548, 4531, 4549, 4530, 4519, 4552, 4527, 4528, 4558, 4540, 4557, 4553, 2758, 4537, 4556, 4543, 4560, 4546, 4544, 1906, 4534, 4555, 277, 2677, 1761, 449, 3527, 4213, 569, 2593, 3068, 89, 826, 591, 3229, 3060, 3723, 3828, 478, 2320, 7, 2086, 1452, 3855, 2790, 4443, 433, 2179, 4368, 3034, 3553, 803, 1373, 602, 3149, 657, 1953, 2945, 237, 64, 691, 1760, 3844, 4339, 221, 3191, 422, 263, 2484, 2232, 1557, 2821, 861, 4483, 4350, 470, 3419, 2832, 2589, 4394, 4423, 2172, 2684, 4345, 1855, 2764, 3996, 223, 814, 2887, 318, 4351, 1048, 185, 965, 3759, 3674, 3494, 1144, 4067, 3873, 57, 72, 3381, 412, 1503, 1368, 1091, 633, 544, 3623, 2330, 1193, 764, 3817, 875, 2894, 2390, 2473, 4019, 513, 2237, 3704, 2951, 190, 3433, 3613, 2190, 2914, 1849, 1477, 3084, 2743, 2129, 738, 4117, 2619, 322, 4226, 1336, 4143, 4239, 2016, 4210, 3491, 1459, 1071, 274, 2813, 2480, 4119, 1307, 247, 4049, 2842, 200, 525, 832, 2037, 1585, 1080, 3140, 3082, 1783, 1232, 1978, 3543, 3596, 630, 178, 2824, 4015, 1387, 1519]\n",
            "=====================\n",
            "AUROC 1.0 0.9782608695652174\n",
            "=======================\n",
            "Saved model to disk....\n",
            "[INFO] compiling model...\n",
            "Train on 4518 samples, validate on 46 samples\n",
            "Epoch 1/350\n",
            "4518/4518 [==============================] - 4s 848us/step - loss: 20.9380 - val_loss: 20.9747\n",
            "Epoch 2/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9759\n",
            "Epoch 3/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 20.9379 - val_loss: 20.9757\n",
            "Epoch 4/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 20.9379 - val_loss: 20.9752\n",
            "Epoch 5/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9741\n",
            "Epoch 6/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9740\n",
            "Epoch 7/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9729\n",
            "Epoch 8/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9744\n",
            "Epoch 9/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9737\n",
            "Epoch 10/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9741\n",
            "Epoch 11/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9750\n",
            "Epoch 12/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9745\n",
            "Epoch 13/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9379 - val_loss: 20.9719\n",
            "Epoch 14/350\n",
            "4518/4518 [==============================] - 1s 304us/step - loss: 20.9380 - val_loss: 20.9742\n",
            "Epoch 15/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9379 - val_loss: 20.9672\n",
            "Epoch 16/350\n",
            "4518/4518 [==============================] - 1s 304us/step - loss: 20.9379 - val_loss: 20.9740\n",
            "Epoch 17/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9379 - val_loss: 20.9747\n",
            "Epoch 18/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9755\n",
            "Epoch 19/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9379 - val_loss: 20.9764\n",
            "Epoch 20/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9380 - val_loss: 20.9756\n",
            "Epoch 21/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9761\n",
            "Epoch 22/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9379 - val_loss: 20.9756\n",
            "Epoch 23/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 20.9379 - val_loss: 20.9751\n",
            "Epoch 24/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9380 - val_loss: 20.9757\n",
            "Epoch 25/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9751\n",
            "Epoch 26/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9759\n",
            "Epoch 27/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9758\n",
            "Epoch 28/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9380 - val_loss: 20.9762\n",
            "Epoch 29/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9380 - val_loss: 20.9761\n",
            "Epoch 30/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9763\n",
            "Epoch 31/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9379 - val_loss: 20.9762\n",
            "Epoch 32/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 20.9380 - val_loss: 20.9759\n",
            "Epoch 33/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9760\n",
            "Epoch 34/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9761\n",
            "Epoch 35/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9761\n",
            "Epoch 36/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9379 - val_loss: 20.9766\n",
            "Epoch 37/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9379 - val_loss: 20.9760\n",
            "Epoch 38/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9761\n",
            "Epoch 39/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9380 - val_loss: 20.9759\n",
            "Epoch 40/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9757\n",
            "Epoch 41/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9757\n",
            "Epoch 42/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9759\n",
            "Epoch 43/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9752\n",
            "Epoch 44/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9760\n",
            "Epoch 45/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9759\n",
            "Epoch 46/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9762\n",
            "Epoch 47/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9380 - val_loss: 20.9759\n",
            "Epoch 48/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9380 - val_loss: 20.9759\n",
            "Epoch 49/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9380 - val_loss: 20.9759\n",
            "Epoch 50/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9759\n",
            "Epoch 51/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9760\n",
            "Epoch 52/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9379 - val_loss: 20.9763\n",
            "Epoch 53/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9764\n",
            "Epoch 54/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9765\n",
            "Epoch 55/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9767\n",
            "Epoch 56/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9380 - val_loss: 20.9764\n",
            "Epoch 57/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9764\n",
            "Epoch 58/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9760\n",
            "Epoch 59/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9761\n",
            "Epoch 60/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9759\n",
            "Epoch 61/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9760\n",
            "Epoch 62/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9761\n",
            "Epoch 63/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9763\n",
            "Epoch 64/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9762\n",
            "Epoch 65/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9762\n",
            "Epoch 66/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9764\n",
            "Epoch 67/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9763\n",
            "Epoch 68/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9763\n",
            "Epoch 69/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9759\n",
            "Epoch 70/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9380 - val_loss: 20.9763\n",
            "Epoch 71/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9763\n",
            "Epoch 72/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9761\n",
            "Epoch 73/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9762\n",
            "Epoch 74/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9760\n",
            "Epoch 75/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9380 - val_loss: 20.9763\n",
            "Epoch 76/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9762\n",
            "Epoch 77/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9379 - val_loss: 20.9764\n",
            "Epoch 78/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9760\n",
            "Epoch 79/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9761\n",
            "Epoch 80/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9760\n",
            "Epoch 81/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9766\n",
            "Epoch 82/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9760\n",
            "Epoch 83/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9764\n",
            "Epoch 84/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9380 - val_loss: 20.9766\n",
            "Epoch 85/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9379 - val_loss: 20.9762\n",
            "Epoch 86/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9769\n",
            "Epoch 87/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9762\n",
            "Epoch 88/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 20.9379 - val_loss: 20.9765\n",
            "Epoch 89/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9760\n",
            "Epoch 90/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9761\n",
            "Epoch 91/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9760\n",
            "Epoch 92/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9762\n",
            "Epoch 93/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9759\n",
            "Epoch 94/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9760\n",
            "Epoch 95/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9380 - val_loss: 20.9754\n",
            "Epoch 96/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9745\n",
            "Epoch 97/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9380 - val_loss: 20.9745\n",
            "Epoch 98/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9380 - val_loss: 20.9748\n",
            "Epoch 99/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9758\n",
            "Epoch 100/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9758\n",
            "Epoch 101/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9767\n",
            "Epoch 102/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9747\n",
            "Epoch 103/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9380 - val_loss: 20.9750\n",
            "Epoch 104/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9749\n",
            "Epoch 105/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9379 - val_loss: 20.9759\n",
            "Epoch 106/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9772\n",
            "Epoch 107/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9380 - val_loss: 20.9758\n",
            "Epoch 108/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9380 - val_loss: 20.9753\n",
            "Epoch 109/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9380 - val_loss: 20.9752\n",
            "Epoch 110/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9753\n",
            "Epoch 111/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9750\n",
            "Epoch 112/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9756\n",
            "Epoch 113/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9379 - val_loss: 20.9754\n",
            "Epoch 114/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 20.9379 - val_loss: 20.9755\n",
            "Epoch 115/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9378 - val_loss: 20.9750\n",
            "Epoch 116/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9756\n",
            "Epoch 117/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9754\n",
            "Epoch 118/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9758\n",
            "Epoch 119/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9380 - val_loss: 20.9716\n",
            "Epoch 120/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9725\n",
            "Epoch 121/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9380 - val_loss: 20.9742\n",
            "Epoch 122/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9750\n",
            "Epoch 123/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9749\n",
            "Epoch 124/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9750\n",
            "Epoch 125/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9750\n",
            "Epoch 126/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9757\n",
            "Epoch 127/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9754\n",
            "Epoch 128/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9750\n",
            "Epoch 129/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 20.9379 - val_loss: 20.9749\n",
            "Epoch 130/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9751\n",
            "Epoch 131/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9755\n",
            "Epoch 132/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9754\n",
            "Epoch 133/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9753\n",
            "Epoch 134/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9756\n",
            "Epoch 135/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9758\n",
            "Epoch 136/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9759\n",
            "Epoch 137/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9763\n",
            "Epoch 138/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9755\n",
            "Epoch 139/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9753\n",
            "Epoch 140/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9753\n",
            "Epoch 141/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9379 - val_loss: 20.9765\n",
            "Epoch 142/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9766\n",
            "Epoch 143/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9770\n",
            "Epoch 144/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9769\n",
            "Epoch 145/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9770\n",
            "Epoch 146/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9378 - val_loss: 20.9766\n",
            "Epoch 147/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9768\n",
            "Epoch 148/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9767\n",
            "Epoch 149/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9769\n",
            "Epoch 150/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9758\n",
            "Epoch 151/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9378 - val_loss: 20.9769\n",
            "Epoch 152/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9764\n",
            "Epoch 153/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9378 - val_loss: 20.9768\n",
            "Epoch 154/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9766\n",
            "Epoch 155/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 20.9379 - val_loss: 20.9766\n",
            "Epoch 156/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9378 - val_loss: 20.9766\n",
            "Epoch 157/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9378 - val_loss: 20.9767\n",
            "Epoch 158/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9752\n",
            "Epoch 159/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9773\n",
            "Epoch 160/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9769\n",
            "Epoch 161/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9771\n",
            "Epoch 162/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9772\n",
            "Epoch 163/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9765\n",
            "Epoch 164/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9379 - val_loss: 20.9772\n",
            "Epoch 165/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9772\n",
            "Epoch 166/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9771\n",
            "Epoch 167/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9769\n",
            "Epoch 168/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9767\n",
            "Epoch 169/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9772\n",
            "Epoch 170/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9767\n",
            "Epoch 171/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9769\n",
            "Epoch 172/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 20.9379 - val_loss: 20.9769\n",
            "Epoch 173/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9767\n",
            "Epoch 174/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 20.9379 - val_loss: 20.9766\n",
            "Epoch 175/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9766\n",
            "Epoch 176/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9767\n",
            "Epoch 177/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 20.9379 - val_loss: 20.9771\n",
            "Epoch 178/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 20.9378 - val_loss: 20.9771\n",
            "Epoch 179/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9378 - val_loss: 20.9768\n",
            "Epoch 180/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9770\n",
            "Epoch 181/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9765\n",
            "Epoch 182/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9769\n",
            "Epoch 183/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9762\n",
            "Epoch 184/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9769\n",
            "Epoch 185/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9775\n",
            "Epoch 186/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9772\n",
            "Epoch 187/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9764\n",
            "Epoch 188/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9771\n",
            "Epoch 189/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9769\n",
            "Epoch 190/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9772\n",
            "Epoch 191/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9794\n",
            "Epoch 192/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 20.9379 - val_loss: 20.9771\n",
            "Epoch 193/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9772\n",
            "Epoch 194/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9773\n",
            "Epoch 195/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9772\n",
            "Epoch 196/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9378 - val_loss: 20.9778\n",
            "Epoch 197/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9771\n",
            "Epoch 198/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9771\n",
            "Epoch 199/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9776\n",
            "Epoch 200/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9771\n",
            "Epoch 201/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9379 - val_loss: 20.9768\n",
            "Epoch 202/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9764\n",
            "Epoch 203/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 20.9378 - val_loss: 20.9761\n",
            "Epoch 204/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9765\n",
            "Epoch 205/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9378 - val_loss: 20.9770\n",
            "Epoch 206/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9769\n",
            "Epoch 207/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9772\n",
            "Epoch 208/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9762\n",
            "Epoch 209/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9769\n",
            "Epoch 210/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9764\n",
            "Epoch 211/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9781\n",
            "Epoch 212/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9379 - val_loss: 20.9766\n",
            "Epoch 213/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9770\n",
            "Epoch 214/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9778\n",
            "Epoch 215/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 20.9378 - val_loss: 20.9769\n",
            "Epoch 216/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9769\n",
            "Epoch 217/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9770\n",
            "Epoch 218/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9770\n",
            "Epoch 219/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 20.9379 - val_loss: 20.9773\n",
            "Epoch 220/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9776\n",
            "Epoch 221/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9379 - val_loss: 20.9775\n",
            "Epoch 222/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9770\n",
            "Epoch 223/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9768\n",
            "Epoch 224/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9379 - val_loss: 20.9795\n",
            "Epoch 225/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9767\n",
            "Epoch 226/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9774\n",
            "Epoch 227/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9769\n",
            "Epoch 228/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9761\n",
            "Epoch 229/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9769\n",
            "Epoch 230/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9767\n",
            "Epoch 231/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9378 - val_loss: 20.9770\n",
            "Epoch 232/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9770\n",
            "Epoch 233/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9769\n",
            "Epoch 234/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9790\n",
            "Epoch 235/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9378 - val_loss: 20.9767\n",
            "Epoch 236/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9378 - val_loss: 20.9791\n",
            "Epoch 237/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 20.9379 - val_loss: 20.9768\n",
            "Epoch 238/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 20.9378 - val_loss: 20.9774\n",
            "Epoch 239/350\n",
            "4518/4518 [==============================] - 1s 303us/step - loss: 20.9379 - val_loss: 20.9770\n",
            "Epoch 240/350\n",
            "4518/4518 [==============================] - 1s 304us/step - loss: 20.9379 - val_loss: 20.9774\n",
            "Epoch 241/350\n",
            "4518/4518 [==============================] - 1s 302us/step - loss: 20.9379 - val_loss: 20.9762\n",
            "Epoch 242/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 20.9379 - val_loss: 20.9771\n",
            "Epoch 243/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 20.9378 - val_loss: 20.9767\n",
            "Epoch 244/350\n",
            "4518/4518 [==============================] - 1s 302us/step - loss: 20.9379 - val_loss: 20.9786\n",
            "Epoch 245/350\n",
            "4518/4518 [==============================] - 1s 304us/step - loss: 20.9379 - val_loss: 20.9766\n",
            "Epoch 246/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9378 - val_loss: 20.9770\n",
            "Epoch 247/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9772\n",
            "Epoch 248/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9770\n",
            "Epoch 249/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9770\n",
            "Epoch 250/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9772\n",
            "Epoch 251/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9768\n",
            "Epoch 252/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9771\n",
            "Epoch 253/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9791\n",
            "Epoch 254/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9781\n",
            "Epoch 255/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9378 - val_loss: 20.9775\n",
            "Epoch 256/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9773\n",
            "Epoch 257/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9775\n",
            "Epoch 258/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9378 - val_loss: 20.9775\n",
            "Epoch 259/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9772\n",
            "Epoch 260/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9771\n",
            "Epoch 261/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9771\n",
            "Epoch 262/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9788\n",
            "Epoch 263/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9768\n",
            "Epoch 264/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9776\n",
            "Epoch 265/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9786\n",
            "Epoch 266/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9794\n",
            "Epoch 267/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9378 - val_loss: 20.9792\n",
            "Epoch 268/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9792\n",
            "Epoch 269/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9792\n",
            "Epoch 270/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 20.9378 - val_loss: 20.9809\n",
            "Epoch 271/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 20.9378 - val_loss: 20.9790\n",
            "Epoch 272/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9772\n",
            "Epoch 273/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9379 - val_loss: 20.9810\n",
            "Epoch 274/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 20.9378 - val_loss: 20.9821\n",
            "Epoch 275/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9774\n",
            "Epoch 276/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9773\n",
            "Epoch 277/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9769\n",
            "Epoch 278/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 20.9379 - val_loss: 20.9771\n",
            "Epoch 279/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9768\n",
            "Epoch 280/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9771\n",
            "Epoch 281/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9378 - val_loss: 20.9769\n",
            "Epoch 282/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 20.9378 - val_loss: 20.9771\n",
            "Epoch 283/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9769\n",
            "Epoch 284/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9378 - val_loss: 20.9770\n",
            "Epoch 285/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9771\n",
            "Epoch 286/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9771\n",
            "Epoch 287/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9774\n",
            "Epoch 288/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9774\n",
            "Epoch 289/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9771\n",
            "Epoch 290/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9378 - val_loss: 20.9768\n",
            "Epoch 291/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9769\n",
            "Epoch 292/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9767\n",
            "Epoch 293/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9773\n",
            "Epoch 294/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9771\n",
            "Epoch 295/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9770\n",
            "Epoch 296/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 20.9378 - val_loss: 20.9790\n",
            "Epoch 297/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9773\n",
            "Epoch 298/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9775\n",
            "Epoch 299/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9771\n",
            "Epoch 300/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9378 - val_loss: 20.9796\n",
            "Epoch 301/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9768\n",
            "Epoch 302/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9379 - val_loss: 20.9770\n",
            "Epoch 303/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 20.9378 - val_loss: 20.9773\n",
            "Epoch 304/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9773\n",
            "Epoch 305/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9773\n",
            "Epoch 306/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9774\n",
            "Epoch 307/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9771\n",
            "Epoch 308/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9773\n",
            "Epoch 309/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9757\n",
            "Epoch 310/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9768\n",
            "Epoch 311/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9801\n",
            "Epoch 312/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9781\n",
            "Epoch 313/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9769\n",
            "Epoch 314/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9379 - val_loss: 20.9806\n",
            "Epoch 315/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9768\n",
            "Epoch 316/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9798\n",
            "Epoch 317/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9379 - val_loss: 20.9791\n",
            "Epoch 318/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9774\n",
            "Epoch 319/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9794\n",
            "Epoch 320/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9800\n",
            "Epoch 321/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9771\n",
            "Epoch 322/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9794\n",
            "Epoch 323/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9379 - val_loss: 20.9792\n",
            "Epoch 324/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9791\n",
            "Epoch 325/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9807\n",
            "Epoch 326/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9801\n",
            "Epoch 327/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9807\n",
            "Epoch 328/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9799\n",
            "Epoch 329/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9799\n",
            "Epoch 330/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9379 - val_loss: 20.9810\n",
            "Epoch 331/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 20.9378 - val_loss: 20.9808\n",
            "Epoch 332/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9791\n",
            "Epoch 333/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9818\n",
            "Epoch 334/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9811\n",
            "Epoch 335/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9808\n",
            "Epoch 336/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9808\n",
            "Epoch 337/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9788\n",
            "Epoch 338/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9791\n",
            "Epoch 339/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9772\n",
            "Epoch 340/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9773\n",
            "Epoch 341/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9796\n",
            "Epoch 342/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9797\n",
            "Epoch 343/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9800\n",
            "Epoch 344/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 20.9378 - val_loss: 20.9815\n",
            "Epoch 345/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9813\n",
            "Epoch 346/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 20.9378 - val_loss: 20.9800\n",
            "Epoch 347/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9794\n",
            "Epoch 348/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 20.9378 - val_loss: 20.9778\n",
            "Epoch 349/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 20.9378 - val_loss: 20.9776\n",
            "Epoch 350/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 20.9378 - val_loss: 20.9778\n",
            "(lamda,Threshold) 10.0 5.0\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (4564, 784) 784\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 0 10.0\n",
            "The shape of N (4564, 784)\n",
            "The minimum value of N  0.0\n",
            "The max value of N 0.0\n",
            "[INFO:] Xclean  MSE Computed shape (4564, 784)\n",
            "[INFO:]Xdecoded  Computed shape (4564, 784)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0022015534, 0.0016917054, 0.0014983624, 0.0014669293])\n",
            "[INFO:] The anomaly threshold computed is  0.0014669293\n",
            "side: 28\n",
            "channel: 1\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [4547, 4563, 4532, 4539, 4550, 4559, 4522, 4524, 4526, 4518, 4536, 4561, 4542, 4521, 4523, 4551, 4533, 4562, 4525, 4541, 4529, 4520, 4554, 4545, 4538, 4535, 4548, 4530, 4558, 4549, 4527, 4531, 4519, 4552, 2254, 4537, 4553, 4528, 4540, 4557, 4556, 4543, 4546, 4555, 1906, 4534]\n",
            "[INFO:] The worstreconstructed_Top200index index are  [4547, 4563, 4532, 4539, 4550, 4559, 4522, 4524, 4526, 4518, 4536, 4561, 4542, 4521, 4523, 4551, 4533, 4562, 4525, 4541, 4529, 4520, 4554, 4545, 4538, 4535, 4548, 4530, 4558, 4549, 4527, 4531, 4519, 4552, 2254, 4537, 4553, 4528, 4540, 4557, 4556, 4543, 4546, 4555, 1906, 4534, 2758, 4544, 4560, 1449, 3060, 277, 1947, 1761, 449, 3527, 4213, 569, 591, 2593, 3068, 3229, 2172, 89, 3844, 2179, 470, 1725, 2157, 826, 3723, 7, 2086, 2832, 2790, 1452, 478, 3302, 3034, 2330, 4339, 4368, 3855, 803, 657, 1373, 3553, 4143, 3817, 2945, 4423, 1953, 3381, 1071, 2237, 2037, 433, 2390, 602, 3828, 422, 3191, 633, 1368, 185, 2764, 2484, 318, 4350, 3674, 3704, 2951, 1144, 3494, 1250, 3996, 2887, 1855, 2359, 2391, 814, 1193, 3873, 57, 1309, 1459, 4015, 1760, 3419, 263, 3992, 691, 1557, 4283, 2684, 4315, 3149, 4483, 2232, 3472, 221, 1065, 322, 3874, 1387, 64, 1849, 3054, 412, 2480, 4027, 2619, 4162, 3433, 4067, 1055, 1091, 525, 237, 388, 3491, 3769, 72, 1503, 247, 4394, 1227, 3307, 1080, 2408, 2842, 840, 1510, 2284, 223, 1678, 4221, 178, 1744, 1605, 1041, 2856, 4239, 3291, 3613, 3247, 4226, 359, 706, 3623, 3901, 2019, 1777, 3912, 2422, 1001, 200, 3125, 544, 4345]\n",
            "=====================\n",
            "AUROC 10.0 0.9782608695652174\n",
            "=======================\n",
            "Saved model to disk....\n",
            "[INFO] compiling model...\n",
            "Train on 4518 samples, validate on 46 samples\n",
            "Epoch 1/350\n",
            "4518/4518 [==============================] - 4s 910us/step - loss: 15.8570 - val_loss: 15.8968\n",
            "Epoch 2/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.8970\n",
            "Epoch 3/350\n",
            "4518/4518 [==============================] - 1s 318us/step - loss: 15.8570 - val_loss: 15.8987\n",
            "Epoch 4/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.8969\n",
            "Epoch 5/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.8991\n",
            "Epoch 6/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.8992\n",
            "Epoch 7/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.8991\n",
            "Epoch 8/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.8984\n",
            "Epoch 9/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.8991\n",
            "Epoch 10/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9000\n",
            "Epoch 11/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.8993\n",
            "Epoch 12/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.8989\n",
            "Epoch 13/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.8989\n",
            "Epoch 14/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 15.8568 - val_loss: 15.9003\n",
            "Epoch 15/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.8986\n",
            "Epoch 16/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.8980\n",
            "Epoch 17/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.8990\n",
            "Epoch 18/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.8987\n",
            "Epoch 19/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8568 - val_loss: 15.8986\n",
            "Epoch 20/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9005\n",
            "Epoch 21/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.8988\n",
            "Epoch 22/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.8989\n",
            "Epoch 23/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8570 - val_loss: 15.9002\n",
            "Epoch 24/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.8992\n",
            "Epoch 25/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.8998\n",
            "Epoch 26/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9005\n",
            "Epoch 27/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.8989\n",
            "Epoch 28/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.8973\n",
            "Epoch 29/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.8988\n",
            "Epoch 30/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.8988\n",
            "Epoch 31/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.8982\n",
            "Epoch 32/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.8985\n",
            "Epoch 33/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.8990\n",
            "Epoch 34/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.8981\n",
            "Epoch 35/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.8988\n",
            "Epoch 36/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.8970\n",
            "Epoch 37/350\n",
            "4518/4518 [==============================] - 1s 318us/step - loss: 15.8569 - val_loss: 15.8973\n",
            "Epoch 38/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9002\n",
            "Epoch 39/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.8969\n",
            "Epoch 40/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8570 - val_loss: 15.9002\n",
            "Epoch 41/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.8999\n",
            "Epoch 42/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.8997\n",
            "Epoch 43/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9003\n",
            "Epoch 44/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8568 - val_loss: 15.9001\n",
            "Epoch 45/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 15.8569 - val_loss: 15.8990\n",
            "Epoch 46/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 15.8569 - val_loss: 15.9010\n",
            "Epoch 47/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.8996\n",
            "Epoch 48/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.8989\n",
            "Epoch 49/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.8989\n",
            "Epoch 50/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.8969\n",
            "Epoch 51/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.8971\n",
            "Epoch 52/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.8986\n",
            "Epoch 53/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.8979\n",
            "Epoch 54/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.8993\n",
            "Epoch 55/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.8988\n",
            "Epoch 56/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.8996\n",
            "Epoch 57/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 15.8569 - val_loss: 15.9003\n",
            "Epoch 58/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9006\n",
            "Epoch 59/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9004\n",
            "Epoch 60/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8568 - val_loss: 15.9008\n",
            "Epoch 61/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 15.8569 - val_loss: 15.9005\n",
            "Epoch 62/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9036\n",
            "Epoch 63/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8570 - val_loss: 15.8984\n",
            "Epoch 64/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9050\n",
            "Epoch 65/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9053\n",
            "Epoch 66/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8570 - val_loss: 15.9055\n",
            "Epoch 67/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9062\n",
            "Epoch 68/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9053\n",
            "Epoch 69/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9054\n",
            "Epoch 70/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8570 - val_loss: 15.9053\n",
            "Epoch 71/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9059\n",
            "Epoch 72/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9071\n",
            "Epoch 73/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9061\n",
            "Epoch 74/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9062\n",
            "Epoch 75/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 15.8569 - val_loss: 15.9061\n",
            "Epoch 76/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 15.8569 - val_loss: 15.9064\n",
            "Epoch 77/350\n",
            "4518/4518 [==============================] - 1s 318us/step - loss: 15.8569 - val_loss: 15.9060\n",
            "Epoch 78/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9059\n",
            "Epoch 79/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8570 - val_loss: 15.9058\n",
            "Epoch 80/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9063\n",
            "Epoch 81/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9042\n",
            "Epoch 82/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 15.8569 - val_loss: 15.9059\n",
            "Epoch 83/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 15.8569 - val_loss: 15.9055\n",
            "Epoch 84/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9044\n",
            "Epoch 85/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9058\n",
            "Epoch 86/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9053\n",
            "Epoch 87/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9058\n",
            "Epoch 88/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9066\n",
            "Epoch 89/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9064\n",
            "Epoch 90/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9062\n",
            "Epoch 91/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9062\n",
            "Epoch 92/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9064\n",
            "Epoch 93/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9059\n",
            "Epoch 94/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9064\n",
            "Epoch 95/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9064\n",
            "Epoch 96/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8570 - val_loss: 15.9064\n",
            "Epoch 97/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9068\n",
            "Epoch 98/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9072\n",
            "Epoch 99/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9074\n",
            "Epoch 100/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 15.8569 - val_loss: 15.9069\n",
            "Epoch 101/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8570 - val_loss: 15.9079\n",
            "Epoch 102/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9066\n",
            "Epoch 103/350\n",
            "4518/4518 [==============================] - 1s 304us/step - loss: 15.8569 - val_loss: 15.9060\n",
            "Epoch 104/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 15.8569 - val_loss: 15.9063\n",
            "Epoch 105/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9060\n",
            "Epoch 106/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 15.8569 - val_loss: 15.9059\n",
            "Epoch 107/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 15.8569 - val_loss: 15.9078\n",
            "Epoch 108/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9074\n",
            "Epoch 109/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 15.8569 - val_loss: 15.9084\n",
            "Epoch 110/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9071\n",
            "Epoch 111/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9075\n",
            "Epoch 112/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9074\n",
            "Epoch 113/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8568 - val_loss: 15.9079\n",
            "Epoch 114/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9091\n",
            "Epoch 115/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9077\n",
            "Epoch 116/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8568 - val_loss: 15.9090\n",
            "Epoch 117/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9076\n",
            "Epoch 118/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8568 - val_loss: 15.9085\n",
            "Epoch 119/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9074\n",
            "Epoch 120/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9090\n",
            "Epoch 121/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9075\n",
            "Epoch 122/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 15.8569 - val_loss: 15.9067\n",
            "Epoch 123/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9097\n",
            "Epoch 124/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9061\n",
            "Epoch 125/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9055\n",
            "Epoch 126/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9060\n",
            "Epoch 127/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8571 - val_loss: 15.8911\n",
            "Epoch 128/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8570 - val_loss: 15.8926\n",
            "Epoch 129/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8570 - val_loss: 15.8962\n",
            "Epoch 130/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 15.8569 - val_loss: 15.8991\n",
            "Epoch 131/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.8995\n",
            "Epoch 132/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9003\n",
            "Epoch 133/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9000\n",
            "Epoch 134/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9014\n",
            "Epoch 135/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9024\n",
            "Epoch 136/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9014\n",
            "Epoch 137/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9019\n",
            "Epoch 138/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 15.8569 - val_loss: 15.9027\n",
            "Epoch 139/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9026\n",
            "Epoch 140/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9031\n",
            "Epoch 141/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9022\n",
            "Epoch 142/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9024\n",
            "Epoch 143/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9025\n",
            "Epoch 144/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9032\n",
            "Epoch 145/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9033\n",
            "Epoch 146/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9034\n",
            "Epoch 147/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9048\n",
            "Epoch 148/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9050\n",
            "Epoch 149/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9051\n",
            "Epoch 150/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9046\n",
            "Epoch 151/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9047\n",
            "Epoch 152/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8570 - val_loss: 15.9057\n",
            "Epoch 153/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9042\n",
            "Epoch 154/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9045\n",
            "Epoch 155/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9038\n",
            "Epoch 156/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9036\n",
            "Epoch 157/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9063\n",
            "Epoch 158/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9047\n",
            "Epoch 159/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9046\n",
            "Epoch 160/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9017\n",
            "Epoch 161/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9048\n",
            "Epoch 162/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9048\n",
            "Epoch 163/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8568 - val_loss: 15.9056\n",
            "Epoch 164/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9041\n",
            "Epoch 165/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9047\n",
            "Epoch 166/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9040\n",
            "Epoch 167/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9052\n",
            "Epoch 168/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8568 - val_loss: 15.9050\n",
            "Epoch 169/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8568 - val_loss: 15.9064\n",
            "Epoch 170/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9062\n",
            "Epoch 171/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9059\n",
            "Epoch 172/350\n",
            "4518/4518 [==============================] - 1s 315us/step - loss: 15.8569 - val_loss: 15.9048\n",
            "Epoch 173/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9050\n",
            "Epoch 174/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8570 - val_loss: 15.9037\n",
            "Epoch 175/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9043\n",
            "Epoch 176/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9036\n",
            "Epoch 177/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9043\n",
            "Epoch 178/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9048\n",
            "Epoch 179/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9051\n",
            "Epoch 180/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9048\n",
            "Epoch 181/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8568 - val_loss: 15.9050\n",
            "Epoch 182/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9052\n",
            "Epoch 183/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9055\n",
            "Epoch 184/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9049\n",
            "Epoch 185/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9045\n",
            "Epoch 186/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9056\n",
            "Epoch 187/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9057\n",
            "Epoch 188/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9061\n",
            "Epoch 189/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9058\n",
            "Epoch 190/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9056\n",
            "Epoch 191/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9053\n",
            "Epoch 192/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9061\n",
            "Epoch 193/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9058\n",
            "Epoch 194/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9064\n",
            "Epoch 195/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9064\n",
            "Epoch 196/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9063\n",
            "Epoch 197/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9073\n",
            "Epoch 198/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9068\n",
            "Epoch 199/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9068\n",
            "Epoch 200/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9061\n",
            "Epoch 201/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9068\n",
            "Epoch 202/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9066\n",
            "Epoch 203/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9070\n",
            "Epoch 204/350\n",
            "4518/4518 [==============================] - 1s 317us/step - loss: 15.8569 - val_loss: 15.9068\n",
            "Epoch 205/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9063\n",
            "Epoch 206/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9060\n",
            "Epoch 207/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8568 - val_loss: 15.9063\n",
            "Epoch 208/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8568 - val_loss: 15.9061\n",
            "Epoch 209/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9065\n",
            "Epoch 210/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9066\n",
            "Epoch 211/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9070\n",
            "Epoch 212/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9059\n",
            "Epoch 213/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9056\n",
            "Epoch 214/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9064\n",
            "Epoch 215/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9064\n",
            "Epoch 216/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9015\n",
            "Epoch 217/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9050\n",
            "Epoch 218/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8568 - val_loss: 15.9054\n",
            "Epoch 219/350\n",
            "4518/4518 [==============================] - 1s 318us/step - loss: 15.8568 - val_loss: 15.9058\n",
            "Epoch 220/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9063\n",
            "Epoch 221/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9049\n",
            "Epoch 222/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9056\n",
            "Epoch 223/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9055\n",
            "Epoch 224/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9051\n",
            "Epoch 225/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9058\n",
            "Epoch 226/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9057\n",
            "Epoch 227/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9062\n",
            "Epoch 228/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8568 - val_loss: 15.9061\n",
            "Epoch 229/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9054\n",
            "Epoch 230/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9067\n",
            "Epoch 231/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9069\n",
            "Epoch 232/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9062\n",
            "Epoch 233/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9066\n",
            "Epoch 234/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8569 - val_loss: 15.9066\n",
            "Epoch 235/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9064\n",
            "Epoch 236/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9071\n",
            "Epoch 237/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9062\n",
            "Epoch 238/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9059\n",
            "Epoch 239/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9085\n",
            "Epoch 240/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9063\n",
            "Epoch 241/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9088\n",
            "Epoch 242/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9063\n",
            "Epoch 243/350\n",
            "4518/4518 [==============================] - 1s 318us/step - loss: 15.8569 - val_loss: 15.9063\n",
            "Epoch 244/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9059\n",
            "Epoch 245/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9069\n",
            "Epoch 246/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 15.8568 - val_loss: 15.9070\n",
            "Epoch 247/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9079\n",
            "Epoch 248/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9087\n",
            "Epoch 249/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8568 - val_loss: 15.9085\n",
            "Epoch 250/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9069\n",
            "Epoch 251/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9073\n",
            "Epoch 252/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8568 - val_loss: 15.9083\n",
            "Epoch 253/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9067\n",
            "Epoch 254/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9072\n",
            "Epoch 255/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9080\n",
            "Epoch 256/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9075\n",
            "Epoch 257/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9079\n",
            "Epoch 258/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9074\n",
            "Epoch 259/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9069\n",
            "Epoch 260/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9072\n",
            "Epoch 261/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9076\n",
            "Epoch 262/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9076\n",
            "Epoch 263/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9084\n",
            "Epoch 264/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9073\n",
            "Epoch 265/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9081\n",
            "Epoch 266/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8568 - val_loss: 15.9072\n",
            "Epoch 267/350\n",
            "4518/4518 [==============================] - 1s 316us/step - loss: 15.8569 - val_loss: 15.9071\n",
            "Epoch 268/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9085\n",
            "Epoch 269/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9086\n",
            "Epoch 270/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 15.8568 - val_loss: 15.9072\n",
            "Epoch 271/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9072\n",
            "Epoch 272/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9088\n",
            "Epoch 273/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9079\n",
            "Epoch 274/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9081\n",
            "Epoch 275/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9089\n",
            "Epoch 276/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9088\n",
            "Epoch 277/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9078\n",
            "Epoch 278/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 15.8569 - val_loss: 15.9088\n",
            "Epoch 279/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8568 - val_loss: 15.9078\n",
            "Epoch 280/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9086\n",
            "Epoch 281/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9073\n",
            "Epoch 282/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9037\n",
            "Epoch 283/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8569 - val_loss: 15.9038\n",
            "Epoch 284/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9036\n",
            "Epoch 285/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8568 - val_loss: 15.9052\n",
            "Epoch 286/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 15.8568 - val_loss: 15.9081\n",
            "Epoch 287/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 15.8569 - val_loss: 15.9021\n",
            "Epoch 288/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9030\n",
            "Epoch 289/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9038\n",
            "Epoch 290/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9065\n",
            "Epoch 291/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9035\n",
            "Epoch 292/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9044\n",
            "Epoch 293/350\n",
            "4518/4518 [==============================] - 1s 319us/step - loss: 15.8569 - val_loss: 15.9059\n",
            "Epoch 294/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9027\n",
            "Epoch 295/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9037\n",
            "Epoch 296/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8568 - val_loss: 15.9039\n",
            "Epoch 297/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9035\n",
            "Epoch 298/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 15.8568 - val_loss: 15.9034\n",
            "Epoch 299/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8568 - val_loss: 15.9074\n",
            "Epoch 300/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9064\n",
            "Epoch 301/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9066\n",
            "Epoch 302/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9067\n",
            "Epoch 303/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9086\n",
            "Epoch 304/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9064\n",
            "Epoch 305/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8568 - val_loss: 15.9055\n",
            "Epoch 306/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8568 - val_loss: 15.9069\n",
            "Epoch 307/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9067\n",
            "Epoch 308/350\n",
            "4518/4518 [==============================] - 1s 314us/step - loss: 15.8568 - val_loss: 15.9071\n",
            "Epoch 309/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8568 - val_loss: 15.9059\n",
            "Epoch 310/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9075\n",
            "Epoch 311/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9064\n",
            "Epoch 312/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 15.8569 - val_loss: 15.9070\n",
            "Epoch 313/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9069\n",
            "Epoch 314/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9073\n",
            "Epoch 315/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8569 - val_loss: 15.9067\n",
            "Epoch 316/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9067\n",
            "Epoch 317/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9063\n",
            "Epoch 318/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9075\n",
            "Epoch 319/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9064\n",
            "Epoch 320/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9061\n",
            "Epoch 321/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9048\n",
            "Epoch 322/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 15.8569 - val_loss: 15.9053\n",
            "Epoch 323/350\n",
            "4518/4518 [==============================] - 1s 304us/step - loss: 15.8568 - val_loss: 15.9060\n",
            "Epoch 324/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 15.8568 - val_loss: 15.9066\n",
            "Epoch 325/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8569 - val_loss: 15.9066\n",
            "Epoch 326/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 15.8569 - val_loss: 15.9064\n",
            "Epoch 327/350\n",
            "4518/4518 [==============================] - 1s 303us/step - loss: 15.8568 - val_loss: 15.9038\n",
            "Epoch 328/350\n",
            "4518/4518 [==============================] - 1s 306us/step - loss: 15.8569 - val_loss: 15.9054\n",
            "Epoch 329/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9065\n",
            "Epoch 330/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 15.8569 - val_loss: 15.9074\n",
            "Epoch 331/350\n",
            "4518/4518 [==============================] - 1s 305us/step - loss: 15.8569 - val_loss: 15.9069\n",
            "Epoch 332/350\n",
            "4518/4518 [==============================] - 1s 308us/step - loss: 15.8568 - val_loss: 15.9061\n",
            "Epoch 333/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8568 - val_loss: 15.9060\n",
            "Epoch 334/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8569 - val_loss: 15.9052\n",
            "Epoch 335/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9067\n",
            "Epoch 336/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9065\n",
            "Epoch 337/350\n",
            "4518/4518 [==============================] - 1s 307us/step - loss: 15.8568 - val_loss: 15.9065\n",
            "Epoch 338/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8568 - val_loss: 15.9070\n",
            "Epoch 339/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9068\n",
            "Epoch 340/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9064\n",
            "Epoch 341/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9066\n",
            "Epoch 342/350\n",
            "4518/4518 [==============================] - 1s 311us/step - loss: 15.8568 - val_loss: 15.9061\n",
            "Epoch 343/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8569 - val_loss: 15.9055\n",
            "Epoch 344/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8568 - val_loss: 15.9059\n",
            "Epoch 345/350\n",
            "4518/4518 [==============================] - 1s 313us/step - loss: 15.8568 - val_loss: 15.9055\n",
            "Epoch 346/350\n",
            "4518/4518 [==============================] - 1s 310us/step - loss: 15.8568 - val_loss: 15.9052\n",
            "Epoch 347/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8568 - val_loss: 15.9059\n",
            "Epoch 348/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8569 - val_loss: 15.9073\n",
            "Epoch 349/350\n",
            "4518/4518 [==============================] - 1s 309us/step - loss: 15.8568 - val_loss: 15.9070\n",
            "Epoch 350/350\n",
            "4518/4518 [==============================] - 1s 312us/step - loss: 15.8568 - val_loss: 15.9072\n",
            "(lamda,Threshold) 100.0 50.0\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (4564, 784) 784\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 0 100.0\n",
            "The shape of N (4564, 784)\n",
            "The minimum value of N  0.0\n",
            "The max value of N 0.0\n",
            "[INFO:] Xclean  MSE Computed shape (4564, 784)\n",
            "[INFO:]Xdecoded  Computed shape (4564, 784)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0022015534, 0.0016917054, 0.0014983624, 0.0014669293, 0.0014666198])\n",
            "[INFO:] The anomaly threshold computed is  0.0014666198\n",
            "side: 28\n",
            "channel: 1\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [4547, 4532, 4563, 4539, 4550, 4524, 4522, 4526, 4559, 4536, 4518, 4523, 4561, 4542, 4533, 4521, 4551, 4562, 4558, 4541, 4545, 4525, 4529, 4520, 4554, 4535, 4538, 4530, 4527, 4549, 4531, 4519, 4548, 4552, 4537, 4528, 4553, 4557, 4555, 1590, 4540, 4546, 4556, 4543, 4544, 4560]\n",
            "[INFO:] The worstreconstructed_Top200index index are  [4547, 4532, 4563, 4539, 4550, 4524, 4522, 4526, 4559, 4536, 4518, 4523, 4561, 4542, 4533, 4521, 4551, 4562, 4558, 4541, 4545, 4525, 4529, 4520, 4554, 4535, 4538, 4530, 4527, 4549, 4531, 4519, 4548, 4552, 4537, 4528, 4553, 4557, 4555, 1590, 4540, 4546, 4556, 4543, 4544, 4560, 1906, 4534, 3302, 277, 927, 4484, 1449, 4213, 449, 569, 3060, 1947, 2593, 3527, 3068, 2157, 591, 2945, 89, 2172, 2179, 478, 826, 1452, 2086, 1373, 3229, 3855, 237, 3996, 2832, 3553, 4339, 7, 422, 4368, 803, 2790, 3034, 4423, 602, 1761, 3191, 263, 2330, 3674, 3759, 2484, 657, 57, 3817, 2237, 4283, 3723, 2764, 691, 470, 1193, 247, 4075, 1953, 185, 2887, 3844, 4350, 1855, 3149, 2951, 3381, 1760, 3935, 322, 3077, 4015, 4345, 2842, 3419, 433, 2422, 3054, 1557, 2684, 901, 64, 2821, 3125, 2390, 2480, 4239, 3873, 4049, 3376, 2408, 633, 3828, 525, 4157, 318, 2232, 4444, 1041, 3538, 814, 2619, 1250, 1368, 2813, 1392, 875, 1441, 3566, 3107, 2190, 388, 3704, 12, 3433, 1459, 2359, 1144, 3494, 3491, 764, 1849, 1091, 158, 412, 1856, 221, 223, 3893, 2307, 3247, 200, 1065, 4394, 1678, 809, 178, 3215, 4232, 199, 3357, 726, 1503, 3236, 1585, 776, 2776, 1387, 1556, 3912, 3623, 544]\n",
            "=====================\n",
            "AUROC 100.0 0.9891304347826088\n",
            "=======================\n",
            "Saved model to disk....\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([])\n",
            "========================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEVCAYAAAAM3jVmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbtiP074AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f65a0e79e80>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "M45sLOg-bdZz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Obtain Training and Test Datasets"
      ]
    },
    {
      "metadata": {
        "id": "gq0nPEffbdZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1997
        },
        "outputId": "ff8d11f3-5d28-437e-a339-1b6fe3e299cf"
      },
      "cell_type": "code",
      "source": [
        "## Obtaining the training and testing data\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from src.models.RCAE import RCAE_AD\n",
        "\n",
        "DATASET = \"mnist\"\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_CHANNEL=1\n",
        "HIDDEN_LAYER_SIZE= 32\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n",
        "PRETRAINED_WT_PATH = \"\"\n",
        "\n",
        "rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n",
        "\n",
        "print(\"Train Data Shape: \",rcae.data._X_train.shape)\n",
        "print(\"Train Label Shape: \",rcae.data._y_train.shape)\n",
        "print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n",
        "print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n",
        "\n",
        "\n",
        "print(\"Test Data Shape: \",rcae.data._X_test.shape)\n",
        "print(\"Test Label Shape: \",rcae.data._y_test.shape)\n",
        "print(\"===========TRAINING AND PREDICTING WITH RCAE============================\")\n",
        "rcae.fit_and_predict()\n",
        "print(\"========================================================================\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[autoreload of src.models.RCAE failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "]\n",
            "[autoreload of src.models.RCAE failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n",
            "Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE/\n",
            "[INFO: ] Loading data...\n",
            "[INFO:] THe shape of X is  (60000, 28, 28, 1)\n",
            "[INFO:] THe shape of y is  (60000,)\n",
            "[INFO] : The idx_normal is:  [ True False False ...  True False False]\n",
            "[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n",
            "[INFO] : The shape of X is:  (60000, 28, 28, 1)\n",
            "[INFO] : The shape of y is:  (60000,)\n",
            "[INFO:] THe shape of X is  (10000, 1, 28, 28)\n",
            "[INFO:] THe shape of y is  (10000,)\n",
            "[INFO] : The idx_normal is:  [False False False ... False  True False]\n",
            "[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n",
            "[INFO] : The shape of X is:  (10000, 1, 28, 28)\n",
            "[INFO] : The shape of y is:  (10000,)\n",
            "[INFO: ] Data loaded.\n",
            "[INFO:] Assertions of memory muted\n",
            "Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE/\n",
            "[INFO: ] Loading data...\n",
            "[INFO:] THe shape of X is  (60000, 28, 28, 1)\n",
            "[INFO:] THe shape of y is  (60000,)\n",
            "[INFO] : The idx_normal is:  [ True False False ...  True False False]\n",
            "[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n",
            "[INFO] : The shape of X is:  (60000, 28, 28, 1)\n",
            "[INFO] : The shape of y is:  (60000,)\n",
            "[INFO:] THe shape of X is  (10000, 1, 28, 28)\n",
            "[INFO:] THe shape of y is  (10000,)\n",
            "[INFO] : The idx_normal is:  [False False False ... False  True False]\n",
            "[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n",
            "[INFO] : The shape of X is:  (10000, 1, 28, 28)\n",
            "[INFO] : The shape of y is:  (10000,)\n",
            "[INFO: ] Data loaded.\n",
            "Train Data Shape:  (4564, 28, 28, 1)\n",
            "Train Label Shape:  (4564,)\n",
            "Validation Data Shape:  (912, 28, 28, 1)\n",
            "Validation Label Shape:  (912,)\n",
            "Test Data Shape:  (10000, 1, 28, 28)\n",
            "Test Label Shape:  (10000,)\n",
            "===========TRAINING AND PREDICTING WITH RCAE============================\n",
            "[INFO:]  Length of Positive data 4518\n",
            "[INFO:]  Length of Negative data 46\n",
            "[INFO:] X_test.shape (4564, 28, 28, 1)\n",
            "[INFO:] y_test.shape [ 1.  1.  1. ... -1. -1. -1.]\n",
            "[INFO:] y_train.shape (4564,)\n",
            "[INFO:] y_train.shape [ 1.  1.  1. ... -1. -1. -1.]\n",
            "[INFO] compiling model...\n",
            "Train on 4107 samples, validate on 457 samples\n",
            "Epoch 1/5\n",
            "4107/4107 [==============================] - 4s 954us/step - loss: 6996.9159 - val_loss: 7212.1266\n",
            "Epoch 2/5\n",
            "4107/4107 [==============================] - 1s 342us/step - loss: 6991.5920 - val_loss: 7211.9601\n",
            "Epoch 3/5\n",
            "4107/4107 [==============================] - 1s 348us/step - loss: 6991.5344 - val_loss: 7211.9303\n",
            "Epoch 4/5\n",
            "4107/4107 [==============================] - 1s 342us/step - loss: 6991.5184 - val_loss: 7211.9164\n",
            "Epoch 5/5\n",
            "4107/4107 [==============================] - 1s 347us/step - loss: 6991.5082 - val_loss: 7211.9071\n",
            "(lamda,Threshold) 0.5 0.25\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (4564, 784) 784\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 1643449 0.5\n",
            "The shape of N (4564, 784)\n",
            "The minimum value of N  -0.75\n",
            "The max value of N 253.966796875\n",
            "[INFO:] Xclean  MSE Computed shape (4564, 784)\n",
            "[INFO:]Xdecoded  Computed shape (4564, 784)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([7008.495])\n",
            "[INFO:] The anomaly threshold computed is  7008.495\n",
            "side: 28\n",
            "channel: 1\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/MNIST/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [4143, 1330, 274, 2391, 4315, 2010, 4289, 2589, 218, 1395, 1246, 437, 3363, 484, 4163, 1506, 1877, 4524, 3202, 2853, 1997, 1493, 1783, 2514, 3960, 2264, 1227, 1071, 1133, 1813, 127, 4351, 608, 4245, 574, 4119, 2468, 3134, 3223, 4483, 816, 994, 624, 4389, 2250, 676]\n",
            "[INFO:] The worstreconstructed_Top200index index are  [4143, 1330, 274, 2391, 4315, 2010, 4289, 2589, 218, 1395, 1246, 437, 3363, 484, 4163, 1506, 1877, 4524, 3202, 2853, 1997, 1493, 1783, 2514, 3960, 2264, 1227, 1071, 1133, 1813, 127, 4351, 608, 4245, 574, 4119, 2468, 3134, 3223, 4483, 816, 994, 624, 4389, 2250, 676, 1519, 826, 1622, 4019, 3844, 829, 2037, 543, 3969, 4529, 1557, 2776, 3491, 4040, 621, 4260, 3047, 3816, 2262, 170, 3247, 3874, 4076, 2232, 2709, 3300, 4559, 2420, 1193, 1203, 4085, 1904, 3266, 2692, 4132, 2583, 2660, 1556, 2936, 2023, 4494, 4440, 2979, 1562, 251, 1571, 405, 1514, 1003, 4064, 4399, 1315, 3370, 2414, 861, 3073, 2097, 3221, 3551, 2210, 2581, 2924, 2565, 2207, 4038, 4027, 1387, 1541, 1093, 3635, 1356, 4107, 485, 433, 939, 3357, 4441, 3804, 4383, 3203, 3303, 2734, 2335, 3456, 4562, 3199, 896, 3246, 745, 1760, 1875, 2120, 1013, 1617, 894, 1999, 1520, 4448, 647, 671, 2279, 4067, 2370, 3360, 156, 4226, 3411, 3104, 1239, 2926, 1509, 4432, 2954, 71, 1483, 642, 1481, 1134, 130, 3732, 2238, 3485, 4117, 3853, 304, 2743, 915, 1144, 3193, 2362, 2803, 2445, 3237, 487, 4451, 2901, 4070, 1841, 4523, 4036, 2192, 2341, 383, 1620, 3480, 2128, 4471, 381, 1351, 2258, 2998, 3582, 42, 2790]\n",
            "=====================\n",
            "AUROC 0.5 0.5108695652173914\n",
            "=======================\n",
            "Saved model to disk....\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([])\n",
            "========================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEVCAYAAAD6u3K7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f34da7f6470>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "kFetanMC7Yi_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_HiGOs197YsG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_etAfbNH7YvN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tjnkvt_y7Y2B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IKP8mM5i7Y5r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "noGHYp_wbdZ4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pretrain Autoencoder"
      ]
    },
    {
      "metadata": {
        "id": "CCR2ZxlIdBZa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "2fe71fe8-b041-43c5-dd64-dee0bc87e5b6"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuel\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c2/b5fb651c90e908f79769b7dd3643982b6a9b1bac9449b8ab16f72612d4f5/fuel-0.2.0.tar.gz (184kB)\n",
            "\u001b[K    100% |████████████████████████████████| 194kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fuel) (1.14.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fuel) (1.11.0)\n",
            "Requirement already satisfied: picklable_itertools in /usr/local/lib/python3.6/dist-packages (from fuel) (0.1.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fuel) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from fuel) (2.8.0)\n",
            "Collecting tables (from fuel)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/1b/21f4c7f296b718575c17ef25e61c05742a283c45077b4c8d5a190b3e0b59/tables-3.4.4-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.8MB 956kB/s \n",
            "\u001b[?25hCollecting progressbar2 (from fuel)\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/6f/acb2dd76f2c77527584bd3a4c2509782bb35c481c610521fc3656de5a9e0/progressbar2-3.38.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from fuel) (16.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fuel) (0.19.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fuel) (4.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fuel) (2.18.4)\n",
            "Collecting numexpr>=2.5.2 (from tables->fuel)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/ea/efd9e16283637eb5b6c0042b6cc3521f1b9a5b47767ac463c88bbd37670c/numexpr-2.6.8-cp36-cp36m-manylinux1_x86_64.whl (162kB)\n",
            "\u001b[K    100% |████████████████████████████████| 163kB 20.3MB/s \n",
            "\u001b[?25hCollecting python-utils>=2.3.0 (from progressbar2->fuel)\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->fuel) (0.46)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2018.8.24)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (3.0.4)\n",
            "Building wheels for collected packages: fuel\n",
            "  Running setup.py bdist_wheel for fuel ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/96/ff/1b/f3708731cf77a6d759cd12e68c0a27700c299fbe2bffd34fa3\n",
            "Successfully built fuel\n",
            "Installing collected packages: numexpr, tables, python-utils, progressbar2, fuel\n",
            "Successfully installed fuel-0.2.0 numexpr-2.6.8 progressbar2-3.38.0 python-utils-2.3.0 tables-3.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hFgZxOe0c4Pm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "dbf6f0f0-0f16-48a7-fe85-2b06580fb643"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting picklable_itertools\n",
            "  Downloading https://files.pythonhosted.org/packages/75/bc/cda9191f0c92960ede6aa95e364443965246385faf62775cc30749931c2c/picklable-itertools-0.1.1.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from picklable_itertools) (1.11.0)\n",
            "Building wheels for collected packages: picklable-itertools\n",
            "  Running setup.py bdist_wheel for picklable-itertools ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/36/dd/e2/ec30ef7c475e1d9fb966735984ba05f8710c67d7de5358c326\n",
            "Successfully built picklable-itertools\n",
            "Installing collected packages: picklable-itertools\n",
            "Successfully installed picklable-itertools-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uMt7wtPkbdZ5",
        "colab_type": "code",
        "colab": {},
        "outputId": "b9307f05-5ac7-47ca-c8f1-91a7e948fc98"
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "DATASET = \"MNIST\"\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_CHANNEL=1\n",
        "HIDDEN_LAYER_SIZE= 128\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/Deep_SVDD/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/Deep_SVDD/\"\n",
        "PRETRAINED_WT_PATH = \"\"\n",
        "\n",
        "## Prepare the data for pretraining CAE\n",
        "x_train = trainX.reshape((len(trainX), 28, 28, 1))\n",
        "x_trainForWtInit= x_train\n",
        "\n",
        "test_ones = test_ones.reshape((len(test_ones), 28, 28, 1))\n",
        "test_sevens = test_sevens.reshape((len(test_sevens), 28, 28, 1))\n",
        "x_test = np.concatenate((test_ones,test_sevens))\n",
        "\n",
        "print(\"Reshaped Training samples for CAE\",x_train.shape)\n",
        "print(\"Reshaped Testing samples for CAE\",x_test.shape)\n",
        "\n",
        "from src.models.Deep_SVDD import Deep_SVDD\n",
        "deep_svdd =   Deep_SVDD(DATASET,x_trainForWtInit,IMG_DIM,HIDDEN_LAYER_SIZE,IMG_HGT,IMG_WDT,IMG_CHANNEL,MODEL_SAVE_PATH,REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainX' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-7deea1159869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m## Prepare the data for pretraining CAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mx_trainForWtInit\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainX' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "IpKJFnVVbdZ8",
        "colab_type": "code",
        "colab": {},
        "outputId": "352cb659-9219-49d6-b015-ce90359885c1"
      },
      "cell_type": "code",
      "source": [
        "Deep_SVDD()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "7Zbkh68AbdZ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8oQZWfYvbdaB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gBEmq6iHbdaC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xo4yNY9ZbdaF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CVcs0VjsbdaG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train and Test  FF_NN Model Supervised Model"
      ]
    },
    {
      "metadata": {
        "id": "Sccbaa8GbdaI",
        "colab_type": "code",
        "colab": {},
        "outputId": "41f7417c-cb38-480d-eb59-7c7c753fdbae"
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_DEPTH=1\n",
        "HIDDEN_LAYER_SIZE=196\n",
        "\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/FF_NN/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/FF_NN/\"\n",
        "\n",
        "print(\"[INFO]\",train_Anomaly_X.shape[0],\"Anomalous Samples Appended to training set\")\n",
        "data_train = np.concatenate((trainX,train_Anomaly_X),axis=0)\n",
        "data_train_label = np.concatenate((trainY,train_Anomaly_Y),axis=0)\n",
        "print(\"[INFO]\",data_train.shape[0],\"Training Samples Contains both 1's and 7s\")\n",
        "nClass =2\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "clf_FF_NN =  FF_NN(IMG_DIM,HIDDEN_LAYER_SIZE,IMG_HGT,IMG_WDT,MODEL_SAVE_PATH,REPORT_SAVE_PATH)\n",
        "clf_FF_NN.fit(data_train,data_train_label,NUM_EPOCHS,IMG_HGT,IMG_WDT,IMG_DEPTH,nClass)\n",
        "\n",
        "## Predict the scores \n",
        "auc_FF_NN = clf_FF_NN.score(test_ones,label_ones,test_sevens,label_sevens)\n",
        "print(\"===========\")\n",
        "print(\"AUC: \",auc_FF_NN)\n",
        "print(\"===========\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_Anomaly_X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-cee4a7dae6a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mREPORT_SAVE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPROJECT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/reports/figures/MNIST/FF_NN/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Anomaly_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Anomalous Samples Appended to training set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Anomaly_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdata_train_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Anomaly_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_Anomaly_X' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "PA74xZ0gbdaL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## FakeNoise FF_NN Model"
      ]
    },
    {
      "metadata": {
        "id": "S98nZz1NbdaM",
        "colab_type": "code",
        "colab": {},
        "outputId": "e140c759-e76c-4933-c518-6ad7ea308808"
      },
      "cell_type": "code",
      "source": [
        "## Fake Noise data to be generated which will be added to the training set before training\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_DEPTH=1\n",
        "HIDDEN_LAYER_SIZE=196\n",
        "\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/FAKE_NOISE_FF_NN/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/FAKE_NOISE_FF_NN/\"\n",
        "\n",
        "\n",
        "from src.models.Fake_Noise_FF_NN import Fake_Noise_FF_NN\n",
        "## Remove the Anomalous data and instead add Noise\n",
        "X_Noise,X_NoiseLabel = createData.get_FAKE_Noise_MNIST_TrainingData(trainX)\n",
        "print(\"[INFO]\",X_Noise.shape[0],\"Noise Samples Appended for training set\")\n",
        "data_train = np.concatenate((trainX,X_Noise),axis=0)\n",
        "data_train_label = np.concatenate((trainY,X_NoiseLabel),axis=0)\n",
        "\n",
        "\n",
        "clf_FakeNoise_FF_NN =   Fake_Noise_FF_NN(IMG_DIM,HIDDEN_LAYER_SIZE,IMG_HGT,IMG_WDT,MODEL_SAVE_PATH,REPORT_SAVE_PATH)\n",
        "clf_FakeNoise_FF_NN.fit(data_train,data_train_label,NUM_EPOCHS,IMG_HGT,IMG_WDT,IMG_DEPTH,nClass)\n",
        "# Predict the scores \n",
        "\n",
        "auc_FAKENOISE_FF_NN = clf_FakeNoise_FF_NN.score(test_ones,label_ones,test_sevens,label_sevens)\n",
        "print(\"===========\")\n",
        "print(\"AUC: \",auc_FAKENOISE_FF_NN)\n",
        "print(\"===========\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] 5000 Noise Samples Appended for training set\n",
            "[INFO] compiling model...\n",
            "[INFO] training network...\n",
            "[INFO] serializing network...\n",
            "[INFO] loading network...\n",
            "5050 Actual test samples\n",
            "5050 Predicted test samples\n",
            "===================================\n",
            "auccary_score: 0.8998019801980198\n",
            "roc_auc_score: 0.5534\n",
            "y_true [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "y_pred [1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0]\n",
            "===================================\n",
            "===========\n",
            "AUC:  0.5534\n",
            "===========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "2fy-jIM3bdaP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SKlearn OCSVM"
      ]
    },
    {
      "metadata": {
        "id": "arBCAHeNbdaQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "b04224e5-971d-4e59-e304-0904510482b6"
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "##create the classifier\n",
        "from src.models.ocsvmSklearn import OCSVM\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "ocsvm = OCSVM(IMG_HGT,IMG_WDT)\n",
        "nu= 0.01\n",
        "kernel = 'linear'\n",
        "clf = ocsvm.fit(trainX,nu,kernel)\n",
        "res = ocsvm.score(clf,test_ones,test_sevens)\n",
        "auc_OCSVM_linear = res\n",
        "print(\"=\"*35)\n",
        "print(\"AUC:\",res)\n",
        "print(\"=\"*35)\n",
        "\n",
        "kernel = 'rbf'\n",
        "clf = ocsvm.fit(trainX,nu,kernel)\n",
        "res = ocsvm.score(clf,test_ones,test_sevens)\n",
        "auc_OCSVM_rbf = res\n",
        "print(\"=\"*35)\n",
        "print(\"AUC:\",res)\n",
        "print(\"=\"*35)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the OCSVM classifier.....\n",
            "===================================\n",
            "AUC: 0.7869240000000001\n",
            "===================================\n",
            "Training the OCSVM classifier.....\n",
            "===================================\n",
            "AUC: 0.9705560000000001\n",
            "===================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "7SXtK0pxbdaT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## OC-NN"
      ]
    },
    {
      "metadata": {
        "id": "J8QqF4wnbdaU",
        "colab_type": "code",
        "colab": {},
        "outputId": "3251127d-4575-4e15-8d89-ff89a7f42d28"
      },
      "cell_type": "code",
      "source": [
        "##create the classifier\n",
        "## Instantiate the object and call the function\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "DATASET= \"MNIST\"\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_DEPTH=1\n",
        "HIDDEN_LAYER_SIZE=196\n",
        "nClass=2\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/OC_NN/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/OC_NN/\"\n",
        "PRE_TRAINED_WT_PATH = PROJECT_DIR +\"/models/MNIST/FF_NN/\"\n",
        "\n",
        "from src.models.OC_NN import OC_NN\n",
        "import keras\n",
        "\n",
        "ocnn = OC_NN(DATASET,IMG_DIM,HIDDEN_LAYER_SIZE,IMG_HGT,IMG_WDT,MODEL_SAVE_PATH,REPORT_SAVE_PATH,PRE_TRAINED_WT_PATH)\n",
        "\n",
        "\n",
        "nu= 0.01\n",
        "NUM_EPOCHS = 100\n",
        "ocnn.fit(trainX,nu,NUM_EPOCHS,IMG_HGT,IMG_WDT,IMG_DEPTH,nClass)\n",
        "res = ocnn.score(test_ones,test_sevens) \n",
        "auc_OCNN = res\n",
        "\n",
        "print(\"=\"*35)\n",
        "print(\"AUC:\",res)\n",
        "print(\"=\"*35)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] compiling model...\n",
            "[INFO]  (196,) input  --> hidden layer weights shape ...\n",
            "[INFO]  (2,) hidden --> output layer weights shape ...\n",
            "[INFO] training network...\n",
            "Train on 4500 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "4500/4500 [==============================] - 0s 63us/step - loss: 44.2036 - val_loss: 43.6791\n",
            "evaluation for epoch: 0\n",
            "output: Tensor(\"Print:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 2/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.1708 - val_loss: 43.7103\n",
            "evaluation for epoch: 1\n",
            "output: Tensor(\"Print_1:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 3/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.1486 - val_loss: 43.7392\n",
            "evaluation for epoch: 2\n",
            "output: Tensor(\"Print_2:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 4/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.1274 - val_loss: 43.7661\n",
            "evaluation for epoch: 3\n",
            "output: Tensor(\"Print_3:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 5/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.1070 - val_loss: 43.7910\n",
            "evaluation for epoch: 4\n",
            "output: Tensor(\"Print_4:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 6/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 44.0875 - val_loss: 43.8141\n",
            "evaluation for epoch: 5\n",
            "output: Tensor(\"Print_5:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 7/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.0683 - val_loss: 43.8355\n",
            "evaluation for epoch: 6\n",
            "output: Tensor(\"Print_6:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 8/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.0497 - val_loss: 43.8560\n",
            "evaluation for epoch: 7\n",
            "output: Tensor(\"Print_7:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 9/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.0315 - val_loss: 43.8742\n",
            "evaluation for epoch: 8\n",
            "output: Tensor(\"Print_8:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 10/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.0135 - val_loss: 43.8920\n",
            "evaluation for epoch: 9\n",
            "output: Tensor(\"Print_9:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 11/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.9958 - val_loss: 43.9087\n",
            "evaluation for epoch: 10\n",
            "output: Tensor(\"Print_10:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 12/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9783 - val_loss: 43.9235\n",
            "evaluation for epoch: 11\n",
            "output: Tensor(\"Print_11:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 13/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9610 - val_loss: 43.9383\n",
            "evaluation for epoch: 12\n",
            "output: Tensor(\"Print_12:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 14/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9438 - val_loss: 43.9518\n",
            "evaluation for epoch: 13\n",
            "output: Tensor(\"Print_13:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 15/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9268 - val_loss: 43.9646\n",
            "evaluation for epoch: 14\n",
            "output: Tensor(\"Print_14:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 16/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9098 - val_loss: 43.9768\n",
            "evaluation for epoch: 15\n",
            "output: Tensor(\"Print_15:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 17/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.8930 - val_loss: 43.9876\n",
            "evaluation for epoch: 16\n",
            "output: Tensor(\"Print_16:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 18/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.8763 - val_loss: 43.9983\n",
            "evaluation for epoch: 17\n",
            "output: Tensor(\"Print_17:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 19/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.8597 - val_loss: 44.0087\n",
            "evaluation for epoch: 18\n",
            "output: Tensor(\"Print_18:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 20/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.8431 - val_loss: 44.0186\n",
            "evaluation for epoch: 19\n",
            "output: Tensor(\"Print_19:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 21/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.8266 - val_loss: 44.0276\n",
            "evaluation for epoch: 20\n",
            "output: Tensor(\"Print_20:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 22/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.8102 - val_loss: 44.0363\n",
            "evaluation for epoch: 21\n",
            "output: Tensor(\"Print_21:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 23/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.7938 - val_loss: 44.0446\n",
            "evaluation for epoch: 22\n",
            "output: Tensor(\"Print_22:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 24/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.7775 - val_loss: 44.0524\n",
            "evaluation for epoch: 23\n",
            "output: Tensor(\"Print_23:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 25/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.7613 - val_loss: 44.0602\n",
            "evaluation for epoch: 24\n",
            "output: Tensor(\"Print_24:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 26/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 43.7450 - val_loss: 44.0676\n",
            "evaluation for epoch: 25\n",
            "output: Tensor(\"Print_25:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 27/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.7290 - val_loss: 44.0743\n",
            "evaluation for epoch: 26\n",
            "output: Tensor(\"Print_26:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 28/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.7129 - val_loss: 44.0810\n",
            "evaluation for epoch: 27\n",
            "output: Tensor(\"Print_27:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 29/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6969 - val_loss: 44.0873\n",
            "evaluation for epoch: 28\n",
            "output: Tensor(\"Print_28:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 30/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6810 - val_loss: 44.0933\n",
            "evaluation for epoch: 29\n",
            "output: Tensor(\"Print_29:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 31/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6651 - val_loss: 44.0989\n",
            "evaluation for epoch: 30\n",
            "output: Tensor(\"Print_30:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 32/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6492 - val_loss: 44.1047\n",
            "evaluation for epoch: 31\n",
            "output: Tensor(\"Print_31:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 33/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6334 - val_loss: 44.1100\n",
            "evaluation for epoch: 32\n",
            "output: Tensor(\"Print_32:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 34/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6176 - val_loss: 44.1152\n",
            "evaluation for epoch: 33\n",
            "output: Tensor(\"Print_33:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 35/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6019 - val_loss: 44.1201\n",
            "evaluation for epoch: 34\n",
            "output: Tensor(\"Print_34:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 36/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.5862 - val_loss: 44.1254\n",
            "evaluation for epoch: 35\n",
            "output: Tensor(\"Print_35:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 37/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.5705 - val_loss: 44.1299\n",
            "evaluation for epoch: 36\n",
            "output: Tensor(\"Print_36:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 38/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.5548 - val_loss: 44.1340\n",
            "evaluation for epoch: 37\n",
            "output: Tensor(\"Print_37:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 39/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.5392 - val_loss: 44.1384\n",
            "evaluation for epoch: 38\n",
            "output: Tensor(\"Print_38:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 40/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.5235 - val_loss: 44.1428\n",
            "evaluation for epoch: 39\n",
            "output: Tensor(\"Print_39:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 41/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.5081 - val_loss: 44.1472\n",
            "evaluation for epoch: 40\n",
            "output: Tensor(\"Print_40:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 42/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.4925 - val_loss: 44.1508\n",
            "evaluation for epoch: 41\n",
            "output: Tensor(\"Print_41:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 43/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 43.4770 - val_loss: 44.1549\n",
            "evaluation for epoch: 42\n",
            "output: Tensor(\"Print_42:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 44/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.4615 - val_loss: 44.1586\n",
            "evaluation for epoch: 43\n",
            "output: Tensor(\"Print_43:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 45/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.4460 - val_loss: 44.1624\n",
            "evaluation for epoch: 44\n",
            "output: Tensor(\"Print_44:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 46/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.4305 - val_loss: 44.1662\n",
            "evaluation for epoch: 45\n",
            "output: Tensor(\"Print_45:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 47/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.4151 - val_loss: 44.1692\n",
            "evaluation for epoch: 46\n",
            "output: Tensor(\"Print_46:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 48/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.3998 - val_loss: 44.1723\n",
            "evaluation for epoch: 47\n",
            "output: Tensor(\"Print_47:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 49/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.3843 - val_loss: 44.1755\n",
            "evaluation for epoch: 48\n",
            "output: Tensor(\"Print_48:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 50/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.3690 - val_loss: 44.1785\n",
            "evaluation for epoch: 49\n",
            "output: Tensor(\"Print_49:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 51/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.3536 - val_loss: 44.1808\n",
            "evaluation for epoch: 50\n",
            "output: Tensor(\"Print_50:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 52/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.3383 - val_loss: 44.1833\n",
            "evaluation for epoch: 51\n",
            "output: Tensor(\"Print_51:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 53/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.3230 - val_loss: 44.1867\n",
            "evaluation for epoch: 52\n",
            "output: Tensor(\"Print_52:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 54/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.3077 - val_loss: 44.1901\n",
            "evaluation for epoch: 53\n",
            "output: Tensor(\"Print_53:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 55/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.2924 - val_loss: 44.1927\n",
            "evaluation for epoch: 54\n",
            "output: Tensor(\"Print_54:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 56/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.2772 - val_loss: 44.1949\n",
            "evaluation for epoch: 55\n",
            "output: Tensor(\"Print_55:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 57/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.2620 - val_loss: 44.1971\n",
            "evaluation for epoch: 56\n",
            "output: Tensor(\"Print_56:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 58/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.2467 - val_loss: 44.1995\n",
            "evaluation for epoch: 57\n",
            "output: Tensor(\"Print_57:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 59/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.2315 - val_loss: 44.2022\n",
            "evaluation for epoch: 58\n",
            "output: Tensor(\"Print_58:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 60/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.2163 - val_loss: 44.2043\n",
            "evaluation for epoch: 59\n",
            "output: Tensor(\"Print_59:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 61/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.2012 - val_loss: 44.2068\n",
            "evaluation for epoch: 60\n",
            "output: Tensor(\"Print_60:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 62/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.1860 - val_loss: 44.2088\n",
            "evaluation for epoch: 61\n",
            "output: Tensor(\"Print_61:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 63/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1708 - val_loss: 44.2115\n",
            "evaluation for epoch: 62\n",
            "output: Tensor(\"Print_62:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 64/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1557 - val_loss: 44.2135\n",
            "evaluation for epoch: 63\n",
            "output: Tensor(\"Print_63:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 65/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1406 - val_loss: 44.2155\n",
            "evaluation for epoch: 64\n",
            "output: Tensor(\"Print_64:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 66/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1255 - val_loss: 44.2175\n",
            "evaluation for epoch: 65\n",
            "output: Tensor(\"Print_65:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 67/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1105 - val_loss: 44.2191\n",
            "evaluation for epoch: 66\n",
            "output: Tensor(\"Print_66:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 68/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.0953 - val_loss: 44.2209\n",
            "evaluation for epoch: 67\n",
            "output: Tensor(\"Print_67:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 69/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.0802 - val_loss: 44.2227\n",
            "evaluation for epoch: 68\n",
            "output: Tensor(\"Print_68:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 70/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.0652 - val_loss: 44.2248\n",
            "evaluation for epoch: 69\n",
            "output: Tensor(\"Print_69:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 71/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.0502 - val_loss: 44.2263\n",
            "evaluation for epoch: 70\n",
            "output: Tensor(\"Print_70:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 72/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.0350 - val_loss: 44.2279\n",
            "evaluation for epoch: 71\n",
            "output: Tensor(\"Print_71:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 73/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.0201 - val_loss: 44.2291\n",
            "evaluation for epoch: 72\n",
            "output: Tensor(\"Print_72:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 74/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.0051 - val_loss: 44.2304\n",
            "evaluation for epoch: 73\n",
            "output: Tensor(\"Print_73:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 75/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9900 - val_loss: 44.2318\n",
            "evaluation for epoch: 74\n",
            "output: Tensor(\"Print_74:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 76/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9750 - val_loss: 44.2337\n",
            "evaluation for epoch: 75\n",
            "output: Tensor(\"Print_75:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 77/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9601 - val_loss: 44.2353\n",
            "evaluation for epoch: 76\n",
            "output: Tensor(\"Print_76:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 78/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 42.9451 - val_loss: 44.2366\n",
            "evaluation for epoch: 77\n",
            "output: Tensor(\"Print_77:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 79/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9301 - val_loss: 44.2378\n",
            "evaluation for epoch: 78\n",
            "output: Tensor(\"Print_78:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 80/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.9153 - val_loss: 44.2391\n",
            "evaluation for epoch: 79\n",
            "output: Tensor(\"Print_79:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 81/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9004 - val_loss: 44.2408\n",
            "evaluation for epoch: 80\n",
            "output: Tensor(\"Print_80:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 82/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 42.8855 - val_loss: 44.2420\n",
            "evaluation for epoch: 81\n",
            "output: Tensor(\"Print_81:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 83/100\n",
            "4500/4500 [==============================] - 0s 32us/step - loss: 42.8707 - val_loss: 44.2429\n",
            "evaluation for epoch: 82\n",
            "output: Tensor(\"Print_82:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 84/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 42.8558 - val_loss: 44.2441\n",
            "evaluation for epoch: 83\n",
            "output: Tensor(\"Print_83:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 85/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.8410 - val_loss: 44.2451\n",
            "evaluation for epoch: 84\n",
            "output: Tensor(\"Print_84:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 86/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4500/4500 [==============================] - 0s 31us/step - loss: 42.8262 - val_loss: 44.2463\n",
            "evaluation for epoch: 85\n",
            "output: Tensor(\"Print_85:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 87/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.8114 - val_loss: 44.2472\n",
            "evaluation for epoch: 86\n",
            "output: Tensor(\"Print_86:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 88/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.7966 - val_loss: 44.2481\n",
            "evaluation for epoch: 87\n",
            "output: Tensor(\"Print_87:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 89/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.7819 - val_loss: 44.2487\n",
            "evaluation for epoch: 88\n",
            "output: Tensor(\"Print_88:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 90/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.7671 - val_loss: 44.2499\n",
            "evaluation for epoch: 89\n",
            "output: Tensor(\"Print_89:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 91/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.7523 - val_loss: 44.2506\n",
            "evaluation for epoch: 90\n",
            "output: Tensor(\"Print_90:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 92/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.7376 - val_loss: 44.2513\n",
            "evaluation for epoch: 91\n",
            "output: Tensor(\"Print_91:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 93/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.7229 - val_loss: 44.2522\n",
            "evaluation for epoch: 92\n",
            "output: Tensor(\"Print_92:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 94/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.7082 - val_loss: 44.2533\n",
            "evaluation for epoch: 93\n",
            "output: Tensor(\"Print_93:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 95/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.6934 - val_loss: 44.2529\n",
            "evaluation for epoch: 94\n",
            "output: Tensor(\"Print_94:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 96/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6788 - val_loss: 44.2538\n",
            "evaluation for epoch: 95\n",
            "output: Tensor(\"Print_95:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 97/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6641 - val_loss: 44.2544\n",
            "evaluation for epoch: 96\n",
            "output: Tensor(\"Print_96:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 98/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6494 - val_loss: 44.2557\n",
            "evaluation for epoch: 97\n",
            "output: Tensor(\"Print_97:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 99/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6347 - val_loss: 44.2564\n",
            "evaluation for epoch: 98\n",
            "output: Tensor(\"Print_98:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 100/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6200 - val_loss: 44.2568\n",
            "evaluation for epoch: 99\n",
            "output: Tensor(\"Print_99:0\", shape=(?, 2), dtype=float32)\n",
            "[INFO] serializing network and saving trained weights...\n",
            "[INFO] Saving model layer weights...\n",
            "[INFO] loading network...\n",
            "5050 Actual test samples\n",
            "===================================\n",
            "auccary_score: 0.9984158415841584\n",
            "roc_auc_score: 0.9199999999999999\n",
            "y_true [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "y_pred [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0]\n",
            "===================================\n",
            "===================================\n",
            "AUC: 0.9199999999999999\n",
            "===================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/raghav/envPython3/lib/python3.6/site-packages/keras/engine/sequential.py:252: UserWarning: Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
            "  warnings.warn('Network returning invalid probability values. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "L-fcJooebdaX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Comparing  AUC scores of various methods"
      ]
    },
    {
      "metadata": {
        "id": "wQDAg1kDbdaY",
        "colab_type": "code",
        "colab": {},
        "outputId": "24ca6518-4f61-465a-f701-b41f81bdb64a"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "auc = np.zeros((1,5))\n",
        "auc[0][0] = auc_FF_NN\n",
        "auc[0][1] = auc_FAKENOISE_FF_NN\n",
        "auc[0][2] = auc_OCSVM_linear\n",
        "auc[0][3] = auc_OCSVM_rbf\n",
        "auc[0][4] = auc_OCNN\n",
        "\n",
        "\n",
        "aucList = [auc_FF_NN,auc_FAKENOISE_FF_NN, auc_OCSVM_linear,auc_OCSVM_rbf, auc_OCNN]\n",
        "\n",
        "index = ['FF_NN', 'Fake_NN', 'OCSVM_L','OCSVM_rbf','OCNN']\n",
        "df = pd.DataFrame({'auc': aucList}, index=index)\n",
        "ax = df.plot.bar(rot=0)\n",
        "\n",
        "plt.ylabel('AUC')\n",
        "plt.xlabel('Methods')\n",
        "plt.title('AUC Comparision for MNIST Dataset ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'AUC Comparision for MNIST Dataset ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Uudzs5Xfbdaa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}