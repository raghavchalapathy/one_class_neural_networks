{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCAE_CIFAR10_Experiments--Bird_Vs_All_New.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/raghavchalapathy/one_class_neural_networks/blob/master/RCAE_CIFAR10_Experiments_Bird_Vs_All_New.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "6TKSizAjYUU1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d596a35-a069-4528-a2cd-03c91c41de28"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# !pip install picklable_itertools\n",
        "# !pip install fuel"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qO-ewRVzYH9O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EX7QNOkjYH9S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "PROJECT_DIR = \"/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/\"\n",
        "import sys,os\n",
        "import numpy as np\n",
        "sys.path.append(PROJECT_DIR)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MBX8bnXdYH9U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Obtain Training and Test Datasets"
      ]
    },
    {
      "metadata": {
        "id": "u45TrAvEYH9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66393
        },
        "outputId": "922bdd2a-3d57-4419-f9d8-9f234aca731d"
      },
      "cell_type": "code",
      "source": [
        "## Obtaining the training and testing data\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from src.models.RCAE import RCAE_AD\n",
        "\n",
        "DATASET = \"cifar10\"\n",
        "IMG_DIM= 3072\n",
        "IMG_HGT =32\n",
        "IMG_WDT=32\n",
        "IMG_CHANNEL=1\n",
        "HIDDEN_LAYER_SIZE= 128\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n",
        "PRETRAINED_WT_PATH = \"\"\n",
        "\n",
        "rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n",
        "\n",
        "print(\"Train Data Shape: \",rcae.data._X_train.shape)\n",
        "print(\"Train Label Shape: \",rcae.data._y_train.shape)\n",
        "print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n",
        "print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n",
        "print(\"Test Data Shape: \",rcae.data._X_test.shape)\n",
        "print(\"Test Label Shape: \",rcae.data._y_test.shape)\n",
        "print(\"===========TRAINING AND PREDICTING WITH RCAE============================\")\n",
        "rcae.fit_and_predict()\n",
        "print(\"========================================================================\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n",
            "This call to matplotlib.use() has no effect because the backend has already\n",
            "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
            "or matplotlib.backends is imported for the first time.\n",
            "\n",
            "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
            "    ioloop.IOLoop.instance().start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
            "    super(ZMQIOLoop, self).start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n",
            "    handler_func(fd_obj, events)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
            "    self._handle_recv()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
            "    self._run_callback(callback, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
            "    callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
            "    return self.dispatch_shell(stream, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
            "    handler(stream, idents, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
            "    user_expressions, allow_stdin)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n",
            "    interactivity=interactivity, compiler=compiler, result=result)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n",
            "    if self.run_code(code, result):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-e96966cecab3>\", line 1, in <module>\n",
            "    get_ipython().magic('matplotlib inline')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
            "    return self.run_line_magic(magic_name, magic_arg_s)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
            "    result = fn(*args,**kwargs)\n",
            "  File \"<decorator-gen-105>\", line 2, in matplotlib\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
            "    call = lambda f, *a, **k: f(*a, **k)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n",
            "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
            "    pt.activate_matplotlib(backend)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n",
            "    matplotlib.pyplot.switch_backend(backend)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
            "    matplotlib.use(newbackend, warn=False, force=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
            "    reload(sys.modules['matplotlib.backends'])\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
            "    line for line in traceback.format_stack()\n",
            "\n",
            "\n",
            "  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n",
            "This call to matplotlib.use() has no effect because the backend has already\n",
            "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
            "or matplotlib.backends is imported for the first time.\n",
            "\n",
            "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
            "    ioloop.IOLoop.instance().start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
            "    super(ZMQIOLoop, self).start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n",
            "    handler_func(fd_obj, events)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
            "    self._handle_recv()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
            "    self._run_callback(callback, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
            "    callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
            "    return self.dispatch_shell(stream, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
            "    handler(stream, idents, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
            "    user_expressions, allow_stdin)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n",
            "    interactivity=interactivity, compiler=compiler, result=result)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n",
            "    if self.run_code(code, result):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-e96966cecab3>\", line 1, in <module>\n",
            "    get_ipython().magic('matplotlib inline')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
            "    return self.run_line_magic(magic_name, magic_arg_s)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
            "    result = fn(*args,**kwargs)\n",
            "  File \"<decorator-gen-105>\", line 2, in matplotlib\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
            "    call = lambda f, *a, **k: f(*a, **k)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n",
            "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
            "    pt.activate_matplotlib(backend)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n",
            "    matplotlib.pyplot.switch_backend(backend)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
            "    matplotlib.use(newbackend, warn=False, force=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
            "    reload(sys.modules['matplotlib.backends'])\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
            "    line for line in traceback.format_stack()\n",
            "\n",
            "\n",
            "  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n",
            "[INFO:] Loading data...\n",
            "Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n",
            "[INFO:] Loading data...\n",
            "[INFO:] Assertions of memory muted\n",
            "[INFO:] Loading data...\n",
            "Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n",
            "[INFO:] Loading data...\n",
            "Train Data Shape:  (5500, 32, 32, 3)\n",
            "Train Label Shape:  (5500,)\n",
            "Validation Data Shape:  (5500, 32, 32, 3)\n",
            "Validation Label Shape:  (5500,)\n",
            "Test Data Shape:  (5500, 32, 32, 3)\n",
            "Test Label Shape:  (5500,)\n",
            "===========TRAINING AND PREDICTING WITH RCAE============================\n",
            "[INFO:]  Length of Positive data 5000\n",
            "[INFO:]  Length of Negative data 500\n",
            "[INFO:] X_test.shape (5500, 32, 32, 3)\n",
            "[INFO:] y_test.shape [ 1.  1.  1. ... -1. -1. -1.]\n",
            "[INFO:] y_train.shape (5500,)\n",
            "[INFO:] y_train.shape [ 1.  1.  1. ... -1. -1. -1.]\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 128) (256, 128)\n",
            "Train on 4950 samples, validate on 550 samples\n",
            "Epoch 1/250\n",
            "4950/4950 [==============================] - 6s 1ms/step - loss: 1.3630 - val_loss: 1.4139\n",
            "Epoch 2/250\n",
            "4950/4950 [==============================] - 2s 437us/step - loss: 1.3341 - val_loss: 1.3610\n",
            "Epoch 3/250\n",
            "4950/4950 [==============================] - 2s 432us/step - loss: 1.3296 - val_loss: 1.3457\n",
            "Epoch 4/250\n",
            "4950/4950 [==============================] - 2s 439us/step - loss: 1.3275 - val_loss: 1.3438\n",
            "Epoch 5/250\n",
            "4950/4950 [==============================] - 2s 440us/step - loss: 1.3261 - val_loss: 1.3396\n",
            "Epoch 6/250\n",
            "4950/4950 [==============================] - 2s 439us/step - loss: 1.3250 - val_loss: 1.3363\n",
            "Epoch 7/250\n",
            "4950/4950 [==============================] - 2s 437us/step - loss: 1.3244 - val_loss: 1.3343\n",
            "Epoch 8/250\n",
            "4950/4950 [==============================] - 2s 438us/step - loss: 1.3237 - val_loss: 1.3322\n",
            "Epoch 9/250\n",
            "4950/4950 [==============================] - 2s 432us/step - loss: 1.3232 - val_loss: 1.3323\n",
            "Epoch 10/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3228 - val_loss: 1.3317\n",
            "Epoch 11/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3226 - val_loss: 1.3320\n",
            "Epoch 12/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3221 - val_loss: 1.3312\n",
            "Epoch 13/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3217 - val_loss: 1.3305\n",
            "Epoch 14/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3213 - val_loss: 1.3299\n",
            "Epoch 15/250\n",
            "4950/4950 [==============================] - 2s 433us/step - loss: 1.3211 - val_loss: 1.3279\n",
            "Epoch 16/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 1.3207 - val_loss: 1.3279\n",
            "Epoch 17/250\n",
            "4950/4950 [==============================] - 2s 439us/step - loss: 1.3204 - val_loss: 1.3279\n",
            "Epoch 18/250\n",
            "4950/4950 [==============================] - 2s 441us/step - loss: 1.3203 - val_loss: 1.3284\n",
            "Epoch 19/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 1.3202 - val_loss: 1.3270\n",
            "Epoch 20/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 1.3199 - val_loss: 1.3275\n",
            "Epoch 21/250\n",
            "4950/4950 [==============================] - 2s 437us/step - loss: 1.3198 - val_loss: 1.3268\n",
            "Epoch 22/250\n",
            "4950/4950 [==============================] - 2s 440us/step - loss: 1.3196 - val_loss: 1.3275\n",
            "Epoch 23/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 1.3193 - val_loss: 1.3265\n",
            "Epoch 24/250\n",
            "4950/4950 [==============================] - 2s 441us/step - loss: 1.3192 - val_loss: 1.3261\n",
            "Epoch 25/250\n",
            "4950/4950 [==============================] - 2s 437us/step - loss: 1.3190 - val_loss: 1.3254\n",
            "Epoch 26/250\n",
            "4950/4950 [==============================] - 2s 434us/step - loss: 1.3190 - val_loss: 1.3252\n",
            "Epoch 27/250\n",
            "4950/4950 [==============================] - 2s 440us/step - loss: 1.3188 - val_loss: 1.3255\n",
            "Epoch 28/250\n",
            "4950/4950 [==============================] - 2s 434us/step - loss: 1.3187 - val_loss: 1.3257\n",
            "Epoch 29/250\n",
            "4950/4950 [==============================] - 2s 436us/step - loss: 1.3186 - val_loss: 1.3266\n",
            "Epoch 30/250\n",
            "4950/4950 [==============================] - 2s 438us/step - loss: 1.3185 - val_loss: 1.3248\n",
            "Epoch 31/250\n",
            "4950/4950 [==============================] - 2s 436us/step - loss: 1.3183 - val_loss: 1.3255\n",
            "Epoch 32/250\n",
            "4950/4950 [==============================] - 2s 440us/step - loss: 1.3183 - val_loss: 1.3249\n",
            "Epoch 33/250\n",
            "4950/4950 [==============================] - 2s 439us/step - loss: 1.3181 - val_loss: 1.3243\n",
            "Epoch 34/250\n",
            "4950/4950 [==============================] - 2s 438us/step - loss: 1.3180 - val_loss: 1.3246\n",
            "Epoch 35/250\n",
            "4950/4950 [==============================] - 2s 434us/step - loss: 1.3179 - val_loss: 1.3243\n",
            "Epoch 36/250\n",
            "4950/4950 [==============================] - 2s 438us/step - loss: 1.3178 - val_loss: 1.3236\n",
            "Epoch 37/250\n",
            "4950/4950 [==============================] - 2s 439us/step - loss: 1.3178 - val_loss: 1.3240\n",
            "Epoch 38/250\n",
            "4950/4950 [==============================] - 2s 440us/step - loss: 1.3177 - val_loss: 1.3239\n",
            "Epoch 39/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 1.3177 - val_loss: 1.3240\n",
            "Epoch 40/250\n",
            "4950/4950 [==============================] - 2s 440us/step - loss: 1.3175 - val_loss: 1.3242\n",
            "Epoch 41/250\n",
            "4950/4950 [==============================] - 2s 433us/step - loss: 1.3173 - val_loss: 1.3233\n",
            "Epoch 42/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3173 - val_loss: 1.3229\n",
            "Epoch 43/250\n",
            "4950/4950 [==============================] - 2s 430us/step - loss: 1.3174 - val_loss: 1.3255\n",
            "Epoch 44/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3175 - val_loss: 1.3241\n",
            "Epoch 45/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3171 - val_loss: 1.3231\n",
            "Epoch 46/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3172 - val_loss: 1.3225\n",
            "Epoch 47/250\n",
            "4950/4950 [==============================] - 2s 430us/step - loss: 1.3170 - val_loss: 1.3237\n",
            "Epoch 48/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3170 - val_loss: 1.3231\n",
            "Epoch 49/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 1.3168 - val_loss: 1.3226\n",
            "Epoch 50/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3168 - val_loss: 1.3223\n",
            "Epoch 51/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 1.3167 - val_loss: 1.3221\n",
            "Epoch 52/250\n",
            "4950/4950 [==============================] - 2s 430us/step - loss: 1.3168 - val_loss: 1.3225\n",
            "Epoch 53/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3167 - val_loss: 1.3239\n",
            "Epoch 54/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3167 - val_loss: 1.3224\n",
            "Epoch 55/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 1.3166 - val_loss: 1.3222\n",
            "Epoch 56/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3164 - val_loss: 1.3221\n",
            "Epoch 57/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3165 - val_loss: 1.3223\n",
            "Epoch 58/250\n",
            "4950/4950 [==============================] - 2s 434us/step - loss: 1.3165 - val_loss: 1.3227\n",
            "Epoch 59/250\n",
            "4950/4950 [==============================] - 2s 433us/step - loss: 1.3164 - val_loss: 1.3214\n",
            "Epoch 60/250\n",
            "4950/4950 [==============================] - 2s 438us/step - loss: 1.3162 - val_loss: 1.3222\n",
            "Epoch 61/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3163 - val_loss: 1.3221\n",
            "Epoch 62/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3161 - val_loss: 1.3219\n",
            "Epoch 63/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3161 - val_loss: 1.3212\n",
            "Epoch 64/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3159 - val_loss: 1.3211\n",
            "Epoch 65/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.3159 - val_loss: 1.3207\n",
            "Epoch 66/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3159 - val_loss: 1.3215\n",
            "Epoch 67/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 1.3159 - val_loss: 1.3213\n",
            "Epoch 68/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3158 - val_loss: 1.3206\n",
            "Epoch 69/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.3158 - val_loss: 1.3207\n",
            "Epoch 70/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3157 - val_loss: 1.3216\n",
            "Epoch 71/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 1.3158 - val_loss: 1.3205\n",
            "Epoch 72/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3157 - val_loss: 1.3211\n",
            "Epoch 73/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3154 - val_loss: 1.3202\n",
            "Epoch 74/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3154 - val_loss: 1.3203\n",
            "Epoch 75/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 1.3155 - val_loss: 1.3201\n",
            "Epoch 76/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3154 - val_loss: 1.3204\n",
            "Epoch 77/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3153 - val_loss: 1.3201\n",
            "Epoch 78/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3152 - val_loss: 1.3202\n",
            "Epoch 79/250\n",
            "4950/4950 [==============================] - 2s 431us/step - loss: 1.3152 - val_loss: 1.3199\n",
            "Epoch 80/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3152 - val_loss: 1.3202\n",
            "Epoch 81/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3150 - val_loss: 1.3198\n",
            "Epoch 82/250\n",
            "4950/4950 [==============================] - 2s 434us/step - loss: 1.3149 - val_loss: 1.3196\n",
            "Epoch 83/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3149 - val_loss: 1.3195\n",
            "Epoch 84/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 1.3149 - val_loss: 1.3198\n",
            "Epoch 85/250\n",
            "4950/4950 [==============================] - 2s 432us/step - loss: 1.3148 - val_loss: 1.3196\n",
            "Epoch 86/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3147 - val_loss: 1.3195\n",
            "Epoch 87/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3150 - val_loss: 1.3202\n",
            "Epoch 88/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3149 - val_loss: 1.3201\n",
            "Epoch 89/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3147 - val_loss: 1.3196\n",
            "Epoch 90/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3147 - val_loss: 1.3195\n",
            "Epoch 91/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3146 - val_loss: 1.3196\n",
            "Epoch 92/250\n",
            "4950/4950 [==============================] - 2s 433us/step - loss: 1.3146 - val_loss: 1.3192\n",
            "Epoch 93/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3146 - val_loss: 1.3194\n",
            "Epoch 94/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3145 - val_loss: 1.3188\n",
            "Epoch 95/250\n",
            "4950/4950 [==============================] - 2s 431us/step - loss: 1.3145 - val_loss: 1.3189\n",
            "Epoch 96/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3143 - val_loss: 1.3186\n",
            "Epoch 97/250\n",
            "4950/4950 [==============================] - 2s 431us/step - loss: 1.3143 - val_loss: 1.3187\n",
            "Epoch 98/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3143 - val_loss: 1.3185\n",
            "Epoch 99/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3142 - val_loss: 1.3184\n",
            "Epoch 100/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3142 - val_loss: 1.3190\n",
            "Epoch 101/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 1.3143 - val_loss: 1.3183\n",
            "Epoch 102/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3143 - val_loss: 1.3185\n",
            "Epoch 103/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.3142 - val_loss: 1.3183\n",
            "Epoch 104/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3141 - val_loss: 1.3183\n",
            "Epoch 105/250\n",
            "4950/4950 [==============================] - 2s 430us/step - loss: 1.3140 - val_loss: 1.3180\n",
            "Epoch 106/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3139 - val_loss: 1.3182\n",
            "Epoch 107/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3140 - val_loss: 1.3182\n",
            "Epoch 108/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3141 - val_loss: 1.3181\n",
            "Epoch 109/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3140 - val_loss: 1.3179\n",
            "Epoch 110/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 1.3140 - val_loss: 1.3181\n",
            "Epoch 111/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3138 - val_loss: 1.3182\n",
            "Epoch 112/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3138 - val_loss: 1.3181\n",
            "Epoch 113/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3137 - val_loss: 1.3184\n",
            "Epoch 114/250\n",
            "4950/4950 [==============================] - 2s 430us/step - loss: 1.3138 - val_loss: 1.3181\n",
            "Epoch 115/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 1.3138 - val_loss: 1.3177\n",
            "Epoch 116/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3137 - val_loss: 1.3175\n",
            "Epoch 117/250\n",
            "4950/4950 [==============================] - 2s 431us/step - loss: 1.3136 - val_loss: 1.3175\n",
            "Epoch 118/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3136 - val_loss: 1.3174\n",
            "Epoch 119/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3137 - val_loss: 1.3173\n",
            "Epoch 120/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3136 - val_loss: 1.3174\n",
            "Epoch 121/250\n",
            "4950/4950 [==============================] - 2s 429us/step - loss: 1.3135 - val_loss: 1.3186\n",
            "Epoch 122/250\n",
            "4950/4950 [==============================] - 2s 429us/step - loss: 1.3137 - val_loss: 1.3173\n",
            "Epoch 123/250\n",
            "4950/4950 [==============================] - 2s 430us/step - loss: 1.3136 - val_loss: 1.3173\n",
            "Epoch 124/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3135 - val_loss: 1.3173\n",
            "Epoch 125/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 1.3135 - val_loss: 1.3172\n",
            "Epoch 126/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3134 - val_loss: 1.3174\n",
            "Epoch 127/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 1.3134 - val_loss: 1.3175\n",
            "Epoch 128/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3133 - val_loss: 1.3172\n",
            "Epoch 129/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3134 - val_loss: 1.3170\n",
            "Epoch 130/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3133 - val_loss: 1.3172\n",
            "Epoch 131/250\n",
            "4950/4950 [==============================] - 2s 434us/step - loss: 1.3135 - val_loss: 1.3175\n",
            "Epoch 132/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3134 - val_loss: 1.3174\n",
            "Epoch 133/250\n",
            "4950/4950 [==============================] - 2s 434us/step - loss: 1.3133 - val_loss: 1.3172\n",
            "Epoch 134/250\n",
            "4950/4950 [==============================] - 2s 431us/step - loss: 1.3134 - val_loss: 1.3168\n",
            "Epoch 135/250\n",
            "4950/4950 [==============================] - 2s 429us/step - loss: 1.3132 - val_loss: 1.3172\n",
            "Epoch 136/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3132 - val_loss: 1.3169\n",
            "Epoch 137/250\n",
            "4950/4950 [==============================] - 2s 438us/step - loss: 1.3133 - val_loss: 1.3170\n",
            "Epoch 138/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3132 - val_loss: 1.3176\n",
            "Epoch 139/250\n",
            "4950/4950 [==============================] - 2s 443us/step - loss: 1.3132 - val_loss: 1.3169\n",
            "Epoch 140/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3131 - val_loss: 1.3165\n",
            "Epoch 141/250\n",
            "4950/4950 [==============================] - 2s 432us/step - loss: 1.3132 - val_loss: 1.3166\n",
            "Epoch 142/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3131 - val_loss: 1.3169\n",
            "Epoch 143/250\n",
            "4950/4950 [==============================] - 2s 436us/step - loss: 1.3130 - val_loss: 1.3167\n",
            "Epoch 144/250\n",
            "4950/4950 [==============================] - 2s 433us/step - loss: 1.3130 - val_loss: 1.3167\n",
            "Epoch 145/250\n",
            "4950/4950 [==============================] - 2s 432us/step - loss: 1.3130 - val_loss: 1.3165\n",
            "Epoch 146/250\n",
            "4950/4950 [==============================] - 2s 436us/step - loss: 1.3130 - val_loss: 1.3165\n",
            "Epoch 147/250\n",
            "4950/4950 [==============================] - 2s 439us/step - loss: 1.3131 - val_loss: 1.3165\n",
            "Epoch 148/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 1.3129 - val_loss: 1.3177\n",
            "Epoch 149/250\n",
            "4950/4950 [==============================] - 2s 439us/step - loss: 1.3130 - val_loss: 1.3169\n",
            "Epoch 150/250\n",
            "4950/4950 [==============================] - 2s 438us/step - loss: 1.3129 - val_loss: 1.3165\n",
            "Epoch 151/250\n",
            "4950/4950 [==============================] - 2s 433us/step - loss: 1.3130 - val_loss: 1.3164\n",
            "Epoch 152/250\n",
            "4950/4950 [==============================] - 2s 434us/step - loss: 1.3129 - val_loss: 1.3166\n",
            "Epoch 153/250\n",
            "4950/4950 [==============================] - 2s 431us/step - loss: 1.3129 - val_loss: 1.3164\n",
            "Epoch 154/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 1.3129 - val_loss: 1.3164\n",
            "Epoch 155/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 1.3128 - val_loss: 1.3163\n",
            "Epoch 156/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 1.3129 - val_loss: 1.3163\n",
            "Epoch 157/250\n",
            "4950/4950 [==============================] - 2s 432us/step - loss: 1.3130 - val_loss: 1.3165\n",
            "Epoch 158/250\n",
            "4950/4950 [==============================] - 2s 436us/step - loss: 1.3128 - val_loss: 1.3163\n",
            "Epoch 159/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3128 - val_loss: 1.3164\n",
            "Epoch 160/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 1.3128 - val_loss: 1.3164\n",
            "Epoch 161/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3129 - val_loss: 1.3163\n",
            "Epoch 162/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3128 - val_loss: 1.3163\n",
            "Epoch 163/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3128 - val_loss: 1.3164\n",
            "Epoch 164/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3128 - val_loss: 1.3163\n",
            "Epoch 165/250\n",
            "4950/4950 [==============================] - 2s 429us/step - loss: 1.3127 - val_loss: 1.3160\n",
            "Epoch 166/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3128 - val_loss: 1.3166\n",
            "Epoch 167/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3128 - val_loss: 1.3160\n",
            "Epoch 168/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3127 - val_loss: 1.3162\n",
            "Epoch 169/250\n",
            "4950/4950 [==============================] - 2s 429us/step - loss: 1.3126 - val_loss: 1.3162\n",
            "Epoch 170/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 1.3126 - val_loss: 1.3159\n",
            "Epoch 171/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3126 - val_loss: 1.3160\n",
            "Epoch 172/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3126 - val_loss: 1.3163\n",
            "Epoch 173/250\n",
            "4950/4950 [==============================] - 2s 430us/step - loss: 1.3128 - val_loss: 1.3164\n",
            "Epoch 174/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3126 - val_loss: 1.3160\n",
            "Epoch 175/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3127 - val_loss: 1.3160\n",
            "Epoch 176/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3126 - val_loss: 1.3163\n",
            "Epoch 177/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 1.3126 - val_loss: 1.3160\n",
            "Epoch 178/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 1.3126 - val_loss: 1.3159\n",
            "Epoch 179/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 1.3126 - val_loss: 1.3160\n",
            "Epoch 180/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 1.3126 - val_loss: 1.3160\n",
            "Epoch 181/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 1.3125 - val_loss: 1.3162\n",
            "Epoch 182/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.3126 - val_loss: 1.3160\n",
            "Epoch 183/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3125 - val_loss: 1.3162\n",
            "Epoch 184/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 1.3124 - val_loss: 1.3159\n",
            "Epoch 185/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3125 - val_loss: 1.3161\n",
            "Epoch 186/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3125 - val_loss: 1.3157\n",
            "Epoch 187/250\n",
            "4950/4950 [==============================] - 2s 433us/step - loss: 1.3125 - val_loss: 1.3157\n",
            "Epoch 188/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 1.3124 - val_loss: 1.3156\n",
            "Epoch 189/250\n",
            "4950/4950 [==============================] - 2s 430us/step - loss: 1.3124 - val_loss: 1.3157\n",
            "Epoch 190/250\n",
            "4950/4950 [==============================] - 2s 430us/step - loss: 1.3124 - val_loss: 1.3157\n",
            "Epoch 191/250\n",
            "4950/4950 [==============================] - 2s 431us/step - loss: 1.3124 - val_loss: 1.3158\n",
            "Epoch 192/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3124 - val_loss: 1.3157\n",
            "Epoch 193/250\n",
            "4950/4950 [==============================] - 2s 431us/step - loss: 1.3124 - val_loss: 1.3159\n",
            "Epoch 194/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 1.3123 - val_loss: 1.3161\n",
            "Epoch 195/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3123 - val_loss: 1.3158\n",
            "Epoch 196/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3124 - val_loss: 1.3158\n",
            "Epoch 197/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3126 - val_loss: 1.3159\n",
            "Epoch 198/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3124 - val_loss: 1.3164\n",
            "Epoch 199/250\n",
            "4950/4950 [==============================] - 2s 430us/step - loss: 1.3123 - val_loss: 1.3160\n",
            "Epoch 200/250\n",
            "4950/4950 [==============================] - 2s 431us/step - loss: 1.3123 - val_loss: 1.3156\n",
            "Epoch 201/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3122 - val_loss: 1.3160\n",
            "Epoch 202/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 1.3122 - val_loss: 1.3155\n",
            "Epoch 203/250\n",
            "4950/4950 [==============================] - 2s 437us/step - loss: 1.3121 - val_loss: 1.3155\n",
            "Epoch 204/250\n",
            "4950/4950 [==============================] - 2s 433us/step - loss: 1.3124 - val_loss: 1.3154\n",
            "Epoch 205/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3122 - val_loss: 1.3154\n",
            "Epoch 206/250\n",
            "4950/4950 [==============================] - 2s 433us/step - loss: 1.3121 - val_loss: 1.3154\n",
            "Epoch 207/250\n",
            "4950/4950 [==============================] - 2s 437us/step - loss: 1.3123 - val_loss: 1.3160\n",
            "Epoch 208/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.3124 - val_loss: 1.3157\n",
            "Epoch 209/250\n",
            "4950/4950 [==============================] - 2s 432us/step - loss: 1.3123 - val_loss: 1.3161\n",
            "Epoch 210/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3122 - val_loss: 1.3155\n",
            "Epoch 211/250\n",
            "4950/4950 [==============================] - 2s 434us/step - loss: 1.3123 - val_loss: 1.3153\n",
            "Epoch 212/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 1.3121 - val_loss: 1.3154\n",
            "Epoch 213/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3121 - val_loss: 1.3155\n",
            "Epoch 214/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 1.3121 - val_loss: 1.3153\n",
            "Epoch 215/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 1.3121 - val_loss: 1.3152\n",
            "Epoch 216/250\n",
            "4950/4950 [==============================] - 2s 432us/step - loss: 1.3122 - val_loss: 1.3153\n",
            "Epoch 217/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 1.3121 - val_loss: 1.3160\n",
            "Epoch 218/250\n",
            "4950/4950 [==============================] - 2s 437us/step - loss: 1.3121 - val_loss: 1.3153\n",
            "Epoch 219/250\n",
            "4950/4950 [==============================] - 2s 441us/step - loss: 1.3120 - val_loss: 1.3154\n",
            "Epoch 220/250\n",
            "4950/4950 [==============================] - 2s 441us/step - loss: 1.3121 - val_loss: 1.3155\n",
            "Epoch 221/250\n",
            "4950/4950 [==============================] - 2s 434us/step - loss: 1.3121 - val_loss: 1.3154\n",
            "Epoch 222/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 1.3122 - val_loss: 1.3152\n",
            "Epoch 223/250\n",
            "4950/4950 [==============================] - 2s 437us/step - loss: 1.3121 - val_loss: 1.3152\n",
            "Epoch 224/250\n",
            "4950/4950 [==============================] - 2s 439us/step - loss: 1.3120 - val_loss: 1.3154\n",
            "Epoch 225/250\n",
            "4950/4950 [==============================] - 2s 439us/step - loss: 1.3120 - val_loss: 1.3152\n",
            "Epoch 226/250\n",
            "4950/4950 [==============================] - 2s 436us/step - loss: 1.3119 - val_loss: 1.3151\n",
            "Epoch 227/250\n",
            "4950/4950 [==============================] - 2s 439us/step - loss: 1.3120 - val_loss: 1.3152\n",
            "Epoch 228/250\n",
            "4950/4950 [==============================] - 2s 434us/step - loss: 1.3120 - val_loss: 1.3150\n",
            "Epoch 229/250\n",
            "4950/4950 [==============================] - 2s 441us/step - loss: 1.3120 - val_loss: 1.3150\n",
            "Epoch 230/250\n",
            "4950/4950 [==============================] - 2s 433us/step - loss: 1.3120 - val_loss: 1.3150\n",
            "Epoch 231/250\n",
            "4950/4950 [==============================] - 2s 441us/step - loss: 1.3120 - val_loss: 1.3151\n",
            "Epoch 232/250\n",
            "4950/4950 [==============================] - 2s 436us/step - loss: 1.3120 - val_loss: 1.3150\n",
            "Epoch 233/250\n",
            "4950/4950 [==============================] - 2s 436us/step - loss: 1.3119 - val_loss: 1.3154\n",
            "Epoch 234/250\n",
            "4950/4950 [==============================] - 2s 437us/step - loss: 1.3121 - val_loss: 1.3153\n",
            "Epoch 235/250\n",
            "4950/4950 [==============================] - 2s 440us/step - loss: 1.3120 - val_loss: 1.3153\n",
            "Epoch 236/250\n",
            "4950/4950 [==============================] - 2s 439us/step - loss: 1.3120 - val_loss: 1.3151\n",
            "Epoch 237/250\n",
            "4950/4950 [==============================] - 2s 442us/step - loss: 1.3119 - val_loss: 1.3150\n",
            "Epoch 238/250\n",
            "4950/4950 [==============================] - 2s 441us/step - loss: 1.3119 - val_loss: 1.3150\n",
            "Epoch 239/250\n",
            "4950/4950 [==============================] - 2s 441us/step - loss: 1.3118 - val_loss: 1.3149\n",
            "Epoch 240/250\n",
            "4950/4950 [==============================] - 2s 438us/step - loss: 1.3118 - val_loss: 1.3150\n",
            "Epoch 241/250\n",
            "4950/4950 [==============================] - 2s 437us/step - loss: 1.3118 - val_loss: 1.3151\n",
            "Epoch 242/250\n",
            "4950/4950 [==============================] - 2s 440us/step - loss: 1.3118 - val_loss: 1.3154\n",
            "Epoch 243/250\n",
            "4950/4950 [==============================] - 2s 437us/step - loss: 1.3119 - val_loss: 1.3154\n",
            "Epoch 244/250\n",
            "4950/4950 [==============================] - 2s 436us/step - loss: 1.3119 - val_loss: 1.3154\n",
            "Epoch 245/250\n",
            "4950/4950 [==============================] - 2s 438us/step - loss: 1.3119 - val_loss: 1.3149\n",
            "Epoch 246/250\n",
            "4950/4950 [==============================] - 2s 442us/step - loss: 1.3118 - val_loss: 1.3150\n",
            "Epoch 247/250\n",
            "4950/4950 [==============================] - 2s 432us/step - loss: 1.3118 - val_loss: 1.3151\n",
            "Epoch 248/250\n",
            "4950/4950 [==============================] - 2s 437us/step - loss: 1.3119 - val_loss: 1.3150\n",
            "Epoch 249/250\n",
            "4950/4950 [==============================] - 2s 439us/step - loss: 1.3119 - val_loss: 1.3149\n",
            "Epoch 250/250\n",
            "4950/4950 [==============================] - 2s 436us/step - loss: 1.3118 - val_loss: 1.3150\n",
            "(lamda,Threshold) 0.0 0.0\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (5500, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 16895991 0.0\n",
            "The shape of N (5500, 3072)\n",
            "The minimum value of N  -0.93208873\n",
            "The max value of N 0.80473864\n",
            "[INFO:] Xclean  MSE Computed shape (5500, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (5500, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0062092817])\n",
            "[INFO:] The anomaly threshold computed is  0.0062092817\n",
            "[INFO:] The shape of input data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (5500, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [2999, 5079, 5330, 5103, 5049, 5461, 5099, 4505, 5105, 1083, 5021, 5226, 2791, 2815, 5396, 5166, 1857, 5065, 5312, 5460, 4724, 3029, 1387, 5032, 5367, 4551, 4308, 5360, 5427, 5001, 4437, 723, 5160, 5357, 1404, 5493, 5270, 5200, 1466, 3826, 5088, 2419, 5114, 86, 5205, 1258, 58, 5248, 5433, 5071, 5133, 1396, 370, 3526, 4839, 3187, 1841, 5239, 900, 5293, 5269, 5324, 4065, 4284, 2487, 882, 2264, 2831, 5214, 2557, 805, 5222, 5432, 1973, 1189, 1796, 616, 5147, 5206, 3530, 3852, 3779, 5060, 630, 1612, 2703, 3902, 5136, 5014, 5306, 2340, 5321, 5255, 5486, 5311, 5059, 5285, 379, 1633, 5011, 5278, 4520, 1947, 3709, 714, 1632, 3678, 193, 5438, 3643, 3214, 403, 5007, 5092, 5068, 4980, 3305, 3697, 2850, 4112, 5043, 4807, 969, 1888, 2399, 5304, 3863, 5364, 5078, 5045, 5332, 1425, 5076, 5051, 1454, 5354, 4146, 1603, 2236, 3980, 5131, 5382, 196, 2561, 5104, 2407, 4158, 5186, 955, 3841, 5351, 5277, 3435, 2808, 5127, 1597, 3840, 5431, 5146, 2728, 458, 5459, 5450, 499, 5325, 5212, 4509, 2888, 168, 5375, 3534, 745, 5385, 2659, 4486, 5005, 5061, 4089, 27, 5300, 5188, 1979, 1531, 5468, 5161, 3925, 5386, 1925, 5202, 1874, 3199, 3069, 5261, 1691, 3723, 1884, 5257, 3232, 806, 5495, 3949, 1050, 3892, 2172, 312, 5397, 5158, 5471, 2339, 4722, 9, 5055, 5084, 5482, 1552, 5374, 5430, 5394, 1227, 2577, 1749, 5404, 4659, 5308, 3759, 4682, 947, 4728, 3528, 1190, 5176, 1127, 5052, 539, 3332, 2925, 5410, 5424, 5096, 5265, 1590, 2290, 3308, 5029, 2521, 4762, 1627, 5279, 2558, 59, 5340, 5113, 5465, 4802, 3850, 420, 5436, 5496, 3554, 1689, 5227, 2715, 687, 5318, 613, 2593, 1630, 3744, 3074, 5236, 2781, 698, 5492, 5143, 4703, 5004, 4538, 5307, 1248, 683, 3, 5024, 5112, 3712, 3411, 1136, 2878, 2544, 5201, 5190, 2799, 2783, 4864, 5462, 5331, 4514, 5454, 2643, 3439, 1812, 5154, 3156, 4029, 895, 5192, 5140, 2913, 5470, 5155, 4229, 3195, 1174, 2865, 4546, 4059, 2314, 3014, 4406, 2542, 2992, 4249, 5064, 1776, 5044, 145, 5230, 1766, 4403, 5095, 5083, 1161, 5282, 3661, 4627, 4011, 5479, 2908, 4438, 576, 2090, 5488, 3243, 1444, 2962, 826, 5164, 128, 513, 5352, 4170, 3650, 954, 1688, 4890, 5415, 2012, 5487, 4877, 1260, 5422, 3172, 3346, 4591, 5476, 3979, 1826, 4870, 5094, 3544, 5225, 5072, 2966, 1301, 3296, 1519, 5494, 960, 99, 3510, 4988, 2813, 5366, 5428, 757, 5219, 1518, 4667, 3991, 477, 1637, 5168, 2034, 2699, 4460, 5204, 3448, 3151, 1737, 4706, 965, 4318, 502, 2207, 4019, 5211, 1543, 939, 1433, 681, 139, 4883, 3268, 3036, 3395, 5137, 5464, 2051, 2883, 3073, 5319, 5046, 5125, 1968, 3553, 1095, 1945, 3714, 2131, 454, 4102, 216, 888, 2652, 604, 4455, 4061, 3830, 1949, 859, 5134, 5437, 1729, 940, 4452, 4813, 4047, 4861, 5466, 3414, 4596, 2379, 2268, 5215, 4482, 2472, 2317, 2460, 4691, 4295, 5128, 5322, 2663, 1109, 127, 5008, 3422, 3962, 4466, 3466, 3221, 5457, 24, 5281, 1473, 2315, 5370, 5377, 1658, 3632, 5435, 692, 557, 3657, 1906, 2366, 2320, 1699, 3518, 517, 3988, 945, 2140, 3237, 501, 3437, 5273, 1071, 5259, 740, 355]\n",
            "=====================\n",
            "AUROC 0.0 0.677\n",
            "=======================\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 128) (256, 128)\n",
            "Train on 4950 samples, validate on 550 samples\n",
            "Epoch 1/250\n",
            "4950/4950 [==============================] - 5s 1ms/step - loss: 1.6257 - val_loss: 1.6289\n",
            "Epoch 2/250\n",
            "4950/4950 [==============================] - 2s 478us/step - loss: 1.6256 - val_loss: 1.6290\n",
            "Epoch 3/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6256 - val_loss: 1.6293\n",
            "Epoch 4/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6256 - val_loss: 1.6288\n",
            "Epoch 5/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6255 - val_loss: 1.6289\n",
            "Epoch 6/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6255 - val_loss: 1.6288\n",
            "Epoch 7/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6255 - val_loss: 1.6288\n",
            "Epoch 8/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 1.6255 - val_loss: 1.6287\n",
            "Epoch 9/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6255 - val_loss: 1.6288\n",
            "Epoch 10/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6255 - val_loss: 1.6287\n",
            "Epoch 11/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6255 - val_loss: 1.6290\n",
            "Epoch 12/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 1.6257 - val_loss: 1.6292\n",
            "Epoch 13/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.6255 - val_loss: 1.6288\n",
            "Epoch 14/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6256 - val_loss: 1.6287\n",
            "Epoch 15/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 1.6254 - val_loss: 1.6289\n",
            "Epoch 16/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6255 - val_loss: 1.6287\n",
            "Epoch 17/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6255 - val_loss: 1.6287\n",
            "Epoch 18/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6255 - val_loss: 1.6287\n",
            "Epoch 19/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6254 - val_loss: 1.6287\n",
            "Epoch 20/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6255 - val_loss: 1.6287\n",
            "Epoch 21/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6254 - val_loss: 1.6286\n",
            "Epoch 22/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6254 - val_loss: 1.6287\n",
            "Epoch 23/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6253 - val_loss: 1.6286\n",
            "Epoch 24/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6254 - val_loss: 1.6286\n",
            "Epoch 25/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6254 - val_loss: 1.6287\n",
            "Epoch 26/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.6254 - val_loss: 1.6286\n",
            "Epoch 27/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.6254 - val_loss: 1.6286\n",
            "Epoch 28/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.6254 - val_loss: 1.6289\n",
            "Epoch 29/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6254 - val_loss: 1.6284\n",
            "Epoch 30/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6253 - val_loss: 1.6287\n",
            "Epoch 31/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6254 - val_loss: 1.6285\n",
            "Epoch 32/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6253 - val_loss: 1.6284\n",
            "Epoch 33/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6253 - val_loss: 1.6286\n",
            "Epoch 34/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6253 - val_loss: 1.6284\n",
            "Epoch 35/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6253 - val_loss: 1.6284\n",
            "Epoch 36/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 1.6253 - val_loss: 1.6284\n",
            "Epoch 37/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6254 - val_loss: 1.6284\n",
            "Epoch 38/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6254 - val_loss: 1.6284\n",
            "Epoch 39/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6254 - val_loss: 1.6284\n",
            "Epoch 40/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 1.6253 - val_loss: 1.6283\n",
            "Epoch 41/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6254 - val_loss: 1.6287\n",
            "Epoch 42/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6253 - val_loss: 1.6284\n",
            "Epoch 43/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6253 - val_loss: 1.6284\n",
            "Epoch 44/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 1.6252 - val_loss: 1.6286\n",
            "Epoch 45/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6252 - val_loss: 1.6285\n",
            "Epoch 46/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6253 - val_loss: 1.6284\n",
            "Epoch 47/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 1.6252 - val_loss: 1.6284\n",
            "Epoch 48/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 1.6253 - val_loss: 1.6283\n",
            "Epoch 49/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6252 - val_loss: 1.6285\n",
            "Epoch 50/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6252 - val_loss: 1.6284\n",
            "Epoch 51/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6252 - val_loss: 1.6282\n",
            "Epoch 52/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6252 - val_loss: 1.6284\n",
            "Epoch 53/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6252 - val_loss: 1.6287\n",
            "Epoch 54/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6253 - val_loss: 1.6286\n",
            "Epoch 55/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6252 - val_loss: 1.6285\n",
            "Epoch 56/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6251 - val_loss: 1.6282\n",
            "Epoch 57/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6252 - val_loss: 1.6281\n",
            "Epoch 58/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6252 - val_loss: 1.6286\n",
            "Epoch 59/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6252 - val_loss: 1.6282\n",
            "Epoch 60/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 1.6251 - val_loss: 1.6283\n",
            "Epoch 61/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.6251 - val_loss: 1.6284\n",
            "Epoch 62/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6251 - val_loss: 1.6283\n",
            "Epoch 63/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6252 - val_loss: 1.6282\n",
            "Epoch 64/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6251 - val_loss: 1.6282\n",
            "Epoch 65/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6252 - val_loss: 1.6282\n",
            "Epoch 66/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6252 - val_loss: 1.6283\n",
            "Epoch 67/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6252 - val_loss: 1.6283\n",
            "Epoch 68/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6251 - val_loss: 1.6281\n",
            "Epoch 69/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 1.6251 - val_loss: 1.6283\n",
            "Epoch 70/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 1.6251 - val_loss: 1.6281\n",
            "Epoch 71/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6251 - val_loss: 1.6283\n",
            "Epoch 72/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 1.6251 - val_loss: 1.6283\n",
            "Epoch 73/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6251 - val_loss: 1.6281\n",
            "Epoch 74/250\n",
            "4950/4950 [==============================] - 2s 405us/step - loss: 1.6250 - val_loss: 1.6281\n",
            "Epoch 75/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6251 - val_loss: 1.6283\n",
            "Epoch 76/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6250 - val_loss: 1.6283\n",
            "Epoch 77/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6250 - val_loss: 1.6284\n",
            "Epoch 78/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.6251 - val_loss: 1.6282\n",
            "Epoch 79/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.6251 - val_loss: 1.6284\n",
            "Epoch 80/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 1.6251 - val_loss: 1.6282\n",
            "Epoch 81/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6252 - val_loss: 1.6282\n",
            "Epoch 82/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6251 - val_loss: 1.6282\n",
            "Epoch 83/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6250 - val_loss: 1.6282\n",
            "Epoch 84/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6249 - val_loss: 1.6280\n",
            "Epoch 85/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6250 - val_loss: 1.6281\n",
            "Epoch 86/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 1.6250 - val_loss: 1.6280\n",
            "Epoch 87/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6250 - val_loss: 1.6280\n",
            "Epoch 88/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6250 - val_loss: 1.6281\n",
            "Epoch 89/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6250 - val_loss: 1.6282\n",
            "Epoch 90/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6249 - val_loss: 1.6279\n",
            "Epoch 91/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6250 - val_loss: 1.6281\n",
            "Epoch 92/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6250 - val_loss: 1.6284\n",
            "Epoch 93/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6250 - val_loss: 1.6282\n",
            "Epoch 94/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 1.6250 - val_loss: 1.6282\n",
            "Epoch 95/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 1.6249 - val_loss: 1.6280\n",
            "Epoch 96/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.6250 - val_loss: 1.6282\n",
            "Epoch 97/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6250 - val_loss: 1.6282\n",
            "Epoch 98/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6251 - val_loss: 1.6285\n",
            "Epoch 99/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 1.6250 - val_loss: 1.6282\n",
            "Epoch 100/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6250 - val_loss: 1.6280\n",
            "Epoch 101/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6249 - val_loss: 1.6280\n",
            "Epoch 102/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6250 - val_loss: 1.6281\n",
            "Epoch 103/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6249 - val_loss: 1.6279\n",
            "Epoch 104/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6249 - val_loss: 1.6281\n",
            "Epoch 105/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6249 - val_loss: 1.6280\n",
            "Epoch 106/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6251 - val_loss: 1.6283\n",
            "Epoch 107/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6249 - val_loss: 1.6279\n",
            "Epoch 108/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 1.6248 - val_loss: 1.6279\n",
            "Epoch 109/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6249 - val_loss: 1.6280\n",
            "Epoch 110/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6248 - val_loss: 1.6280\n",
            "Epoch 111/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 1.6248 - val_loss: 1.6278\n",
            "Epoch 112/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6248 - val_loss: 1.6279\n",
            "Epoch 113/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6249 - val_loss: 1.6280\n",
            "Epoch 114/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6249 - val_loss: 1.6278\n",
            "Epoch 115/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6248 - val_loss: 1.6280\n",
            "Epoch 116/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 1.6249 - val_loss: 1.6279\n",
            "Epoch 117/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6249 - val_loss: 1.6283\n",
            "Epoch 118/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6249 - val_loss: 1.6281\n",
            "Epoch 119/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.6250 - val_loss: 1.6280\n",
            "Epoch 120/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6249 - val_loss: 1.6280\n",
            "Epoch 121/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6249 - val_loss: 1.6281\n",
            "Epoch 122/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6248 - val_loss: 1.6279\n",
            "Epoch 123/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6249 - val_loss: 1.6279\n",
            "Epoch 124/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6249 - val_loss: 1.6280\n",
            "Epoch 125/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6248 - val_loss: 1.6279\n",
            "Epoch 126/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6248 - val_loss: 1.6278\n",
            "Epoch 127/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6248 - val_loss: 1.6280\n",
            "Epoch 128/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 1.6248 - val_loss: 1.6279\n",
            "Epoch 129/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6249 - val_loss: 1.6279\n",
            "Epoch 130/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6248 - val_loss: 1.6279\n",
            "Epoch 131/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6249 - val_loss: 1.6279\n",
            "Epoch 132/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6249 - val_loss: 1.6280\n",
            "Epoch 133/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 1.6249 - val_loss: 1.6279\n",
            "Epoch 134/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6248 - val_loss: 1.6278\n",
            "Epoch 135/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6248 - val_loss: 1.6282\n",
            "Epoch 136/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6248 - val_loss: 1.6278\n",
            "Epoch 137/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6248 - val_loss: 1.6279\n",
            "Epoch 138/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6248 - val_loss: 1.6278\n",
            "Epoch 139/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 1.6247 - val_loss: 1.6277\n",
            "Epoch 140/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6248 - val_loss: 1.6279\n",
            "Epoch 141/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6248 - val_loss: 1.6280\n",
            "Epoch 142/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6248 - val_loss: 1.6278\n",
            "Epoch 143/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 144/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6248 - val_loss: 1.6280\n",
            "Epoch 145/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6248 - val_loss: 1.6280\n",
            "Epoch 146/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6248 - val_loss: 1.6278\n",
            "Epoch 147/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 148/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6248 - val_loss: 1.6279\n",
            "Epoch 149/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 150/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6247 - val_loss: 1.6277\n",
            "Epoch 151/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 152/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6248 - val_loss: 1.6278\n",
            "Epoch 153/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 1.6248 - val_loss: 1.6279\n",
            "Epoch 154/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 155/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 156/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.6247 - val_loss: 1.6279\n",
            "Epoch 157/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 1.6248 - val_loss: 1.6279\n",
            "Epoch 158/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6247 - val_loss: 1.6279\n",
            "Epoch 159/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6247 - val_loss: 1.6279\n",
            "Epoch 160/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6247 - val_loss: 1.6277\n",
            "Epoch 161/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6247 - val_loss: 1.6280\n",
            "Epoch 162/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 1.6247 - val_loss: 1.6277\n",
            "Epoch 163/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 164/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6249 - val_loss: 1.6279\n",
            "Epoch 165/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6247 - val_loss: 1.6277\n",
            "Epoch 166/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 1.6247 - val_loss: 1.6276\n",
            "Epoch 167/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 168/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6248 - val_loss: 1.6278\n",
            "Epoch 169/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6247 - val_loss: 1.6277\n",
            "Epoch 170/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6247 - val_loss: 1.6277\n",
            "Epoch 171/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 1.6247 - val_loss: 1.6277\n",
            "Epoch 172/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 1.6246 - val_loss: 1.6278\n",
            "Epoch 173/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6248 - val_loss: 1.6278\n",
            "Epoch 174/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 175/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6246 - val_loss: 1.6278\n",
            "Epoch 176/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 1.6246 - val_loss: 1.6277\n",
            "Epoch 177/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6246 - val_loss: 1.6277\n",
            "Epoch 178/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 179/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 180/250\n",
            "4950/4950 [==============================] - 2s 405us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 181/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 182/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6246 - val_loss: 1.6278\n",
            "Epoch 183/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6248 - val_loss: 1.6277\n",
            "Epoch 184/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6247 - val_loss: 1.6279\n",
            "Epoch 185/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 1.6246 - val_loss: 1.6281\n",
            "Epoch 186/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 187/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6247 - val_loss: 1.6278\n",
            "Epoch 188/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6247 - val_loss: 1.6279\n",
            "Epoch 189/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 1.6246 - val_loss: 1.6278\n",
            "Epoch 190/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6247 - val_loss: 1.6277\n",
            "Epoch 191/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6247 - val_loss: 1.6277\n",
            "Epoch 192/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 193/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 194/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6247 - val_loss: 1.6277\n",
            "Epoch 195/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 1.6246 - val_loss: 1.6277\n",
            "Epoch 196/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 197/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 1.6246 - val_loss: 1.6277\n",
            "Epoch 198/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 199/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 200/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 1.6245 - val_loss: 1.6275\n",
            "Epoch 201/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 1.6246 - val_loss: 1.6277\n",
            "Epoch 202/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 203/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 204/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6246 - val_loss: 1.6277\n",
            "Epoch 205/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 206/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 207/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 208/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 209/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6246 - val_loss: 1.6278\n",
            "Epoch 210/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 1.6246 - val_loss: 1.6279\n",
            "Epoch 211/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 212/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 213/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 1.6246 - val_loss: 1.6278\n",
            "Epoch 214/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6245 - val_loss: 1.6277\n",
            "Epoch 215/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 1.6246 - val_loss: 1.6277\n",
            "Epoch 216/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 1.6246 - val_loss: 1.6277\n",
            "Epoch 217/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 1.6245 - val_loss: 1.6278\n",
            "Epoch 218/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6247 - val_loss: 1.6279\n",
            "Epoch 219/250\n",
            "4950/4950 [==============================] - 2s 404us/step - loss: 1.6246 - val_loss: 1.6278\n",
            "Epoch 220/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 1.6245 - val_loss: 1.6277\n",
            "Epoch 221/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 222/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6245 - val_loss: 1.6276\n",
            "Epoch 223/250\n",
            "4950/4950 [==============================] - 2s 402us/step - loss: 1.6246 - val_loss: 1.6279\n",
            "Epoch 224/250\n",
            "4950/4950 [==============================] - 2s 402us/step - loss: 1.6245 - val_loss: 1.6275\n",
            "Epoch 225/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 1.6245 - val_loss: 1.6275\n",
            "Epoch 226/250\n",
            "4950/4950 [==============================] - 2s 405us/step - loss: 1.6245 - val_loss: 1.6274\n",
            "Epoch 227/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6246 - val_loss: 1.6275\n",
            "Epoch 228/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 229/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 1.6245 - val_loss: 1.6276\n",
            "Epoch 230/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6245 - val_loss: 1.6276\n",
            "Epoch 231/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6246 - val_loss: 1.6277\n",
            "Epoch 232/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 1.6245 - val_loss: 1.6275\n",
            "Epoch 233/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6245 - val_loss: 1.6277\n",
            "Epoch 234/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 1.6246 - val_loss: 1.6275\n",
            "Epoch 235/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6245 - val_loss: 1.6277\n",
            "Epoch 236/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 237/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6246 - val_loss: 1.6276\n",
            "Epoch 238/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 1.6245 - val_loss: 1.6276\n",
            "Epoch 239/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 1.6245 - val_loss: 1.6277\n",
            "Epoch 240/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 1.6245 - val_loss: 1.6275\n",
            "Epoch 241/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 1.6245 - val_loss: 1.6275\n",
            "Epoch 242/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 1.6245 - val_loss: 1.6275\n",
            "Epoch 243/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6245 - val_loss: 1.6276\n",
            "Epoch 244/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 1.6245 - val_loss: 1.6277\n",
            "Epoch 245/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 1.6245 - val_loss: 1.6276\n",
            "Epoch 246/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 1.6245 - val_loss: 1.6275\n",
            "Epoch 247/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 1.6245 - val_loss: 1.6275\n",
            "Epoch 248/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6244 - val_loss: 1.6275\n",
            "Epoch 249/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 1.6244 - val_loss: 1.6275\n",
            "Epoch 250/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 1.6245 - val_loss: 1.6275\n",
            "(lamda,Threshold) 0.01 0.005\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (5500, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 15478655 0.01\n",
            "The shape of N (5500, 3072)\n",
            "The minimum value of N  -0.8614628314971924\n",
            "The max value of N 0.7455500364303589\n",
            "[INFO:] Xclean  MSE Computed shape (5500, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (5500, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0062092817, 0.0050763334])\n",
            "[INFO:] The anomaly threshold computed is  0.0050763334\n",
            "[INFO:] The shape of input data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (5500, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [5103, 2999, 5330, 5079, 5461, 5049, 5105, 5226, 5166, 5099, 5367, 5114, 1083, 1857, 5200, 2791, 4551, 5001, 5427, 5357, 5493, 5312, 4724, 5032, 4505, 5021, 5396, 5060, 3029, 5460, 5270, 5293, 5071, 5324, 5360, 3187, 5160, 3826, 5433, 5269, 5088, 5133, 1387, 5239, 5014, 5205, 86, 5332, 5065, 1466, 4437, 3530, 1404, 5206, 58, 5364, 1258, 5147, 5214, 370, 1189, 5255, 2831, 5306, 5248, 4839, 2419, 5007, 1796, 5438, 5285, 5136, 1396, 2487, 5059, 5354, 5146, 5386, 1947, 2815, 1841, 5068, 3709, 4980, 5311, 5051, 5131, 5304, 5430, 882, 1425, 4520, 805, 3902, 5092, 5045, 2703, 3199, 5188, 5482, 5382, 5351, 2264, 5104, 5076, 2340, 3526, 969, 27, 5300, 5212, 4284, 5375, 5186, 2808, 5486, 5158, 947, 714, 5450, 2850, 4065, 312, 3643, 5397, 1603, 5004, 5127, 3841, 5011, 5005, 806, 5227, 5374, 196, 5278, 5465, 2557, 900, 955, 5043, 630, 5052, 5222, 5202, 3069, 3214, 5261, 4158, 5078, 3852, 4682, 5432, 4728, 5471, 5257, 3678, 5394, 5459, 3925, 5321, 5492, 2561, 3305, 3892, 4762, 5415, 5404, 5385, 379, 5265, 723, 5236, 2728, 403, 4308, 1973, 5468, 2521, 5161, 5112, 4089, 5143, 2888, 616, 1597, 1979, 1552, 1925, 5061, 1190, 4659, 3863, 1531, 5140, 5225, 5410, 5318, 5154, 3850, 3980, 1612, 2659, 5308, 5084, 2577, 954, 5096, 3, 5024, 5395, 2314, 5176, 5436, 5331, 3232, 193, 5470, 3332, 2339, 5366, 613, 4807, 2236, 499, 5204, 1632, 5044, 5094, 3074, 5277, 2715, 5422, 9, 5055, 1888, 1227, 1630, 3296, 5428, 5325, 3435, 3759, 1776, 5307, 3840, 3528, 539, 5064, 2399, 3661, 5113, 5319, 4146, 5095, 2643, 1633, 1136, 3411, 1689, 4509, 3697, 5464, 5310, 1174, 59, 5282, 168, 3534, 2925, 1444, 5219, 745, 1433, 4029, 3949, 1248, 3723, 4864, 5494, 2781, 4627, 3308, 4406, 5431, 5495, 4988, 757, 5340, 1688, 513, 1454, 5339, 2799, 5437, 2544, 683, 5201, 1627, 5290, 3712, 2542, 458, 5377, 5424, 2699, 1590, 2558, 1050, 5295, 1691, 4249, 4102, 5211, 5462, 3544, 1161, 2813, 2966, 5488, 4890, 4974, 4229, 2034, 5046, 5038, 3744, 3243, 2407, 5128, 5230, 1949, 4482, 3268, 1884, 5029, 3195, 1874, 2593, 5072, 4035, 895, 4722, 4538, 4047, 2992, 5164, 2012, 3991, 3979, 3156, 5454, 3346, 5168, 5466, 4438, 4883, 2962, 3466, 5119, 3430, 4019, 496, 1779, 1637, 2090, 4591, 5391, 5279, 5019, 3095, 3172, 3014, 2320, 5155, 4986, 5451, 4059, 5445, 4318, 4467, 1260, 5496, 681, 4112, 4703, 1301, 2021, 5192, 502, 4377, 2290, 2000, 4514, 5228, 4802, 4287, 2865, 1598, 128, 3554, 2051, 454, 1793, 5137, 3518, 731, 5329, 2672, 2472, 2652, 4486, 3510, 1628, 5008, 2663, 355, 4546, 5323, 5389, 710, 3632, 5378, 1742, 2783, 3822, 4645, 4466, 1264, 3221, 5370, 3779, 2207, 3395, 4490, 1851, 2172, 5118, 180, 4011, 3690, 1512, 3988, 99, 5487, 5301, 3414, 4539, 5109, 4706, 1975, 4976, 740, 1543, 3714, 5208, 2878, 1326, 5125, 5056, 5322, 4691, 3696, 1996, 1766, 1502, 2140, 5258, 1963, 4295, 2571, 1249, 1749, 4764, 420, 4169, 5297, 3657, 3415, 501, 5352, 3650, 4813, 3568, 4592, 939, 5273, 2394, 5074, 5073, 2908, 3439, 3167, 260, 5028, 960, 4996, 524, 3201, 3322]\n",
            "=====================\n",
            "AUROC 0.01 0.6930000000000001\n",
            "=======================\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 128) (256, 128)\n",
            "Train on 4950 samples, validate on 550 samples\n",
            "Epoch 1/250\n",
            "4950/4950 [==============================] - 6s 1ms/step - loss: 3.3768 - val_loss: 3.3798\n",
            "Epoch 2/250\n",
            "4950/4950 [==============================] - 3s 552us/step - loss: 3.3768 - val_loss: 3.3797\n",
            "Epoch 3/250\n",
            "4950/4950 [==============================] - 2s 468us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 4/250\n",
            "4950/4950 [==============================] - 2s 440us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 5/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 6/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 7/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3767 - val_loss: 3.3797\n",
            "Epoch 8/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 9/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3767 - val_loss: 3.3799\n",
            "Epoch 10/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 11/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 12/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 13/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 3.3767 - val_loss: 3.3797\n",
            "Epoch 14/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 15/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 16/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3767 - val_loss: 3.3799\n",
            "Epoch 17/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3767 - val_loss: 3.3797\n",
            "Epoch 18/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3767 - val_loss: 3.3797\n",
            "Epoch 19/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3767 - val_loss: 3.3797\n",
            "Epoch 20/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 21/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3767 - val_loss: 3.3797\n",
            "Epoch 22/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3767 - val_loss: 3.3797\n",
            "Epoch 23/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 24/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 25/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 26/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3767 - val_loss: 3.3797\n",
            "Epoch 27/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 28/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 29/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3768 - val_loss: 3.3798\n",
            "Epoch 30/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.3767 - val_loss: 3.3797\n",
            "Epoch 31/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 32/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 33/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 34/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 35/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 36/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3767 - val_loss: 3.3797\n",
            "Epoch 37/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 38/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3767 - val_loss: 3.3797\n",
            "Epoch 39/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 40/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3767 - val_loss: 3.3796\n",
            "Epoch 41/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 42/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3767 - val_loss: 3.3797\n",
            "Epoch 43/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3767 - val_loss: 3.3800\n",
            "Epoch 44/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 45/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 46/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 47/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3767 - val_loss: 3.3796\n",
            "Epoch 48/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 49/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 50/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 51/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 52/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 53/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3767 - val_loss: 3.3799\n",
            "Epoch 54/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 55/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 56/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 57/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 58/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 59/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 3.3767 - val_loss: 3.3799\n",
            "Epoch 60/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 61/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 62/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 63/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 3.3767 - val_loss: 3.3800\n",
            "Epoch 64/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 65/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 66/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 67/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 68/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 69/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 70/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 71/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 72/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 73/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 74/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 75/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 76/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 77/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3765 - val_loss: 3.3798\n",
            "Epoch 78/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 79/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3767 - val_loss: 3.3797\n",
            "Epoch 80/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 81/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 82/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 83/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 84/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 85/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3765 - val_loss: 3.3797\n",
            "Epoch 86/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 87/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 88/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 89/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 90/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 91/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 92/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 93/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 94/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 95/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 96/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 97/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 98/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 99/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 100/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3766 - val_loss: 3.3798\n",
            "Epoch 101/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3765 - val_loss: 3.3797\n",
            "Epoch 102/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 103/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3765 - val_loss: 3.3797\n",
            "Epoch 104/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 105/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.3765 - val_loss: 3.3797\n",
            "Epoch 106/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 107/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3767 - val_loss: 3.3798\n",
            "Epoch 108/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 109/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 110/250\n",
            "4950/4950 [==============================] - 2s 405us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 111/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 112/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3766 - val_loss: 3.3795\n",
            "Epoch 113/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3765 - val_loss: 3.3797\n",
            "Epoch 114/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 115/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 116/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 117/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 3.3765 - val_loss: 3.3797\n",
            "Epoch 118/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3765 - val_loss: 3.3797\n",
            "Epoch 119/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3766 - val_loss: 3.3797\n",
            "Epoch 120/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 121/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.3765 - val_loss: 3.3799\n",
            "Epoch 122/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3765 - val_loss: 3.3797\n",
            "Epoch 123/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 124/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 125/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3765 - val_loss: 3.3798\n",
            "Epoch 126/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 127/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 128/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 129/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 130/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3766 - val_loss: 3.3795\n",
            "Epoch 131/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 132/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3765 - val_loss: 3.3797\n",
            "Epoch 133/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3765 - val_loss: 3.3798\n",
            "Epoch 134/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3765 - val_loss: 3.3797\n",
            "Epoch 135/250\n",
            "4950/4950 [==============================] - 2s 405us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 136/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 137/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 138/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 139/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 140/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 141/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 142/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 143/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 144/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 145/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3765 - val_loss: 3.3797\n",
            "Epoch 146/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3766 - val_loss: 3.3796\n",
            "Epoch 147/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 148/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 149/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 150/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.3765 - val_loss: 3.3797\n",
            "Epoch 151/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 152/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 153/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 154/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 155/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3765 - val_loss: 3.3797\n",
            "Epoch 156/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 157/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 158/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 159/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 160/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 161/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 162/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3764 - val_loss: 3.3794\n",
            "Epoch 163/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 164/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 165/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 166/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3765 - val_loss: 3.3799\n",
            "Epoch 167/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 168/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 169/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 170/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 171/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 172/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 173/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 174/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3764 - val_loss: 3.3797\n",
            "Epoch 175/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 176/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 177/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 178/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 179/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 180/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 181/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 182/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3764 - val_loss: 3.3798\n",
            "Epoch 183/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 184/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 185/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 186/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 187/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3764 - val_loss: 3.3797\n",
            "Epoch 188/250\n",
            "4950/4950 [==============================] - 2s 402us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 189/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 190/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 191/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3764 - val_loss: 3.3797\n",
            "Epoch 192/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 193/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 194/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3765 - val_loss: 3.3794\n",
            "Epoch 195/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 196/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3764 - val_loss: 3.3797\n",
            "Epoch 197/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3764 - val_loss: 3.3794\n",
            "Epoch 198/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 199/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 200/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 201/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3764 - val_loss: 3.3794\n",
            "Epoch 202/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 203/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 204/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 205/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 206/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 207/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3764 - val_loss: 3.3794\n",
            "Epoch 208/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 209/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 210/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 211/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3763 - val_loss: 3.3795\n",
            "Epoch 212/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 213/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 214/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3764 - val_loss: 3.3797\n",
            "Epoch 215/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3764 - val_loss: 3.3797\n",
            "Epoch 216/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3763 - val_loss: 3.3796\n",
            "Epoch 217/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 218/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 219/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 220/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3764 - val_loss: 3.3794\n",
            "Epoch 221/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 222/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 223/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 224/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3764 - val_loss: 3.3797\n",
            "Epoch 225/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3764 - val_loss: 3.3797\n",
            "Epoch 226/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.3764 - val_loss: 3.3794\n",
            "Epoch 227/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 228/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3764 - val_loss: 3.3797\n",
            "Epoch 229/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 230/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.3763 - val_loss: 3.3795\n",
            "Epoch 231/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3763 - val_loss: 3.3795\n",
            "Epoch 232/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 233/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 234/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.3765 - val_loss: 3.3796\n",
            "Epoch 235/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.3763 - val_loss: 3.3796\n",
            "Epoch 236/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 237/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 238/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 239/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 240/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3764 - val_loss: 3.3797\n",
            "Epoch 241/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 242/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.3763 - val_loss: 3.3796\n",
            "Epoch 243/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 244/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 245/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.3763 - val_loss: 3.3795\n",
            "Epoch 246/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3763 - val_loss: 3.3795\n",
            "Epoch 247/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.3764 - val_loss: 3.3796\n",
            "Epoch 248/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.3765 - val_loss: 3.3795\n",
            "Epoch 249/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.3764 - val_loss: 3.3795\n",
            "Epoch 250/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.3764 - val_loss: 3.3794\n",
            "(lamda,Threshold) 0.1 0.05\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (5500, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 5883928 0.1\n",
            "The shape of N (5500, 3072)\n",
            "The minimum value of N  -0.7873380780220032\n",
            "The max value of N 0.6795176267623901\n",
            "[INFO:] Xclean  MSE Computed shape (5500, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (5500, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0062092817, 0.0050763334, 0.0046889368])\n",
            "[INFO:] The anomaly threshold computed is  0.0046889368\n",
            "[INFO:] The shape of input data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (5500, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [2999, 5330, 5103, 5079, 5461, 5049, 5226, 5166, 5312, 5105, 5099, 5367, 5114, 5357, 5200, 5001, 5396, 5293, 4505, 5493, 5433, 5427, 1857, 4551, 5032, 1083, 2791, 5065, 5060, 5460, 5071, 5160, 4724, 3187, 5360, 5239, 5285, 5214, 5088, 5332, 5206, 5269, 3029, 5147, 3826, 5014, 5270, 5051, 5136, 86, 5133, 3530, 5248, 5059, 5306, 5438, 5146, 1387, 5382, 3709, 5255, 1189, 1841, 1947, 5304, 5212, 1466, 2419, 5386, 5311, 5205, 5092, 1258, 5188, 4980, 5021, 5354, 5131, 5486, 5158, 2831, 5364, 1796, 370, 5324, 5004, 3199, 5430, 1396, 5011, 1404, 5471, 805, 4839, 5375, 969, 5078, 58, 2264, 3902, 27, 5300, 5104, 5007, 5351, 1425, 2808, 5450, 196, 4437, 5278, 5068, 5432, 882, 5043, 714, 5052, 947, 3643, 5397, 5076, 5222, 5227, 5005, 5482, 4284, 2703, 2340, 5257, 5202, 806, 5186, 5045, 2487, 5465, 5176, 3925, 5318, 5112, 5385, 3069, 4520, 5468, 5261, 4682, 4308, 312, 2850, 3526, 5084, 1597, 5470, 1603, 5366, 4065, 3852, 4158, 5331, 5265, 3841, 5374, 5395, 403, 5127, 5394, 4974, 1190, 4762, 5436, 3678, 900, 4807, 5143, 5492, 4728, 630, 2815, 5459, 2557, 3, 5024, 4659, 5154, 5494, 5164, 2888, 3214, 2715, 2561, 955, 3850, 5044, 3232, 5161, 5428, 379, 3305, 5225, 5064, 1979, 613, 5236, 1973, 5201, 3892, 5096, 1227, 2728, 3528, 2659, 5325, 3435, 5168, 5308, 5410, 3980, 3332, 5279, 1531, 5464, 2577, 3296, 5094, 5061, 5113, 5204, 3759, 5307, 2521, 5282, 5230, 3863, 5321, 2339, 1612, 513, 4509, 5462, 2643, 4089, 5404, 1689, 4146, 1136, 954, 1925, 2314, 5072, 5038, 1433, 5496, 59, 5219, 5466, 745, 539, 1552, 2236, 5424, 5140, 3411, 5422, 3661, 1691, 5377, 5319, 1776, 1444, 723, 4988, 2966, 5454, 1688, 1174, 4627, 5019, 1630, 3534, 5437, 1633, 1627, 3074, 5329, 5095, 5340, 3308, 2699, 2558, 2925, 193, 5277, 4406, 5415, 9, 5055, 5445, 757, 499, 1888, 5027, 5322, 5137, 2799, 5046, 4102, 1454, 5295, 3268, 2399, 3430, 4029, 4035, 1632, 4986, 4890, 3949, 1248, 616, 2813, 4249, 5155, 5211, 2290, 454, 3840, 1779, 5323, 496, 5119, 5208, 4645, 5352, 1590, 4482, 5228, 4864, 4019, 5488, 1598, 2000, 3243, 3723, 3697, 2781, 4996, 2542, 128, 3014, 5301, 1637, 2962, 4059, 1543, 1301, 3544, 5391, 3095, 4229, 4883, 3632, 168, 1851, 4287, 3346, 4722, 5479, 4764, 895, 5431, 2544, 2021, 1050, 5192, 5393, 3466, 2593, 683, 1884, 1264, 5370, 3322, 2051, 3712, 2571, 3510, 4047, 5451, 5125, 5056, 5128, 1949, 3696, 1975, 3744, 710, 3414, 3991, 458, 4969, 3172, 502, 1963, 3568, 1793, 3167, 3979, 681, 4490, 2472, 5028, 260, 2320, 5389, 5297, 1628, 3156, 4377, 5380, 2865, 5273, 5401, 1161, 5145, 5118, 4538, 3822, 2090, 4467, 524, 5426, 4667, 2992, 3415, 355, 4460, 2407, 3195, 5290, 1713, 731, 4539, 1742, 5315, 3690, 4112, 1874, 5310, 3219, 3395, 3714, 1249, 5097, 3619, 2394, 4813, 5476, 4802, 5029, 2368, 3988, 1512, 4318, 2672, 5156, 1766, 4011, 5008, 4546, 4438, 4706, 5000, 99, 3151, 4514, 1996, 4592, 2012, 3402, 1737, 3830, 4781, 5073, 2878, 1109, 4703, 1326, 3002, 1790, 4169, 4072, 1260, 5083, 3221, 4976, 1481, 2034, 5339, 3650, 4591, 1699]\n",
            "=====================\n",
            "AUROC 0.1 0.7\n",
            "=======================\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 128) (256, 128)\n",
            "Train on 4950 samples, validate on 550 samples\n",
            "Epoch 1/250\n",
            "4950/4950 [==============================] - 7s 1ms/step - loss: 10.7750 - val_loss: 10.7782\n",
            "Epoch 2/250\n",
            "4950/4950 [==============================] - 2s 486us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 3/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 4/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 5/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 6/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 10.7749 - val_loss: 10.7779\n",
            "Epoch 7/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 8/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 9/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7750 - val_loss: 10.7782\n",
            "Epoch 10/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 11/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 12/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7750 - val_loss: 10.7780\n",
            "Epoch 13/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 14/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 15/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 16/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 17/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 18/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 19/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 20/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 21/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 22/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 23/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 24/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 25/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 26/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 27/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7750 - val_loss: 10.7780\n",
            "Epoch 28/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 29/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 30/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 31/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 10.7749 - val_loss: 10.7783\n",
            "Epoch 32/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 33/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 34/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 10.7749 - val_loss: 10.7779\n",
            "Epoch 35/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 10.7749 - val_loss: 10.7779\n",
            "Epoch 36/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 37/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 38/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 39/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 40/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 41/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 42/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 43/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 44/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 45/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 46/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 47/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7749 - val_loss: 10.7779\n",
            "Epoch 48/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 49/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7749 - val_loss: 10.7782\n",
            "Epoch 50/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 51/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7749 - val_loss: 10.7779\n",
            "Epoch 52/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7749 - val_loss: 10.7779\n",
            "Epoch 53/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 54/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 55/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7750 - val_loss: 10.7782\n",
            "Epoch 56/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7749 - val_loss: 10.7782\n",
            "Epoch 57/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 10.7749 - val_loss: 10.7782\n",
            "Epoch 58/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 59/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 60/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 61/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 10.7749 - val_loss: 10.7779\n",
            "Epoch 62/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 63/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7778\n",
            "Epoch 64/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 65/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 66/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 67/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 68/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7748 - val_loss: 10.7783\n",
            "Epoch 69/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 70/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 71/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 72/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 73/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 74/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7749 - val_loss: 10.7779\n",
            "Epoch 75/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 76/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7749 - val_loss: 10.7784\n",
            "Epoch 77/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 78/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 79/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 80/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 81/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 82/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7749 - val_loss: 10.7779\n",
            "Epoch 83/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 84/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 85/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 86/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 87/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 88/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 89/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 90/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 91/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 92/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 93/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 94/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 95/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 96/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 97/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 98/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 99/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 100/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 101/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 102/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7749 - val_loss: 10.7779\n",
            "Epoch 103/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 104/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 105/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 106/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 107/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 108/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 109/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 110/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 111/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 112/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 113/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 114/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 115/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 116/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 117/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 118/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 119/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 120/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 121/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 122/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 123/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 124/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 125/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 126/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 127/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 128/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 129/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 130/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 131/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7747 - val_loss: 10.7781\n",
            "Epoch 132/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 133/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 134/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 135/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 136/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 137/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 138/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 139/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 140/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 141/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 142/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 143/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 144/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 145/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 146/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 147/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 148/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 149/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 150/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 151/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 152/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 153/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 154/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 155/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 156/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 157/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7749 - val_loss: 10.7780\n",
            "Epoch 158/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 159/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 160/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 161/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 162/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 163/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 10.7747 - val_loss: 10.7781\n",
            "Epoch 164/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 165/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 166/250\n",
            "4950/4950 [==============================] - 2s 404us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 167/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 168/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 169/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 170/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 171/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 172/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 173/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 174/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 175/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 176/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 177/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 178/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7749 - val_loss: 10.7781\n",
            "Epoch 179/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 180/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 181/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 182/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 183/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 184/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 185/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 186/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 187/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 188/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 189/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 190/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 191/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 192/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 193/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 194/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 10.7747 - val_loss: 10.7778\n",
            "Epoch 195/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 196/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 197/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 198/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7748 - val_loss: 10.7782\n",
            "Epoch 199/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 200/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 201/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 202/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 203/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 204/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7747 - val_loss: 10.7778\n",
            "Epoch 205/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 206/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 207/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 208/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 209/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 210/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 211/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7748 - val_loss: 10.7782\n",
            "Epoch 212/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 213/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 214/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 215/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 216/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 217/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 218/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 219/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 220/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 221/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 222/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 223/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 224/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 225/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 226/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 227/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 228/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 229/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 230/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 231/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 232/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7748 - val_loss: 10.7782\n",
            "Epoch 233/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 234/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 235/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 236/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 237/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 238/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 239/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 240/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7781\n",
            "Epoch 241/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 10.7748 - val_loss: 10.7780\n",
            "Epoch 242/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 243/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 244/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 245/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7747 - val_loss: 10.7778\n",
            "Epoch 246/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 247/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 10.7747 - val_loss: 10.7779\n",
            "Epoch 248/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7748 - val_loss: 10.7779\n",
            "Epoch 249/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "Epoch 250/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 10.7747 - val_loss: 10.7780\n",
            "(lamda,Threshold) 0.5 0.25\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (5500, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 101313 0.5\n",
            "The shape of N (5500, 3072)\n",
            "The minimum value of N  -0.5781857967376709\n",
            "The max value of N 0.4563542604446411\n",
            "[INFO:] Xclean  MSE Computed shape (5500, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (5500, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0062092817, 0.0050763334, 0.0046889368, 0.004568247])\n",
            "[INFO:] The anomaly threshold computed is  0.004568247\n",
            "[INFO:] The shape of input data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (5500, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [5103, 5330, 2999, 5079, 5226, 5105, 5312, 5461, 5166, 5049, 5099, 5367, 5357, 5032, 5200, 5001, 5433, 5114, 5427, 5493, 5293, 5360, 5160, 4505, 4551, 5460, 1857, 5065, 5239, 5071, 5060, 5396, 5214, 5059, 1083, 2791, 5014, 5206, 5285, 4724, 3187, 5147, 5269, 3029, 5486, 5306, 5270, 5051, 3826, 5088, 5188, 5136, 5332, 5146, 5131, 5438, 5248, 5324, 5212, 5351, 5205, 1466, 5382, 5255, 5092, 3709, 1387, 86, 5311, 5430, 5354, 1947, 3530, 5133, 5202, 5468, 5078, 1841, 5158, 5432, 5450, 2419, 1796, 5021, 1189, 5364, 4839, 2831, 5318, 5375, 4980, 969, 5386, 5011, 5385, 3199, 5043, 2808, 5076, 5397, 5186, 4437, 5471, 196, 714, 1258, 5007, 5304, 1404, 27, 5300, 5465, 5227, 5278, 947, 2703, 3902, 5222, 5084, 2264, 3925, 5261, 882, 2340, 4974, 370, 5366, 5265, 5436, 5482, 5004, 5112, 58, 5005, 5104, 3643, 805, 5470, 1396, 5045, 806, 5064, 5395, 5113, 1425, 5201, 5257, 5096, 5394, 5052, 5176, 3069, 5044, 4284, 5127, 5236, 5462, 3852, 4520, 4682, 5404, 1603, 4762, 5494, 5308, 312, 5331, 4802, 5459, 5428, 3841, 2487, 3526, 5424, 5492, 403, 5068, 4158, 5072, 4807, 5329, 5307, 4728, 3, 5024, 5321, 1597, 2888, 4065, 5282, 5161, 5143, 3332, 5168, 1190, 3678, 3305, 630, 5422, 2850, 5374, 3232, 5410, 1227, 3073, 2561, 5496, 2715, 5466, 900, 5325, 3980, 5279, 2815, 1979, 5230, 379, 3214, 5164, 3435, 5340, 5219, 955, 5061, 5211, 3296, 3892, 4659, 5204, 4308, 5225, 3528, 3863, 2521, 4146, 2557, 5027, 5094, 613, 2643, 1776, 5140, 2577, 5322, 3850, 1136, 4509, 1925, 2659, 5464, 5019, 1433, 5046, 3759, 745, 4988, 1973, 5154, 5377, 2728, 5137, 5437, 5155, 4089, 3268, 1612, 3661, 2314, 539, 3534, 9, 5055, 4986, 757, 59, 1174, 1688, 2399, 2339, 1627, 5095, 1531, 1689, 1691, 513, 2966, 5277, 1444, 1552, 2925, 4996, 5038, 5380, 5415, 2699, 3411, 1630, 5454, 4249, 2236, 4627, 4035, 3074, 1633, 1632, 954, 5488, 5352, 3697, 2558, 5028, 5319, 5445, 5228, 5315, 2799, 1888, 1248, 499, 3430, 1454, 5128, 3840, 5323, 2813, 3014, 5297, 4019, 4229, 2781, 3308, 5119, 4460, 3712, 5125, 5295, 4482, 5301, 1851, 3568, 1598, 454, 2962, 5393, 683, 1590, 4047, 1264, 4890, 3095, 5479, 1779, 4645, 5273, 4764, 4029, 5097, 5370, 4102, 2992, 3195, 5444, 1637, 723, 5145, 193, 5310, 3167, 2593, 5208, 2290, 3949, 616, 1766, 3632, 3346, 5407, 2542, 895, 4969, 5476, 4883, 710, 2544, 5391, 1963, 4490, 2051, 496, 260, 3979, 5056, 4059, 3322, 5192, 3243, 2472, 3723, 5290, 731, 5457, 5316, 3650, 4976, 5431, 3466, 1301, 1713, 3510, 5000, 3830, 1543, 1244, 3544, 1512, 1949, 3991, 355, 5339, 1737, 2000, 3415, 4539, 3690, 4538, 4406, 681, 5029, 2394, 5083, 5389, 5495, 3414, 3172, 4287, 4813, 3696, 3714, 1481, 2320, 2021, 4781, 5412, 5156, 5098, 458, 4706, 5426, 5116, 5040, 1260, 4667, 5008, 5073, 1326, 4864, 3219, 5451, 1249, 5498, 2459, 3744, 1742, 4467, 1975, 4169, 2571, 1050, 128, 502, 2677, 1699, 1790, 4377, 2368, 1884, 4722, 4546, 2407, 5296, 4072, 3201, 1996, 3002, 1482, 2090, 1628, 168, 1161, 3822, 3988, 3439, 5178, 3583, 5305, 2878, 3657, 5151, 3151, 2140]\n",
            "=====================\n",
            "AUROC 0.5 0.712\n",
            "=======================\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 128) (256, 128)\n",
            "Train on 4950 samples, validate on 550 samples\n",
            "Epoch 1/250\n",
            "4950/4950 [==============================] - 7s 1ms/step - loss: 8.5136 - val_loss: 8.5166\n",
            "Epoch 2/250\n",
            "4950/4950 [==============================] - 3s 555us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 3/250\n",
            "4950/4950 [==============================] - 2s 498us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 4/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 5/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 6/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 7/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 8/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 9/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 10/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 11/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 12/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 13/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 14/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 15/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 16/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5134 - val_loss: 8.5168\n",
            "Epoch 17/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 18/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 19/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 20/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 21/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 22/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 23/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 24/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 25/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 26/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 27/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 28/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 29/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 30/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 31/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 32/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 33/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5168\n",
            "Epoch 34/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 35/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5134 - val_loss: 8.5170\n",
            "Epoch 36/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 37/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 38/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 39/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 40/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 41/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 42/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 43/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 44/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 45/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 46/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 47/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 48/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 49/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 50/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 51/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 52/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 53/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 54/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 55/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 56/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 8.5134 - val_loss: 8.5168\n",
            "Epoch 57/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 58/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 59/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 8.5134 - val_loss: 8.5165\n",
            "Epoch 60/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 61/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 62/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 63/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 64/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 65/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5133 - val_loss: 8.5165\n",
            "Epoch 66/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 67/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 68/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 69/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 70/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 71/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 72/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 73/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 74/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 75/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 8.5134 - val_loss: 8.5169\n",
            "Epoch 76/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 77/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5134 - val_loss: 8.5168\n",
            "Epoch 78/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 79/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 80/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 8.5134 - val_loss: 8.5168\n",
            "Epoch 81/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 82/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 83/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 84/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 85/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 86/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 87/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 88/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 89/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 90/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 91/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 92/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 93/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 94/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 95/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 96/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 8.5134 - val_loss: 8.5165\n",
            "Epoch 97/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 98/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 99/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 100/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 101/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5135 - val_loss: 8.5166\n",
            "Epoch 102/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5135 - val_loss: 8.5168\n",
            "Epoch 103/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5135 - val_loss: 8.5168\n",
            "Epoch 104/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 105/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 106/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5168\n",
            "Epoch 107/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 108/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 109/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 110/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 111/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 112/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 113/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 114/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 115/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5133 - val_loss: 8.5168\n",
            "Epoch 116/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 8.5135 - val_loss: 8.5168\n",
            "Epoch 117/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 118/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 119/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 120/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 121/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 122/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 8.5133 - val_loss: 8.5168\n",
            "Epoch 123/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5133 - val_loss: 8.5165\n",
            "Epoch 124/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 125/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 126/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 127/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 128/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5168\n",
            "Epoch 129/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 130/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 131/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 132/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 133/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 134/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 135/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 136/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 137/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5133 - val_loss: 8.5168\n",
            "Epoch 138/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 139/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 140/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 141/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 142/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 8.5135 - val_loss: 8.5169\n",
            "Epoch 143/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5134 - val_loss: 8.5168\n",
            "Epoch 144/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 145/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 146/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 147/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 148/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 149/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 150/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 151/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 152/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 153/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 154/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 155/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5134 - val_loss: 8.5168\n",
            "Epoch 156/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 157/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 158/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 159/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 160/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 161/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 162/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 163/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 164/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 165/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 166/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 167/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 168/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 169/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5134 - val_loss: 8.5165\n",
            "Epoch 170/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 171/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 172/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 173/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 174/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 175/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 176/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 8.5133 - val_loss: 8.5165\n",
            "Epoch 177/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 178/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 179/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 8.5134 - val_loss: 8.5169\n",
            "Epoch 180/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 181/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 182/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 183/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 184/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 185/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 186/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 187/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 188/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 189/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 190/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 191/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 192/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 193/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 194/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 195/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 196/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 197/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 198/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 199/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 200/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 201/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 202/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 203/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 204/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5133 - val_loss: 8.5168\n",
            "Epoch 205/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 206/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 207/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 208/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 8.5132 - val_loss: 8.5166\n",
            "Epoch 209/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 210/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 211/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 8.5133 - val_loss: 8.5168\n",
            "Epoch 212/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 213/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 214/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 8.5134 - val_loss: 8.5169\n",
            "Epoch 215/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 8.5134 - val_loss: 8.5169\n",
            "Epoch 216/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 217/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 218/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 219/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 220/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 221/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 222/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 223/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 224/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 225/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 226/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5133 - val_loss: 8.5168\n",
            "Epoch 227/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5133 - val_loss: 8.5168\n",
            "Epoch 228/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 229/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 230/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 231/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 232/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 233/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 234/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 8.5134 - val_loss: 8.5169\n",
            "Epoch 235/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 236/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 237/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 238/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 239/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 8.5134 - val_loss: 8.5167\n",
            "Epoch 240/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 241/250\n",
            "4950/4950 [==============================] - 2s 405us/step - loss: 8.5133 - val_loss: 8.5168\n",
            "Epoch 242/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 243/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 8.5134 - val_loss: 8.5166\n",
            "Epoch 244/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 245/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 8.5133 - val_loss: 8.5165\n",
            "Epoch 246/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 247/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 248/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "Epoch 249/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 8.5133 - val_loss: 8.5167\n",
            "Epoch 250/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 8.5133 - val_loss: 8.5166\n",
            "(lamda,Threshold) 1.0 0.5\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (5500, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 1044 1.0\n",
            "The shape of N (5500, 3072)\n",
            "The minimum value of N  -0.36142224073410034\n",
            "The max value of N 0.24657344818115234\n",
            "[INFO:] Xclean  MSE Computed shape (5500, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (5500, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0062092817, 0.0050763334, 0.0046889368, 0.004568247, 0.0044301264])\n",
            "[INFO:] The anomaly threshold computed is  0.0044301264\n",
            "[INFO:] The shape of input data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (5500, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [5103, 5330, 5079, 2999, 5226, 5049, 5105, 5461, 5312, 5166, 5367, 5433, 5001, 5200, 5032, 5114, 5427, 5099, 5357, 5360, 5071, 5293, 5493, 5239, 5065, 4505, 5396, 5460, 4551, 5269, 5014, 5060, 5486, 5188, 1083, 5059, 5160, 5136, 1857, 3187, 5146, 5051, 2791, 5214, 5088, 5285, 5306, 5351, 3029, 5270, 5206, 5147, 4724, 5432, 5450, 5131, 4980, 5078, 5438, 3826, 5255, 5332, 5382, 5212, 5133, 5158, 5318, 5354, 5092, 5311, 5205, 1841, 5248, 5397, 5430, 5278, 1189, 86, 5011, 3709, 2419, 5324, 1466, 5104, 3530, 5043, 5375, 1947, 5236, 4839, 5386, 1387, 3199, 1796, 5007, 5471, 5222, 5436, 5465, 5227, 5005, 969, 5329, 5112, 5482, 2808, 5186, 2831, 5076, 947, 5366, 5468, 714, 5021, 5364, 196, 5202, 5052, 1258, 3902, 27, 5300, 5176, 5470, 4974, 806, 5261, 5304, 1425, 5084, 2340, 5265, 1404, 5394, 4437, 5308, 5257, 5331, 2264, 5492, 5307, 2703, 5385, 3925, 5459, 58, 4284, 882, 5404, 4682, 5161, 5044, 370, 5004, 5045, 5428, 805, 5072, 5340, 3643, 5374, 4520, 4762, 5096, 5113, 5395, 5064, 5225, 5494, 5211, 3069, 5321, 5325, 5127, 2487, 5027, 3841, 5462, 5230, 1396, 5204, 5143, 3, 5024, 2888, 5068, 5164, 312, 3852, 1603, 5410, 1190, 5219, 5201, 5496, 4728, 900, 5445, 4065, 5168, 4158, 403, 4659, 1597, 5282, 379, 630, 5422, 4807, 3232, 5061, 5424, 3305, 3526, 5466, 2715, 5377, 5140, 2561, 3980, 5464, 5137, 1979, 5322, 5154, 2577, 955, 1227, 2659, 2815, 5019, 5315, 4986, 3678, 3850, 5155, 613, 4308, 2643, 3296, 4988, 1136, 1433, 1973, 4146, 2850, 3892, 2521, 5095, 5437, 3435, 2557, 4509, 5454, 4089, 5125, 3528, 3268, 3863, 5415, 3332, 2339, 3214, 4019, 5094, 1627, 5277, 3759, 59, 5488, 745, 3661, 3074, 5279, 5046, 5380, 513, 1531, 1688, 4996, 1776, 2925, 2699, 757, 5145, 1925, 1552, 1691, 5228, 1689, 5119, 2314, 2728, 539, 5029, 1444, 1612, 5128, 3534, 9, 5055, 2236, 4627, 1632, 5301, 1174, 3411, 5339, 2966, 4035, 4969, 5352, 723, 1454, 5323, 5401, 3308, 1630, 3014, 2813, 5297, 5183, 5056, 3095, 5290, 5431, 1633, 5208, 5097, 1888, 1248, 5258, 2962, 3430, 2799, 616, 5028, 2399, 895, 3840, 5310, 5412, 1598, 2558, 5370, 4249, 4482, 4764, 5273, 4645, 2593, 5444, 4976, 1851, 499, 5295, 5083, 3632, 5391, 4890, 3466, 5156, 1264, 5319, 3712, 2781, 1766, 1512, 5316, 954, 4229, 5426, 5389, 5038, 454, 683, 5151, 4059, 5192, 5479, 1637, 4102, 5109, 4047, 4460, 5134, 1301, 3243, 5305, 260, 4539, 1779, 3510, 3949, 4883, 1963, 4029, 5407, 5000, 3322, 1590, 2021, 502, 3979, 5073, 3830, 710, 5178, 3697, 3568, 3415, 496, 4287, 193, 3723, 1949, 2544, 3195, 5476, 4490, 2000, 731, 5498, 3395, 1543, 5040, 1244, 1050, 1975, 1713, 5116, 3167, 5451, 1742, 638, 3346, 5098, 3690, 4377, 2290, 2320, 5008, 2992, 2472, 2051, 4781, 2542, 3657, 1790, 3172, 4667, 1326, 3650, 4406, 3619, 355, 3544, 2571, 5457, 3991, 2672, 1996, 2090, 4072, 1884, 3219, 2865, 4538, 2407, 4169, 3714, 5081, 5118, 5495, 5102, 1793, 3002, 458, 2368, 4968, 2394, 4591, 3414, 5487, 681, 4953, 4467, 4514, 5435, 3822, 3583, 5296, 1109, 5050, 5259, 168, 3151, 24, 5281, 4438]\n",
            "=====================\n",
            "AUROC 1.0 0.724\n",
            "=======================\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 128) (256, 128)\n",
            "Train on 4950 samples, validate on 550 samples\n",
            "Epoch 1/250\n",
            "4950/4950 [==============================] - 7s 1ms/step - loss: 4.0807 - val_loss: 4.0840\n",
            "Epoch 2/250\n",
            "4950/4950 [==============================] - 2s 494us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 3/250\n",
            "4950/4950 [==============================] - 2s 481us/step - loss: 4.0806 - val_loss: 4.0841\n",
            "Epoch 4/250\n",
            "4950/4950 [==============================] - 2s 466us/step - loss: 4.0806 - val_loss: 4.0840\n",
            "Epoch 5/250\n",
            "4950/4950 [==============================] - 2s 435us/step - loss: 4.0806 - val_loss: 4.0838\n",
            "Epoch 6/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0806 - val_loss: 4.0838\n",
            "Epoch 7/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 8/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 9/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0806 - val_loss: 4.0840\n",
            "Epoch 10/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 11/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 12/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 4.0806 - val_loss: 4.0838\n",
            "Epoch 13/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 14/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 15/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 16/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 17/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 18/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0804 - val_loss: 4.0838\n",
            "Epoch 19/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 20/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 21/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 22/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 23/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 24/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 25/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 26/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 27/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 28/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 29/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 30/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 31/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0806 - val_loss: 4.0838\n",
            "Epoch 32/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 33/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 34/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 35/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 36/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 37/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 38/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 39/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 40/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 41/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0806 - val_loss: 4.0838\n",
            "Epoch 42/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 43/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 44/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 45/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 46/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 47/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 48/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 49/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0806 - val_loss: 4.0840\n",
            "Epoch 50/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 51/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 52/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 53/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 54/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 55/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 56/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 57/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 58/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 59/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 60/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 61/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 62/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 4.0804 - val_loss: 4.0838\n",
            "Epoch 63/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 64/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 65/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 66/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 67/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 68/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 69/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 70/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0806 - val_loss: 4.0838\n",
            "Epoch 71/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 72/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0807 - val_loss: 4.0840\n",
            "Epoch 73/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0806 - val_loss: 4.0840\n",
            "Epoch 74/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 75/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0837\n",
            "Epoch 76/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 77/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 78/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 79/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 80/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 81/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 82/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 83/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 84/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 85/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 86/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 87/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 88/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 89/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 90/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 91/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 92/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 93/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0805 - val_loss: 4.0841\n",
            "Epoch 94/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 95/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 96/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 97/250\n",
            "4950/4950 [==============================] - 2s 402us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 98/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 99/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 100/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 101/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 102/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 103/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 104/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 4.0806 - val_loss: 4.0840\n",
            "Epoch 105/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 106/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 107/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 4.0805 - val_loss: 4.0841\n",
            "Epoch 108/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 109/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 110/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0804 - val_loss: 4.0840\n",
            "Epoch 111/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 112/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 113/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 114/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 115/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 116/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 117/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 118/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 119/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 120/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 121/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 122/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 123/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 124/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 125/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 126/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 127/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 128/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 129/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 130/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 131/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 132/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 133/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 134/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 135/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 136/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 137/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 138/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 139/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 140/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 141/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 142/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 143/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 144/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0804 - val_loss: 4.0840\n",
            "Epoch 145/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 146/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 147/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 148/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 149/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 150/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 151/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 152/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 153/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 154/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 155/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0804 - val_loss: 4.0839\n",
            "Epoch 156/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 157/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 158/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 159/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0806 - val_loss: 4.0841\n",
            "Epoch 160/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0806 - val_loss: 4.0840\n",
            "Epoch 161/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 4.0806 - val_loss: 4.0840\n",
            "Epoch 162/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 163/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 164/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 165/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0804 - val_loss: 4.0840\n",
            "Epoch 166/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 167/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 168/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 169/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 170/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 171/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 172/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 173/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0804 - val_loss: 4.0838\n",
            "Epoch 174/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 4.0804 - val_loss: 4.0839\n",
            "Epoch 175/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 176/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 177/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 178/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 179/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 180/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 181/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 182/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 183/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 184/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 185/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 186/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 187/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 188/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 189/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 190/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 191/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 192/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 193/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 194/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0806 - val_loss: 4.0839\n",
            "Epoch 195/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 196/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 4.0804 - val_loss: 4.0838\n",
            "Epoch 197/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 198/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0804 - val_loss: 4.0838\n",
            "Epoch 199/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0841\n",
            "Epoch 200/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 201/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 202/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0841\n",
            "Epoch 203/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 204/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 205/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 206/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 207/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 208/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0804 - val_loss: 4.0838\n",
            "Epoch 209/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 210/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 211/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 212/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 213/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 4.0804 - val_loss: 4.0838\n",
            "Epoch 214/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0804 - val_loss: 4.0839\n",
            "Epoch 215/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0804 - val_loss: 4.0839\n",
            "Epoch 216/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 4.0804 - val_loss: 4.0837\n",
            "Epoch 217/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 218/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 219/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 220/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 221/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 222/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0804 - val_loss: 4.0838\n",
            "Epoch 223/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0804 - val_loss: 4.0839\n",
            "Epoch 224/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 225/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 226/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 227/250\n",
            "4950/4950 [==============================] - 2s 426us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 228/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 229/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0804 - val_loss: 4.0839\n",
            "Epoch 230/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 231/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 4.0804 - val_loss: 4.0838\n",
            "Epoch 232/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 4.0804 - val_loss: 4.0839\n",
            "Epoch 233/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 234/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 235/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 236/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 4.0804 - val_loss: 4.0839\n",
            "Epoch 237/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0841\n",
            "Epoch 238/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 239/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0806 - val_loss: 4.0840\n",
            "Epoch 240/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 241/250\n",
            "4950/4950 [==============================] - 2s 427us/step - loss: 4.0804 - val_loss: 4.0839\n",
            "Epoch 242/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0804 - val_loss: 4.0838\n",
            "Epoch 243/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 4.0804 - val_loss: 4.0839\n",
            "Epoch 244/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0838\n",
            "Epoch 245/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 246/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 247/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 248/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 4.0805 - val_loss: 4.0840\n",
            "Epoch 249/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 4.0805 - val_loss: 4.0839\n",
            "Epoch 250/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 4.0805 - val_loss: 4.0841\n",
            "(lamda,Threshold) 10.0 5.0\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (5500, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 0 10.0\n",
            "The shape of N (5500, 3072)\n",
            "The minimum value of N  0.0\n",
            "The max value of N 0.0\n",
            "[INFO:] Xclean  MSE Computed shape (5500, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (5500, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0062092817, 0.0050763334, 0.0046889368, 0.004568247, 0.0044301264, 0.0046750032])\n",
            "[INFO:] The anomaly threshold computed is  0.0046750032\n",
            "[INFO:] The shape of input data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (5500, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [5103, 5330, 2999, 5079, 5049, 5226, 5461, 5105, 5433, 5239, 5166, 5367, 5360, 5200, 5293, 5032, 5312, 5001, 5357, 5427, 5099, 5493, 5114, 5059, 4505, 5460, 5065, 5396, 5071, 5014, 5160, 4551, 5060, 5486, 1083, 1857, 3187, 5270, 2791, 5438, 5206, 5269, 5188, 5285, 5306, 5136, 5088, 4980, 5051, 5324, 4724, 5255, 5432, 5133, 5214, 3029, 5351, 5332, 5147, 5131, 5078, 5248, 5146, 5212, 5382, 5318, 5092, 3826, 5354, 5227, 5450, 3709, 5158, 5278, 4437, 2419, 5236, 1466, 5311, 1841, 5468, 5386, 1796, 2808, 5430, 5375, 1189, 5186, 5011, 5471, 1387, 86, 3530, 5436, 5366, 5021, 5104, 3199, 5112, 969, 5084, 947, 4839, 5261, 3902, 5397, 1947, 5005, 5329, 5007, 2831, 196, 5043, 714, 1258, 5205, 4974, 5492, 1425, 5496, 5364, 5482, 5465, 27, 5300, 5072, 1404, 5127, 5331, 2340, 5257, 4682, 5394, 2264, 1227, 5304, 2703, 806, 5385, 5176, 4284, 805, 3925, 5076, 882, 5470, 58, 5052, 5325, 3069, 5161, 5265, 5308, 5374, 5459, 5340, 5428, 3841, 3643, 3, 5024, 403, 5202, 5143, 370, 5068, 312, 5064, 3852, 5404, 5096, 5201, 1603, 5230, 4762, 5282, 1190, 5424, 5154, 5494, 5004, 5395, 2888, 5225, 4988, 2487, 4520, 4146, 4065, 5410, 5462, 5321, 4728, 5307, 5044, 5113, 5445, 5027, 613, 5164, 5155, 5095, 5045, 4659, 5219, 2715, 3678, 3232, 1396, 5377, 955, 3980, 4807, 2815, 1979, 5222, 5168, 4158, 5061, 5019, 2561, 3296, 5323, 379, 3305, 900, 5464, 5204, 4308, 3759, 5370, 1597, 3850, 3892, 3332, 5211, 5415, 5322, 630, 5140, 5466, 745, 5137, 3435, 1691, 1136, 5422, 4019, 3526, 1776, 757, 2577, 4509, 2557, 5046, 1627, 723, 5279, 1689, 2521, 2314, 3661, 1531, 5128, 5488, 2659, 3863, 1925, 2850, 2728, 5301, 5431, 5315, 3214, 2643, 3074, 1174, 1552, 1688, 9, 5055, 5454, 2339, 3528, 59, 1444, 2966, 2925, 4089, 1612, 5380, 1433, 3534, 1632, 683, 5056, 1973, 2699, 5319, 3268, 4249, 5437, 3840, 4986, 4996, 513, 1630, 5277, 5352, 539, 3411, 1598, 4035, 4969, 4627, 616, 2236, 3308, 3430, 2399, 5028, 1779, 5479, 5259, 954, 5038, 2558, 1888, 5258, 3243, 5444, 4645, 5083, 5208, 193, 5094, 895, 5401, 2799, 4047, 5029, 1512, 1633, 1248, 1454, 5125, 3151, 4102, 4460, 5156, 3014, 3712, 5228, 4764, 1637, 5273, 454, 4029, 5391, 3172, 4539, 2593, 5339, 3544, 3095, 5119, 5097, 5192, 4976, 1050, 3322, 5118, 4229, 4890, 3650, 4883, 2962, 2021, 5290, 4538, 2472, 502, 3415, 5040, 2813, 5393, 5008, 5183, 5073, 638, 1543, 2051, 2992, 731, 5426, 3346, 499, 3632, 1851, 4059, 1975, 5145, 5102, 5498, 1790, 5407, 1713, 3697, 5435, 5310, 5451, 4482, 2090, 1996, 2781, 4169, 3395, 1264, 3949, 5297, 2000, 2407, 710, 3510, 3466, 4490, 4377, 3568, 3988, 496, 260, 5151, 1766, 3723, 2542, 1301, 4667, 355, 2290, 5476, 2544, 1874, 5237, 2459, 4781, 5295, 5296, 1244, 5418, 1249, 3991, 4287, 1949, 2368, 3219, 3979, 3619, 681, 2571, 1318, 3195, 5050, 5178, 5495, 3822, 5487, 1326, 5412, 458, 2140, 1742, 420, 4011, 4467, 4406, 2672, 4813, 3583, 4706, 99, 3714, 5109, 5458, 1963, 4546, 5256, 5122, 5116, 3402, 2320, 1287, 4422, 1095, 3165, 3690, 5074, 128, 2865, 5316, 1590]\n",
            "=====================\n",
            "AUROC 10.0 0.723\n",
            "=======================\n",
            "[INFO] compiling model...\n",
            "[INFO:] Shape of U, V (256, 128) (256, 128)\n",
            "Train on 4950 samples, validate on 550 samples\n",
            "Epoch 1/250\n",
            "4950/4950 [==============================] - 7s 1ms/step - loss: 3.1185 - val_loss: 3.1220\n",
            "Epoch 2/250\n",
            "4950/4950 [==============================] - 3s 542us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 3/250\n",
            "4950/4950 [==============================] - 2s 456us/step - loss: 3.1185 - val_loss: 3.1220\n",
            "Epoch 4/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.1185 - val_loss: 3.1218\n",
            "Epoch 5/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 6/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 7/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 8/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 9/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 10/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1185 - val_loss: 3.1221\n",
            "Epoch 11/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 12/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 13/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1185 - val_loss: 3.1218\n",
            "Epoch 14/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1185 - val_loss: 3.1218\n",
            "Epoch 15/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 16/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 17/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1217\n",
            "Epoch 18/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 19/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 20/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1185 - val_loss: 3.1218\n",
            "Epoch 21/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 22/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 23/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 24/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 25/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 26/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 27/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 28/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 29/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 30/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 31/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 32/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 33/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 34/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 35/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 36/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 37/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 38/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 39/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 40/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 41/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 42/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 3.1185 - val_loss: 3.1220\n",
            "Epoch 43/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 44/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 45/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 46/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 47/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 48/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 49/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 50/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 51/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 52/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 53/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 54/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 55/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 56/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 57/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1185 - val_loss: 3.1220\n",
            "Epoch 58/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 59/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 60/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 61/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 62/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 63/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 64/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 65/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 66/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 67/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1185 - val_loss: 3.1220\n",
            "Epoch 68/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1185 - val_loss: 3.1220\n",
            "Epoch 69/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 70/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 71/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 72/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 73/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1185 - val_loss: 3.1218\n",
            "Epoch 74/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 75/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 76/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 77/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1185 - val_loss: 3.1218\n",
            "Epoch 78/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 79/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 80/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 81/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 82/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 83/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 84/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 85/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 86/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1221\n",
            "Epoch 87/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 88/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 89/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 90/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 91/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 92/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 93/250\n",
            "4950/4950 [==============================] - 2s 429us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 94/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 95/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 96/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1183 - val_loss: 3.1219\n",
            "Epoch 97/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 98/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 99/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 100/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 101/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 102/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 103/250\n",
            "4950/4950 [==============================] - 2s 424us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 104/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 105/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 106/250\n",
            "4950/4950 [==============================] - 2s 428us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 107/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 108/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 109/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 110/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 111/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1185 - val_loss: 3.1220\n",
            "Epoch 112/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 113/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 114/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 115/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.1185 - val_loss: 3.1220\n",
            "Epoch 116/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1185 - val_loss: 3.1222\n",
            "Epoch 117/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1185 - val_loss: 3.1224\n",
            "Epoch 118/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1185 - val_loss: 3.1220\n",
            "Epoch 119/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 120/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 121/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 122/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 123/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 124/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 125/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 126/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 127/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1221\n",
            "Epoch 128/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 129/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 130/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 131/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 132/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 133/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 134/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 135/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 136/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 137/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 138/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 139/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 140/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 141/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 142/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 143/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 144/250\n",
            "4950/4950 [==============================] - 2s 404us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 145/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 146/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 147/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 148/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 149/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 150/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 151/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.1184 - val_loss: 3.1221\n",
            "Epoch 152/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 153/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 154/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 155/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 156/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 157/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 158/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 159/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 160/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 161/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 162/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 163/250\n",
            "4950/4950 [==============================] - 2s 422us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 164/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1221\n",
            "Epoch 165/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 166/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 167/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 168/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 169/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 170/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 171/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 172/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 173/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 174/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 175/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 176/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 177/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 178/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 179/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 180/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 181/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 182/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 183/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 184/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 185/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 186/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 187/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1183 - val_loss: 3.1217\n",
            "Epoch 188/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 189/250\n",
            "4950/4950 [==============================] - 2s 420us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 190/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 191/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 192/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 193/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1183 - val_loss: 3.1219\n",
            "Epoch 194/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 195/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 196/250\n",
            "4950/4950 [==============================] - 2s 421us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 197/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 198/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 199/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 200/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 201/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 202/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 203/250\n",
            "4950/4950 [==============================] - 2s 423us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 204/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 205/250\n",
            "4950/4950 [==============================] - 2s 425us/step - loss: 3.1184 - val_loss: 3.1221\n",
            "Epoch 206/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1185 - val_loss: 3.1219\n",
            "Epoch 207/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 208/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 209/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1221\n",
            "Epoch 210/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.1185 - val_loss: 3.1220\n",
            "Epoch 211/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 212/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1183 - val_loss: 3.1219\n",
            "Epoch 213/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 214/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.1184 - val_loss: 3.1221\n",
            "Epoch 215/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 216/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 217/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 218/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 219/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 220/250\n",
            "4950/4950 [==============================] - 2s 403us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 221/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1185 - val_loss: 3.1218\n",
            "Epoch 222/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 223/250\n",
            "4950/4950 [==============================] - 2s 418us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 224/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 225/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.1185 - val_loss: 3.1220\n",
            "Epoch 226/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1221\n",
            "Epoch 227/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 228/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 229/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1183 - val_loss: 3.1219\n",
            "Epoch 230/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 231/250\n",
            "4950/4950 [==============================] - 2s 414us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 232/250\n",
            "4950/4950 [==============================] - 2s 417us/step - loss: 3.1183 - val_loss: 3.1218\n",
            "Epoch 233/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 234/250\n",
            "4950/4950 [==============================] - 2s 409us/step - loss: 3.1183 - val_loss: 3.1219\n",
            "Epoch 235/250\n",
            "4950/4950 [==============================] - 2s 407us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 236/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 237/250\n",
            "4950/4950 [==============================] - 2s 419us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 238/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 239/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 240/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 241/250\n",
            "4950/4950 [==============================] - 2s 408us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 242/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 243/250\n",
            "4950/4950 [==============================] - 2s 410us/step - loss: 3.1183 - val_loss: 3.1219\n",
            "Epoch 244/250\n",
            "4950/4950 [==============================] - 2s 415us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 245/250\n",
            "4950/4950 [==============================] - 2s 406us/step - loss: 3.1184 - val_loss: 3.1218\n",
            "Epoch 246/250\n",
            "4950/4950 [==============================] - 2s 411us/step - loss: 3.1184 - val_loss: 3.1221\n",
            "Epoch 247/250\n",
            "4950/4950 [==============================] - 2s 413us/step - loss: 3.1184 - val_loss: 3.1219\n",
            "Epoch 248/250\n",
            "4950/4950 [==============================] - 2s 405us/step - loss: 3.1184 - val_loss: 3.1220\n",
            "Epoch 249/250\n",
            "4950/4950 [==============================] - 2s 416us/step - loss: 3.1185 - val_loss: 3.1222\n",
            "Epoch 250/250\n",
            "4950/4950 [==============================] - 2s 412us/step - loss: 3.1185 - val_loss: 3.1220\n",
            "(lamda,Threshold) 100.0 50.0\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (5500, 3072) 3072\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 0 100.0\n",
            "The shape of N (5500, 3072)\n",
            "The minimum value of N  0.0\n",
            "The max value of N 0.0\n",
            "[INFO:] Xclean  MSE Computed shape (5500, 3072)\n",
            "[INFO:]Xdecoded  Computed shape (5500, 3072)\n",
            "[INFO:] MSE Computed shape ()\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([0.0062092817, 0.0050763334, 0.0046889368, 0.004568247, 0.0044301264, 0.0046750032, 0.0044200444])\n",
            "[INFO:] The anomaly threshold computed is  0.0044200444\n",
            "[INFO:] The shape of input data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of decoded  data   (5500, 32, 32, 3)\n",
            "[INFO:] The shape of N  data   (5500, 32, 32, 3)\n",
            "img shape: (128, 320, 3)\n",
            "\n",
            "Saving results for best after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n",
            "\n",
            "Saving results for worst after being encoded and decoded: @\n",
            "/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n",
            "[INFO:] The anomaly index are  [5103, 5226, 5330, 5079, 5049, 2999, 5105, 5461, 5166, 5433, 5239, 5357, 5114, 5293, 5032, 5360, 5312, 5367, 5099, 5001, 5200, 5427, 5396, 5065, 5493, 5071, 5324, 4505, 5206, 5014, 5188, 5460, 5060, 5059, 4551, 5486, 5088, 5214, 5133, 5285, 1083, 5160, 5136, 5306, 5269, 3187, 2791, 5248, 5382, 5270, 1857, 5438, 5147, 5351, 5332, 4980, 5318, 5212, 5146, 5354, 5255, 5450, 5432, 3029, 5397, 5131, 4724, 5078, 5158, 5205, 1466, 5311, 5051, 5364, 5278, 5236, 5092, 5005, 1841, 5386, 5430, 5227, 5104, 3826, 5436, 5043, 2419, 5329, 5471, 5465, 1189, 5468, 5011, 5186, 3709, 5007, 5021, 5112, 5482, 5375, 5492, 3199, 1387, 5385, 5084, 5113, 947, 1796, 714, 27, 5300, 4839, 3530, 5366, 969, 5394, 5261, 5331, 5176, 5076, 4974, 2808, 86, 5072, 5202, 1258, 1947, 5052, 5496, 5459, 806, 5470, 2831, 196, 1425, 3902, 4437, 5127, 5404, 2703, 5265, 5424, 5064, 5304, 5201, 1404, 5321, 5340, 5164, 4284, 3925, 2340, 4682, 5428, 5257, 5494, 58, 312, 5374, 882, 5308, 2264, 5068, 5230, 5219, 5143, 5319, 3643, 5061, 5044, 805, 5096, 5307, 4988, 5004, 5282, 5410, 5445, 5045, 370, 4762, 3069, 3841, 3, 5024, 5325, 5161, 5322, 2888, 5464, 5204, 1190, 5140, 4728, 5211, 5462, 5315, 1603, 900, 4659, 5222, 5431, 2561, 955, 5225, 5395, 3232, 2487, 5027, 5279, 1979, 5154, 4520, 4807, 2715, 5259, 3980, 4158, 5019, 3305, 5168, 5377, 1136, 5422, 379, 1597, 5155, 5479, 3526, 2815, 403, 3852, 5437, 5466, 1973, 1396, 4308, 3678, 4509, 513, 5301, 5277, 5137, 2521, 5095, 613, 5454, 3435, 5415, 630, 4065, 3332, 3892, 3759, 1227, 1627, 4986, 3296, 59, 5046, 1689, 3214, 9, 5055, 2577, 5125, 2699, 1691, 2850, 1531, 745, 5488, 4089, 5290, 2643, 5128, 4019, 2557, 5258, 1433, 3850, 5228, 5192, 5393, 3268, 5273, 5028, 1552, 5056, 3661, 4996, 4969, 3528, 1925, 1454, 5323, 2339, 1688, 5370, 3074, 1612, 5208, 4146, 1776, 757, 5094, 1174, 5097, 5426, 5040, 5119, 4035, 5145, 2728, 1630, 5316, 539, 1444, 5029, 2314, 5178, 3863, 3308, 3534, 5156, 5407, 2966, 3014, 1598, 1633, 2659, 2236, 4764, 4627, 5134, 5083, 1888, 5380, 5295, 2925, 1632, 4645, 5339, 1766, 3243, 895, 5297, 5038, 5008, 5352, 4059, 2558, 4482, 4539, 5451, 4976, 3095, 616, 3632, 954, 5444, 454, 1590, 3411, 3840, 2962, 4047, 5256, 5391, 3430, 2799, 5389, 5183, 4102, 2021, 260, 723, 1264, 5487, 3346, 3466, 3415, 4249, 5412, 1301, 4883, 193, 1248, 5030, 4229, 1851, 499, 4029, 5151, 2992, 5435, 4890, 2000, 5310, 3650, 5000, 2544, 710, 1779, 731, 4377, 3979, 2593, 1790, 1713, 3583, 1244, 502, 4953, 5118, 3712, 5498, 5109, 1326, 4460, 1637, 5458, 2813, 3322, 3172, 3568, 5305, 5476, 496, 3395, 1512, 2012, 1975, 683, 2672, 5215, 4438, 5418, 4406, 2399, 1742, 3991, 2542, 2472, 3690, 2320, 2051, 4538, 458, 2865, 2781, 3195, 3949, 5122, 5296, 4287, 1543, 3697, 2290, 5116, 4490, 4169, 1996, 5098, 4072, 4011, 2368, 3723, 2571, 355, 1318, 4781, 5266, 5102, 4968, 1109, 4455, 5401, 24, 5281, 5074, 1050, 3544, 5081, 1519, 1949, 5495, 681, 3402, 1963, 4514, 5376, 638, 5369, 5355, 3156, 2407, 3510, 5419, 3733]\n",
            "=====================\n",
            "AUROC 100.0 0.734\n",
            "=======================\n",
            "\n",
            " Mean square error Score ((Xclean, Xdecoded):\n",
            "dict_values([])\n",
            "========================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEVCAYAAAAM3jVmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAGWhJREFUeJzt3Xt8XHWd//HXJJPSWwqhpLYIgqX0\nw6XeEES2FAtbEJYqyk1dljtFZAFRwdUV2aI+Fi/4AETYH4iCyq6LrvwArYJQ2sKC8MNdUKjlY5Hb\nowXa0CZt0lyaufz+OCdtmmTSSTJnJpnv+/l4QGfO5fv9fua075x858yZVD6fR0REwlBT6QGIiEj5\nKPRFRAKi0BcRCYhCX0QkIAp9EZGAKPRFRAKi0BcpwMxuN7PFO9nmHDN7uNjlIpWm0BcRCUi60gMQ\nKQUz2xf4PXA9cD6QAs4Cvgq8F3jQ3c+Ltz0N+Beiv/+vA4vc/a9mNhX4GbA/8GegHVgT73MQ8G/A\nDKALONfd/1Dk2HYH/g/wHiAL/NjdvxWv+wZwWjzeNcA/uPvrhZYP9/UR6aEzfakmewBvursBfwLu\nBs4G3g38vZntZ2bvAH4AfMzdDwCWALfG+/8T0OTu7wT+EfgwgJnVAPcCP3H32cBFwH1mVuxJ078C\nzfG4jgQuNrMjzexg4HRgTtzu/wUWFFo+/JdFZDuFvlSTNPCL+PFzwNPu/pa7bwDeAPYEjgWWufuL\n8Xa3A0fHAX4U8HMAd38FWBFvcwAwDfhRvO5xoAn4myLHdSJwS7zvRuAe4DigBWgEzjCzBne/yd1/\nMshykRFT6Es1ybp7R89joK33OqCWKEybexa6+yaiKZQ9gN2BTb326dluN2AisMrMXjCzF4h+CEwt\nclw79Bk/nubua4GTiaZxXjOzJWa2d6HlRfYlMijN6Uto1gFH9DwxswYgB7xFFMa79tq2EXiJaN5/\nczwdtAMzO6fIPqcCr8XPp8bLcPdlwDIzmwRcB3wTOKPQ8qKrFClAZ/oSmoeAo8xsZvz8IuB37p4h\neiP44wBmth/R/DvAq8AaMzs1XreHmf0sDuRi/Bq4sGdforP4JWZ2nJndbGY17r4F+COQL7R8pIWL\ngEJfAuPua4ALiN6IfYFoHv/T8eprgX3M7GXgJqK5d9w9D3wSuCTe51FgaRzIxbgKaOi17zfd/f/F\njycCfzGzlcAngKsHWS4yYindT19EJBw60xcRCYhCX0QkIAp9EZGAKPRFRAIyqq/Tb2pqHdG7zA0N\nE2lubi/VcMYE1RwG1RyG4dbc2FifKrSuqs/00+naSg+h7FRzGFRzGJKouapDX0REdqTQFxEJiEJf\nRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgo/rDWcPVnc1w0+P/RXtuC9lsrtLDKVLBz1IMSTpdQyYz\nVmouDdUchtBqPnjq/nzm2IUlb7cqQ3/d5k28uPUZUjX5UmXp2JFFNYdANY85LSvXs9vB03a63drf\n/IU9jtib5q0b+QylD/1RfT/9kdyGYe3GZjK1WTZt6tj5xhWWL+GXIu2664QxUXMpqeYwjOWaNzSt\n555/v5NFl3+x6H1mNk5j5jsaaWpqHXJ/g92GoWpDH6CxsX5YL9hYpprDoJrHliuv/CyrVq1k06ZN\nHHfcCbzxxuvccMMtXHvt12hqWk9HRwfnnXchc+fO45JLLuTzn/8iy5YtJZfbivtq1q5dw2WXfYEj\njphbVH+DhX5VTu+IiBTy80de5OkX1pe0zcMOmMbpx8wquP5TnzqTe+75Oe9853689tor3HLL7TQ3\nb+QDH/ggJ5ywkLVr1/DVr36JuXPn7bDfm2++yXXXfY8nn3yC++77ZdGhPxiFvohIGR144MEA1NdP\nYdWqldx//z2kUjVs3ryp37aHHHIIANOmTaOtra0k/Sv0RSQopx8za9Cz8qTV1dUB8NBDD7B582Zu\nvvl2Nm/ezAUXnNlv23R6e0SXaipe1+mLiCSspqaGbDa7w7KWlhZmzNiTmpoaVqx4hO7u7vKMpSy9\niIgEbJ993on7C2zZsn2KZv78Y3jiicf47Gc/w4QJE5g2bRp33PGDxMeiq3eqjGoOg2oOw3BrDvab\ns0REZEcKfRGRgCj0RUQCotAXEQlIotfpm9kc4D7genf/vpntDfwUqAXeAM50964kxyAiItsldqZv\nZpOAm4ClvRZ/DbjZ3ecBLwLnJdW/iIj0l+T0Thfwd8DrvZbNB+6PH/8KWJBg/yIio8by5Ut3vlEv\nzz77v2zYsKHk40hsesfdM0DGzHovntRrOmc9MGOwNhoaJpJO145oHI2N9SPafyxSzWFQzWPHmjVr\neOyxRzjttI8Vvc/Spb9ln31mMHv27JKOpZL33tnp1yE0N7ePqAN9mCMMqjkMY7nmq666mlWrVvKt\nb32Xl156kdbWVrLZLJdffiWzZu3PXXfdyYoVy6ipqWHu3HkceOBBPPTQQ6xevZrFi7/J9OnTh9Tf\nYD8cyx36bWY2wd07gLez49SPiEji7nnx1zyz/rmStvm+ae/i5FmFv+Wq59bKNTU1HH743/CRj3yM\nl19+iRtvvI4bbriF//zPu7j33geora3l3nt/yWGHfZBZs2bz9a9fQ0PD0AJ/Z8od+g8DpwB3xX8+\nUOb+RUQq5rnn/kRLSzMPPvgbALq6OgGYP/9vufzyizn22OM57rjjEx1DYqFvZu8HvgvsC3Sb2anA\nGcCdZvZp4FXgx0n1LyIykJNnLRz0rDxJdXVpPve5K5kz5907LL/iii/z6quv8MgjD3HppZ/mttuS\ni8Yk38j9H6Krdfo6Nqk+RURGo55bKx900BwefXQ5c+a8m5dffomnnnqChQs/xi9+8TPOPXcR5567\niGeffYb29i0D3o65FPQlKiIiCeu5tfKMGXuybt2bXHzxBeRyOS6//AomT55MS0szixadxYQJE5kz\n591MmbIr733vIVx22WV84xvfYebM/Uo2Ft1aucqo5jCo5jDo1soiIjIiCn0RkYAo9EVEAqLQFxEJ\niEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVE\nAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYCkKz2AJOTzeXz5H1nZuZXOzu5K\nD6esJkyoo6MjrJrHT6ijUzWPDqnkmh4/fhydnVuT62CUmTF7Lxo/NKfk7VZl6G9a28Syp1oqPYwK\nGYVBkDjVHIawap7y5z/xHoV+cXZ9eyMLDp1AtnPr6DwbStD48XWj7rebVIJnfwDjd6mjs2t01Zy0\nUVlzPp9o86Px73aSptsBibRblaGfSqXYf8HhNDbW09TUWunhlJVqDoNqluHSG7kiIgFR6IuIBESh\nLyISEIW+iEhAFPoiIgEp69U7ZjYZ+AnQAOwCXOPuD5ZzDCIiISv3mf45gLv70cCpwI1l7l9EJGjl\nDv23gKnx44b4uYiIlEkqn/Cn6PoysweAWUShf6K7P1lo20wmm0+na8s2NhGRKlHwc/DlntP/B+A1\ndz/ezN4D/BA4tND2zc3tI+ovxE/wqeYwqOYwDLfmxsb6guvKPb0zF3gQwN3/COxpZjqVFxEpk3KH\n/ovA4QBmtg/Q5u7ZMo9BRCRY5b7h2q3Aj8xsRdz3RWXuX0QkaGUNfXdvA04vZ58iIrKdPpErIhIQ\nhb6ISEAU+iIiAVHoi4gERKEvIhIQhb6ISEAU+iIiAVHoi4gERKEvIhIQhb6ISEAU+iIiAVHoi4gE\nRKEvIhIQhb6ISEAU+iIiAVHoi4gERKEvIhIQhb6ISEAU+iIiAVHoi4gERKEvIhIQhb6ISEAU+iIi\nAVHoi4gERKEvIhIQhb6ISEAU+iIiAVHoi4gEZMihb2a7mNneSQxGRESSlS5mIzP7MtAG/BD4A9Bq\nZr9z968mOTgRESmtYs/0PwJ8HzgN+JW7Hw7MTWxUIiKSiGJDv9vd88AJwL3xstpkhiQiIkkpanoH\naDGzJcBe7v57M1sI5BIcl4iIJKDY0P974Fjg8fh5J3D2cDo0szOALwIZ4Gp3XzKcdkREZOiKnd5p\nBJrcvcnMFgGfAiYNtTMzmwr8C3AksBA4aahtiIjI8BV7pn8H8EUzex9wAXAN8D2is/+hWAA87O6t\nQCtw4RD3FxGREUjl8/mdbmRmj7j7MWb2NWC1u//UzB529wVD6czM/gk4ENgdaAAWu/vSQttnMtl8\nOq33i0VEhihVaEWxZ/qTzeww4FTgQ2a2C1FoD2cgU4GPA/sAy8xsn/jKoH6am9uH0cV2jY31NDW1\njqiNsUY1h0E1h2G4NTc21hdcV+yc/neBHwC3unsTsBj4jyGPBNYBT7h7xt3/SjTF0ziMdkREZBiK\nOtN397uBu81sdzNrAP650Nn5TvwOuNPMvkX0m8Jk4K1htCMiIsNQ1Jm+mc01s78CLwCrgVVmduhQ\nO3P3tcB/AU8CvwUudXdd7y8iUibFzulfC5zk7s8DxFfx3AgcNdQO3f1W4Nah7iciIiNX7Jx+tifw\nAdz9GaIPV4mIyBhS7Jl+zsxOAR6Knx8PZJMZkoiIJKXYM/2LgEXAK8DLRLdg+HRCYxIRkYQMeqZv\nZo8BPVfppICV8eMpwJ0MY05fREQqZ2fTO1eVZRQiIlIWg4a+u68o10BERCR5+mJ0EZGAKPRFRAKi\n0BcRCYhCX0QkIAp9EZGAKPRFRAKi0BcRCYhCX0QkIAp9EZGAKPRFRAKi0BcRCYhCX0QkIAp9EZGA\nKPRFRAKi0BcRCYhCX0QkIAp9EZGAKPRFRAKi0BcRCYhCX0QkIAp9EZGAKPRFRAKi0BcRCYhCX0Qk\nIAp9EZGAKPRFRAKi0BcRCUi6Ep2a2QTgeeDr7n5nqdvv6O7ijudXsiVXW+qmRUQGlS9RO7Mn5VnU\nOK9ErW1XkdAHrgI2JtV4LpehI5ujM1+p8gpJUbq/EiIyWqVK0EZHd0cJWumv7KloZgcABwFLkupj\n0i6T+MKhh9LYWE9TU2tS3YxKqjkMqlmGK5XPl/fM08yWAJcAZwOvDDa9k8lk8+m0pmhERIao4C8b\nZT3TN7OzgN+7+8tmttPtm5vbR9RfiGcGqjkMqjkMw625sbG+4LpyT++cCMw0s4XAXkCXma1x94fL\nPA4RkSCVNfTd/RM9j81sMdH0jgJfRKRMdJ2+iEhAKnZNo7svrlTfIiKh0pm+iEhAFPoiIgFR6IuI\nBEShLyISEIW+iEhARtsdyUqmbWsn3S0ZmreM7FO9Y013XeVqLsVNpoYjU8GaK2Xgmit1BMojMy4b\n1HHebfzERNqtytBvad/MdSvfIKdfZERkjNqzdjPXTJ9f8narMvTrx0/kA5NaaM9DLhvWrYxralMV\nqbmSr3JtbYpsYMdZNVe//aZMSKTdqgz92po0Hz3ocN2gKRCqOQwh1pwEzX+IiAREoS8iEpCqnN7p\n6O7iX1f8iK58O7lcOHOAADU1KdUcANVc/fabMourP3pmydutytBv2dLBhuzrkO4K83eZMtScSkGZ\nv3RtcDrOYQio5peaxyXSbtm/LnEomppahz247kyWKbtO5K0NbaUc0qi3x9TJqjkAqrn6Tdglzdum\nTRnuN2eNjq9LLKe6dC2TJ46jY0tdpYdSVqo5DKpZhiugX5ZEREShLyISEIW+iEhAFPoiIgFR6IuI\nBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhAFPoi\nIgFR6IuIBKTs35xlZt8G5sV9X+vu95R7DCIioSrrmb6ZHQ3McfcjgOOBG8rZv4hI6Mo9vfMocFr8\nuAWYZGa1ZR6DiEiwyjq94+5ZYEv89HzgN/EyEREpg1Q+ny97p2Z2EvDPwHHuvqnQdplMNp9O6xcB\nEZEhShVaUYk3cj8MfAU4frDAB2hubh9RX42N9TQ1tY6ojbFGNYdBNYdhuDU3NtYXXFfW0DezXYHv\nAAvcfWM5+xYRkfKf6X8C2AP4uZn1LDvL3V8r8zhERIJU7jdybwNuK2efIiKynT6RKyISkLK/kVsO\n2UyWJ362nO6uLJlMrtLDSU6q/9N0upZMJqyrYOvqaunuHnrN216+VMELHYpQ/qvfAOrq0nR3ZyrS\n98gN7/Ue7nGGER7iCtlr1ttoPH1eydutytDfsm4DK9ekyKfGVXooFVJX6QFUQIg1V+U/350Ip+Z1\nTa9x5Omlb7cqX8Epb5/GJz8xm3Smi82tnSNub0ifZSjbiV9+wKf19RNobe0o1yAGL7dMnwGprx9P\n61CPcwmGVplz/MjkybvQ1tZVwREM0wj+TkyuH09bCf49jxVTZ70rkXarMvQBdpu5l67rDYRqDkOI\nNSehat/IzeTG6nyniEhyqjL0N29t5crHFnPzUz9W+IuI9FKV0zvZrWm6Wyex4pUnefq1P9M4bjrj\natPUptKka9KkU7XUxn+ma6LHdak0tTW1pGvS1KZq421qqSEFqRQpUtuuAEjFz0nFPzVTNaSIrklI\n9d122/7Rumj/bWvi6xhS29rd/rhni+3tbLsCIdV7rx23W5+ZREtL+w7L+urfdu+V/ZYMeOVD3/b7\nbtN37AMNp++iVGr7azTQFr1X9W5/67h2NrZtGWQcO+zYr+UdHqf6bD9Am33735nUEC4dGbjd/svq\nOvNs3to27HENuF2BXQfaduBNCywdcHGxbW5f09ldR2emK25z6PuX07B77FVXOpXMfceqMvQn7TKO\nfduPZfWWx9nSsI723F8qPSQRkSGZktmL28/4SsnbrcrQH1dXyxWnH0pL53tZuXo9LZ2tdHZ3053L\nkMtlyZAhm8+SyWXJ5rNk8xly+SxZcuTzWXLkyJElRxbIR1dp5OM/tz3f9ij6M74ooc9W8WO2XbWQ\n73fNR77f/ws92nHpwFdB1NSmyGVz/fbctl++0JriFeq70PJ+i1MDbLvTIfXfoGdJbU2KbC6/00a2\nXTiSGmS7ooY1lNdvhC0UGGtNKkWuiCthCh6TohW7f/H9DPcVSdWkyOcKbVdg+RBPuUtzwVlpruua\nMfkdJWmnr6oM/R77793AbuOrusR+QrzCQTWHIcSak1CVb+SKiMjAFPoiIgFR6IuIBEShLyISEIW+\niEhAFPoiIgFR6IuIBEShLyISkNSQ7hUvIiJjms70RUQCotAXEQmIQl9EJCAKfRGRgCj0RUQCotAX\nEQmIQl9EJCBV+Q0jZnY98EGir7D5rLs/XeEhJcLM5gO/AFbGi54Dvg38FKgF3gDOdPeuigywhMxs\nDnAfcL27f9/M9maAOs3sDOByIAfc5u4/rNigR2iAmu8E3g9siDf5jrsvqbKavw3MI8qma4Gnqf7j\n3Lfmj5Lgca66M30z+xCwv7sfAZwPfK/CQ0raCnefH/93KfA14GZ3nwe8CJxX2eGNnJlNAm4ClvZa\n3K/OeLurgQXAfOBzZrZ7mYdbEgVqBvhyr+O9pMpqPhqYE//bPR64geo/zgPVDAke56oLfeBvgXsB\n3H0V0GBmUyo7pLKaD9wfP/4V0V+Ssa4L+Dvg9V7L5tO/zsOBp919k7t3AI8Dc8s4zlIaqOaBVFPN\njwKnxY9bgElU/3EeqObaAbYrWc3VOL0zHfifXs+b4mWbKzOcxB1kZvcDuwPXAJN6TeesB2ZUbGQl\n4u4ZIGNmvRcPVOd0ouNNn+VjToGaAS4xs88T1XYJ1VVzFtgSPz0f+A3w4So/zgPVnCXB41yNZ/p9\npSo9gAStJgr6k4CzgR+y4w/yaq69t0J1Vlv9PwW+5O7HAM8CiwfYZszXbGYnEQXgJX1WVe1x7lNz\nose5GkP/daKfij32JHoDqOq4+1p3v9vd8+7+V+BNoumsCfEmb2fn0wNjVdsAdfY99lVVv7svdfdn\n46f3A++iymo2sw8DXwFOcPdNBHCc+9ac9HGuxtD/HXAqgJkdArzu7q2VHVIyzOwMM7sifjwdeBtw\nB3BKvMkpwAMVGl7SHqZ/nU8Bh5nZbmY2mWjO87EKja/kzOyXZjYzfjofeJ4qqtnMdgW+Ayx0943x\n4qo+zgPVnPRxrspbK5vZN4GjiC5t+kd3/2OFh5QIM6sH/gPYDRhHNNXzDPATYDzwKnCuu3dXbJAl\nYGbvB74L7At0A2uBM4A76VOnmZ0KXEl0ue5N7v7vlRjzSBWo+SbgS0A70EZU8/oqqvlCoqmMv/Ra\nfDZwO9V7nAeq+Q6iaZ5EjnNVhr6IiAysGqd3RESkAIW+iEhAFPoiIgFR6IuIBEShLyISEIW+SELM\n7Bwzu6vS4xDpTaEvIhIQXacvwTOzS4HTie5b9ALRdxL8Gvgt8J54s0+6+1ozO5HoFrft8X8XxssP\nJ7ot7lZgI3AW0SdITya62d9BRB8uOtnd9Y9OKkZn+hI0M/sA8HHgqPie5i1Et++dCdwR38d9OfAF\nM5tI9OnQU9z9aKIfCt+Im7oLWOTuHwJWACfGyw8GLiT6Uow5wCHlqEukkGq8tbLIUMwHZgHL4tsY\nTyK6mdUGd++5RffjRN9YNBtY5+5r4uXLgYvMbA9gN3d/HsDdb4BoTp/oHujt8fO1RLfMEKkYhb6E\nrgu439233cbXzPYF/rfXNimi+530nZbpvbzQb82ZAfYRqRhN70joHgdOiO9ciJldTPTlFA1m9r54\nmyOBPxHdFGuamb0jXr4AeNLdNwBvmdlhcRtfiNsRGXUU+hI0d/8DcDOw3Mz+m2i6ZxPRXS3PMbNH\niG5je338NXXnA3eb2XKir+a8Km7qTOBGM1tBdIdXXaopo5Ku3hHpI57e+W9336vSYxEpNZ3pi4gE\nRGf6IiIB0Zm+iEhAFPoiIgFR6IuIBEShLyISEIW+iEhA/j9cr9yd2Q/RHQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f21028c8e48>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "k7LsoI6hYH9a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pretrain Autoencoder"
      ]
    },
    {
      "metadata": {
        "id": "W4Yy89LJYH9a",
        "colab_type": "code",
        "colab": {},
        "outputId": "56e2e9e1-d502-4caa-fccc-e2f24c9a50a7"
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "DATASET = \"MNIST\"\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_CHANNEL=1\n",
        "HIDDEN_LAYER_SIZE= 128\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/Deep_SVDD/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/Deep_SVDD/\"\n",
        "PRETRAINED_WT_PATH = \"\"\n",
        "\n",
        "## Prepare the data for pretraining CAE\n",
        "x_train = trainX.reshape((len(trainX), 28, 28, 1))\n",
        "x_trainForWtInit= x_train\n",
        "\n",
        "test_ones = test_ones.reshape((len(test_ones), 28, 28, 1))\n",
        "test_sevens = test_sevens.reshape((len(test_sevens), 28, 28, 1))\n",
        "x_test = np.concatenate((test_ones,test_sevens))\n",
        "\n",
        "print(\"Reshaped Training samples for CAE\",x_train.shape)\n",
        "print(\"Reshaped Testing samples for CAE\",x_test.shape)\n",
        "\n",
        "from src.models.Deep_SVDD import Deep_SVDD\n",
        "deep_svdd =   Deep_SVDD(DATASET,x_trainForWtInit,IMG_DIM,HIDDEN_LAYER_SIZE,IMG_HGT,IMG_WDT,IMG_CHANNEL,MODEL_SAVE_PATH,REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainX' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-7deea1159869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m## Prepare the data for pretraining CAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mx_trainForWtInit\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainX' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "CfwOCe5JYH9e",
        "colab_type": "code",
        "colab": {},
        "outputId": "59cfed4a-eef8-47da-b34f-59ec7ff4180a"
      },
      "cell_type": "code",
      "source": [
        "Deep_SVDD()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "2S2obnZNYH9h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j__NnYSqYH9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vqw48vtBYH9l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ksSgdUD2YH9n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IcPzp8zKYH9s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train and Test  FF_NN Model Supervised Model"
      ]
    },
    {
      "metadata": {
        "id": "kgfQX1g-YH9t",
        "colab_type": "code",
        "colab": {},
        "outputId": "3c9b0445-5ad5-4242-e886-2aff7d14af94"
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_DEPTH=1\n",
        "HIDDEN_LAYER_SIZE=196\n",
        "\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/FF_NN/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/FF_NN/\"\n",
        "\n",
        "print(\"[INFO]\",train_Anomaly_X.shape[0],\"Anomalous Samples Appended to training set\")\n",
        "data_train = np.concatenate((trainX,train_Anomaly_X),axis=0)\n",
        "data_train_label = np.concatenate((trainY,train_Anomaly_Y),axis=0)\n",
        "print(\"[INFO]\",data_train.shape[0],\"Training Samples Contains both 1's and 7s\")\n",
        "nClass =2\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "clf_FF_NN =  FF_NN(IMG_DIM,HIDDEN_LAYER_SIZE,IMG_HGT,IMG_WDT,MODEL_SAVE_PATH,REPORT_SAVE_PATH)\n",
        "clf_FF_NN.fit(data_train,data_train_label,NUM_EPOCHS,IMG_HGT,IMG_WDT,IMG_DEPTH,nClass)\n",
        "\n",
        "## Predict the scores \n",
        "auc_FF_NN = clf_FF_NN.score(test_ones,label_ones,test_sevens,label_sevens)\n",
        "print(\"===========\")\n",
        "print(\"AUC: \",auc_FF_NN)\n",
        "print(\"===========\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_Anomaly_X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-cee4a7dae6a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mREPORT_SAVE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPROJECT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/reports/figures/MNIST/FF_NN/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Anomaly_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Anomalous Samples Appended to training set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Anomaly_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdata_train_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Anomaly_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_Anomaly_X' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "M2jLg_y1YH9w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## FakeNoise FF_NN Model"
      ]
    },
    {
      "metadata": {
        "id": "EFmS6rfoYH9w",
        "colab_type": "code",
        "colab": {},
        "outputId": "93d46785-0124-4fea-b295-e2f01569f216"
      },
      "cell_type": "code",
      "source": [
        "## Fake Noise data to be generated which will be added to the training set before training\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_DEPTH=1\n",
        "HIDDEN_LAYER_SIZE=196\n",
        "\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/FAKE_NOISE_FF_NN/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/FAKE_NOISE_FF_NN/\"\n",
        "\n",
        "\n",
        "from src.models.Fake_Noise_FF_NN import Fake_Noise_FF_NN\n",
        "## Remove the Anomalous data and instead add Noise\n",
        "X_Noise,X_NoiseLabel = createData.get_FAKE_Noise_MNIST_TrainingData(trainX)\n",
        "print(\"[INFO]\",X_Noise.shape[0],\"Noise Samples Appended for training set\")\n",
        "data_train = np.concatenate((trainX,X_Noise),axis=0)\n",
        "data_train_label = np.concatenate((trainY,X_NoiseLabel),axis=0)\n",
        "\n",
        "\n",
        "clf_FakeNoise_FF_NN =   Fake_Noise_FF_NN(IMG_DIM,HIDDEN_LAYER_SIZE,IMG_HGT,IMG_WDT,MODEL_SAVE_PATH,REPORT_SAVE_PATH)\n",
        "clf_FakeNoise_FF_NN.fit(data_train,data_train_label,NUM_EPOCHS,IMG_HGT,IMG_WDT,IMG_DEPTH,nClass)\n",
        "# Predict the scores \n",
        "\n",
        "auc_FAKENOISE_FF_NN = clf_FakeNoise_FF_NN.score(test_ones,label_ones,test_sevens,label_sevens)\n",
        "print(\"===========\")\n",
        "print(\"AUC: \",auc_FAKENOISE_FF_NN)\n",
        "print(\"===========\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] 5000 Noise Samples Appended for training set\n",
            "[INFO] compiling model...\n",
            "[INFO] training network...\n",
            "[INFO] serializing network...\n",
            "[INFO] loading network...\n",
            "5050 Actual test samples\n",
            "5050 Predicted test samples\n",
            "===================================\n",
            "auccary_score: 0.8998019801980198\n",
            "roc_auc_score: 0.5534\n",
            "y_true [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "y_pred [1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0]\n",
            "===================================\n",
            "===========\n",
            "AUC:  0.5534\n",
            "===========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gnSNB5duYH90",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SKlearn OCSVM"
      ]
    },
    {
      "metadata": {
        "id": "BgAzFZldYH92",
        "colab_type": "code",
        "colab": {},
        "outputId": "e8017000-ef1f-42fb-849e-6e8f47840e70"
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "##create the classifier\n",
        "from src.models.ocsvmSklearn import OCSVM\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "ocsvm = OCSVM(IMG_HGT,IMG_WDT)\n",
        "nu= 0.01\n",
        "kernel = 'linear'\n",
        "clf = ocsvm.fit(trainX,nu,kernel)\n",
        "res = ocsvm.score(clf,test_ones,test_sevens)\n",
        "auc_OCSVM_linear = res\n",
        "print(\"=\"*35)\n",
        "print(\"AUC:\",res)\n",
        "print(\"=\"*35)\n",
        "\n",
        "kernel = 'rbf'\n",
        "clf = ocsvm.fit(trainX,nu,kernel)\n",
        "res = ocsvm.score(clf,test_ones,test_sevens)\n",
        "auc_OCSVM_rbf = res\n",
        "print(\"=\"*35)\n",
        "print(\"AUC:\",res)\n",
        "print(\"=\"*35)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the OCSVM classifier.....\n",
            "===================================\n",
            "AUC: 0.7869240000000001\n",
            "===================================\n",
            "Training the OCSVM classifier.....\n",
            "===================================\n",
            "AUC: 0.9705560000000001\n",
            "===================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "DAtAx6oQYH95",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## OC-NN"
      ]
    },
    {
      "metadata": {
        "id": "OGTQQ6WXYH96",
        "colab_type": "code",
        "colab": {},
        "outputId": "66266b96-8dda-4eb7-ae15-2551070415c0"
      },
      "cell_type": "code",
      "source": [
        "##create the classifier\n",
        "## Instantiate the object and call the function\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "DATASET= \"MNIST\"\n",
        "IMG_DIM= 784\n",
        "IMG_HGT =28\n",
        "IMG_WDT=28\n",
        "IMG_DEPTH=1\n",
        "HIDDEN_LAYER_SIZE=196\n",
        "nClass=2\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/OC_NN/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/OC_NN/\"\n",
        "PRE_TRAINED_WT_PATH = PROJECT_DIR +\"/models/MNIST/FF_NN/\"\n",
        "\n",
        "from src.models.OC_NN import OC_NN\n",
        "import keras\n",
        "\n",
        "ocnn = OC_NN(DATASET,IMG_DIM,HIDDEN_LAYER_SIZE,IMG_HGT,IMG_WDT,MODEL_SAVE_PATH,REPORT_SAVE_PATH,PRE_TRAINED_WT_PATH)\n",
        "\n",
        "\n",
        "nu= 0.01\n",
        "NUM_EPOCHS = 100\n",
        "ocnn.fit(trainX,nu,NUM_EPOCHS,IMG_HGT,IMG_WDT,IMG_DEPTH,nClass)\n",
        "res = ocnn.score(test_ones,test_sevens) \n",
        "auc_OCNN = res\n",
        "\n",
        "print(\"=\"*35)\n",
        "print(\"AUC:\",res)\n",
        "print(\"=\"*35)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] compiling model...\n",
            "[INFO]  (196,) input  --> hidden layer weights shape ...\n",
            "[INFO]  (2,) hidden --> output layer weights shape ...\n",
            "[INFO] training network...\n",
            "Train on 4500 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "4500/4500 [==============================] - 0s 63us/step - loss: 44.2036 - val_loss: 43.6791\n",
            "evaluation for epoch: 0\n",
            "output: Tensor(\"Print:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 2/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.1708 - val_loss: 43.7103\n",
            "evaluation for epoch: 1\n",
            "output: Tensor(\"Print_1:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 3/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.1486 - val_loss: 43.7392\n",
            "evaluation for epoch: 2\n",
            "output: Tensor(\"Print_2:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 4/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.1274 - val_loss: 43.7661\n",
            "evaluation for epoch: 3\n",
            "output: Tensor(\"Print_3:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 5/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.1070 - val_loss: 43.7910\n",
            "evaluation for epoch: 4\n",
            "output: Tensor(\"Print_4:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 6/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 44.0875 - val_loss: 43.8141\n",
            "evaluation for epoch: 5\n",
            "output: Tensor(\"Print_5:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 7/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.0683 - val_loss: 43.8355\n",
            "evaluation for epoch: 6\n",
            "output: Tensor(\"Print_6:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 8/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.0497 - val_loss: 43.8560\n",
            "evaluation for epoch: 7\n",
            "output: Tensor(\"Print_7:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 9/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.0315 - val_loss: 43.8742\n",
            "evaluation for epoch: 8\n",
            "output: Tensor(\"Print_8:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 10/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 44.0135 - val_loss: 43.8920\n",
            "evaluation for epoch: 9\n",
            "output: Tensor(\"Print_9:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 11/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.9958 - val_loss: 43.9087\n",
            "evaluation for epoch: 10\n",
            "output: Tensor(\"Print_10:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 12/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9783 - val_loss: 43.9235\n",
            "evaluation for epoch: 11\n",
            "output: Tensor(\"Print_11:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 13/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9610 - val_loss: 43.9383\n",
            "evaluation for epoch: 12\n",
            "output: Tensor(\"Print_12:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 14/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9438 - val_loss: 43.9518\n",
            "evaluation for epoch: 13\n",
            "output: Tensor(\"Print_13:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 15/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9268 - val_loss: 43.9646\n",
            "evaluation for epoch: 14\n",
            "output: Tensor(\"Print_14:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 16/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.9098 - val_loss: 43.9768\n",
            "evaluation for epoch: 15\n",
            "output: Tensor(\"Print_15:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 17/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.8930 - val_loss: 43.9876\n",
            "evaluation for epoch: 16\n",
            "output: Tensor(\"Print_16:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 18/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.8763 - val_loss: 43.9983\n",
            "evaluation for epoch: 17\n",
            "output: Tensor(\"Print_17:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 19/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.8597 - val_loss: 44.0087\n",
            "evaluation for epoch: 18\n",
            "output: Tensor(\"Print_18:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 20/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.8431 - val_loss: 44.0186\n",
            "evaluation for epoch: 19\n",
            "output: Tensor(\"Print_19:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 21/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.8266 - val_loss: 44.0276\n",
            "evaluation for epoch: 20\n",
            "output: Tensor(\"Print_20:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 22/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.8102 - val_loss: 44.0363\n",
            "evaluation for epoch: 21\n",
            "output: Tensor(\"Print_21:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 23/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.7938 - val_loss: 44.0446\n",
            "evaluation for epoch: 22\n",
            "output: Tensor(\"Print_22:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 24/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.7775 - val_loss: 44.0524\n",
            "evaluation for epoch: 23\n",
            "output: Tensor(\"Print_23:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 25/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.7613 - val_loss: 44.0602\n",
            "evaluation for epoch: 24\n",
            "output: Tensor(\"Print_24:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 26/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 43.7450 - val_loss: 44.0676\n",
            "evaluation for epoch: 25\n",
            "output: Tensor(\"Print_25:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 27/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.7290 - val_loss: 44.0743\n",
            "evaluation for epoch: 26\n",
            "output: Tensor(\"Print_26:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 28/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.7129 - val_loss: 44.0810\n",
            "evaluation for epoch: 27\n",
            "output: Tensor(\"Print_27:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 29/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6969 - val_loss: 44.0873\n",
            "evaluation for epoch: 28\n",
            "output: Tensor(\"Print_28:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 30/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6810 - val_loss: 44.0933\n",
            "evaluation for epoch: 29\n",
            "output: Tensor(\"Print_29:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 31/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6651 - val_loss: 44.0989\n",
            "evaluation for epoch: 30\n",
            "output: Tensor(\"Print_30:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 32/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6492 - val_loss: 44.1047\n",
            "evaluation for epoch: 31\n",
            "output: Tensor(\"Print_31:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 33/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6334 - val_loss: 44.1100\n",
            "evaluation for epoch: 32\n",
            "output: Tensor(\"Print_32:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 34/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6176 - val_loss: 44.1152\n",
            "evaluation for epoch: 33\n",
            "output: Tensor(\"Print_33:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 35/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.6019 - val_loss: 44.1201\n",
            "evaluation for epoch: 34\n",
            "output: Tensor(\"Print_34:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 36/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.5862 - val_loss: 44.1254\n",
            "evaluation for epoch: 35\n",
            "output: Tensor(\"Print_35:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 37/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.5705 - val_loss: 44.1299\n",
            "evaluation for epoch: 36\n",
            "output: Tensor(\"Print_36:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 38/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.5548 - val_loss: 44.1340\n",
            "evaluation for epoch: 37\n",
            "output: Tensor(\"Print_37:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 39/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.5392 - val_loss: 44.1384\n",
            "evaluation for epoch: 38\n",
            "output: Tensor(\"Print_38:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 40/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.5235 - val_loss: 44.1428\n",
            "evaluation for epoch: 39\n",
            "output: Tensor(\"Print_39:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 41/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.5081 - val_loss: 44.1472\n",
            "evaluation for epoch: 40\n",
            "output: Tensor(\"Print_40:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 42/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.4925 - val_loss: 44.1508\n",
            "evaluation for epoch: 41\n",
            "output: Tensor(\"Print_41:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 43/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 43.4770 - val_loss: 44.1549\n",
            "evaluation for epoch: 42\n",
            "output: Tensor(\"Print_42:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 44/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.4615 - val_loss: 44.1586\n",
            "evaluation for epoch: 43\n",
            "output: Tensor(\"Print_43:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 45/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.4460 - val_loss: 44.1624\n",
            "evaluation for epoch: 44\n",
            "output: Tensor(\"Print_44:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 46/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.4305 - val_loss: 44.1662\n",
            "evaluation for epoch: 45\n",
            "output: Tensor(\"Print_45:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 47/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.4151 - val_loss: 44.1692\n",
            "evaluation for epoch: 46\n",
            "output: Tensor(\"Print_46:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 48/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.3998 - val_loss: 44.1723\n",
            "evaluation for epoch: 47\n",
            "output: Tensor(\"Print_47:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 49/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.3843 - val_loss: 44.1755\n",
            "evaluation for epoch: 48\n",
            "output: Tensor(\"Print_48:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 50/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.3690 - val_loss: 44.1785\n",
            "evaluation for epoch: 49\n",
            "output: Tensor(\"Print_49:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 51/100\n",
            "4500/4500 [==============================] - 0s 28us/step - loss: 43.3536 - val_loss: 44.1808\n",
            "evaluation for epoch: 50\n",
            "output: Tensor(\"Print_50:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 52/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.3383 - val_loss: 44.1833\n",
            "evaluation for epoch: 51\n",
            "output: Tensor(\"Print_51:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 53/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.3230 - val_loss: 44.1867\n",
            "evaluation for epoch: 52\n",
            "output: Tensor(\"Print_52:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 54/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.3077 - val_loss: 44.1901\n",
            "evaluation for epoch: 53\n",
            "output: Tensor(\"Print_53:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 55/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.2924 - val_loss: 44.1927\n",
            "evaluation for epoch: 54\n",
            "output: Tensor(\"Print_54:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 56/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.2772 - val_loss: 44.1949\n",
            "evaluation for epoch: 55\n",
            "output: Tensor(\"Print_55:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 57/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.2620 - val_loss: 44.1971\n",
            "evaluation for epoch: 56\n",
            "output: Tensor(\"Print_56:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 58/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.2467 - val_loss: 44.1995\n",
            "evaluation for epoch: 57\n",
            "output: Tensor(\"Print_57:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 59/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.2315 - val_loss: 44.2022\n",
            "evaluation for epoch: 58\n",
            "output: Tensor(\"Print_58:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 60/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.2163 - val_loss: 44.2043\n",
            "evaluation for epoch: 59\n",
            "output: Tensor(\"Print_59:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 61/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.2012 - val_loss: 44.2068\n",
            "evaluation for epoch: 60\n",
            "output: Tensor(\"Print_60:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 62/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.1860 - val_loss: 44.2088\n",
            "evaluation for epoch: 61\n",
            "output: Tensor(\"Print_61:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 63/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1708 - val_loss: 44.2115\n",
            "evaluation for epoch: 62\n",
            "output: Tensor(\"Print_62:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 64/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1557 - val_loss: 44.2135\n",
            "evaluation for epoch: 63\n",
            "output: Tensor(\"Print_63:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 65/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1406 - val_loss: 44.2155\n",
            "evaluation for epoch: 64\n",
            "output: Tensor(\"Print_64:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 66/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1255 - val_loss: 44.2175\n",
            "evaluation for epoch: 65\n",
            "output: Tensor(\"Print_65:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 67/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.1105 - val_loss: 44.2191\n",
            "evaluation for epoch: 66\n",
            "output: Tensor(\"Print_66:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 68/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.0953 - val_loss: 44.2209\n",
            "evaluation for epoch: 67\n",
            "output: Tensor(\"Print_67:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 69/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.0802 - val_loss: 44.2227\n",
            "evaluation for epoch: 68\n",
            "output: Tensor(\"Print_68:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 70/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.0652 - val_loss: 44.2248\n",
            "evaluation for epoch: 69\n",
            "output: Tensor(\"Print_69:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 71/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 43.0502 - val_loss: 44.2263\n",
            "evaluation for epoch: 70\n",
            "output: Tensor(\"Print_70:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 72/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.0350 - val_loss: 44.2279\n",
            "evaluation for epoch: 71\n",
            "output: Tensor(\"Print_71:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 73/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.0201 - val_loss: 44.2291\n",
            "evaluation for epoch: 72\n",
            "output: Tensor(\"Print_72:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 74/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 43.0051 - val_loss: 44.2304\n",
            "evaluation for epoch: 73\n",
            "output: Tensor(\"Print_73:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 75/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9900 - val_loss: 44.2318\n",
            "evaluation for epoch: 74\n",
            "output: Tensor(\"Print_74:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 76/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9750 - val_loss: 44.2337\n",
            "evaluation for epoch: 75\n",
            "output: Tensor(\"Print_75:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 77/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9601 - val_loss: 44.2353\n",
            "evaluation for epoch: 76\n",
            "output: Tensor(\"Print_76:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 78/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 42.9451 - val_loss: 44.2366\n",
            "evaluation for epoch: 77\n",
            "output: Tensor(\"Print_77:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 79/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9301 - val_loss: 44.2378\n",
            "evaluation for epoch: 78\n",
            "output: Tensor(\"Print_78:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 80/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.9153 - val_loss: 44.2391\n",
            "evaluation for epoch: 79\n",
            "output: Tensor(\"Print_79:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 81/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.9004 - val_loss: 44.2408\n",
            "evaluation for epoch: 80\n",
            "output: Tensor(\"Print_80:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 82/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 42.8855 - val_loss: 44.2420\n",
            "evaluation for epoch: 81\n",
            "output: Tensor(\"Print_81:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 83/100\n",
            "4500/4500 [==============================] - 0s 32us/step - loss: 42.8707 - val_loss: 44.2429\n",
            "evaluation for epoch: 82\n",
            "output: Tensor(\"Print_82:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 84/100\n",
            "4500/4500 [==============================] - 0s 31us/step - loss: 42.8558 - val_loss: 44.2441\n",
            "evaluation for epoch: 83\n",
            "output: Tensor(\"Print_83:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 85/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.8410 - val_loss: 44.2451\n",
            "evaluation for epoch: 84\n",
            "output: Tensor(\"Print_84:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 86/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4500/4500 [==============================] - 0s 31us/step - loss: 42.8262 - val_loss: 44.2463\n",
            "evaluation for epoch: 85\n",
            "output: Tensor(\"Print_85:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 87/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.8114 - val_loss: 44.2472\n",
            "evaluation for epoch: 86\n",
            "output: Tensor(\"Print_86:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 88/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.7966 - val_loss: 44.2481\n",
            "evaluation for epoch: 87\n",
            "output: Tensor(\"Print_87:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 89/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.7819 - val_loss: 44.2487\n",
            "evaluation for epoch: 88\n",
            "output: Tensor(\"Print_88:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 90/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.7671 - val_loss: 44.2499\n",
            "evaluation for epoch: 89\n",
            "output: Tensor(\"Print_89:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 91/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.7523 - val_loss: 44.2506\n",
            "evaluation for epoch: 90\n",
            "output: Tensor(\"Print_90:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 92/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.7376 - val_loss: 44.2513\n",
            "evaluation for epoch: 91\n",
            "output: Tensor(\"Print_91:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 93/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.7229 - val_loss: 44.2522\n",
            "evaluation for epoch: 92\n",
            "output: Tensor(\"Print_92:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 94/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.7082 - val_loss: 44.2533\n",
            "evaluation for epoch: 93\n",
            "output: Tensor(\"Print_93:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 95/100\n",
            "4500/4500 [==============================] - 0s 29us/step - loss: 42.6934 - val_loss: 44.2529\n",
            "evaluation for epoch: 94\n",
            "output: Tensor(\"Print_94:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 96/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6788 - val_loss: 44.2538\n",
            "evaluation for epoch: 95\n",
            "output: Tensor(\"Print_95:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 97/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6641 - val_loss: 44.2544\n",
            "evaluation for epoch: 96\n",
            "output: Tensor(\"Print_96:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 98/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6494 - val_loss: 44.2557\n",
            "evaluation for epoch: 97\n",
            "output: Tensor(\"Print_97:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 99/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6347 - val_loss: 44.2564\n",
            "evaluation for epoch: 98\n",
            "output: Tensor(\"Print_98:0\", shape=(?, 2), dtype=float32)\n",
            "Epoch 100/100\n",
            "4500/4500 [==============================] - 0s 30us/step - loss: 42.6200 - val_loss: 44.2568\n",
            "evaluation for epoch: 99\n",
            "output: Tensor(\"Print_99:0\", shape=(?, 2), dtype=float32)\n",
            "[INFO] serializing network and saving trained weights...\n",
            "[INFO] Saving model layer weights...\n",
            "[INFO] loading network...\n",
            "5050 Actual test samples\n",
            "===================================\n",
            "auccary_score: 0.9984158415841584\n",
            "roc_auc_score: 0.9199999999999999\n",
            "y_true [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "y_pred [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0]\n",
            "===================================\n",
            "===================================\n",
            "AUC: 0.9199999999999999\n",
            "===================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/raghav/envPython3/lib/python3.6/site-packages/keras/engine/sequential.py:252: UserWarning: Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
            "  warnings.warn('Network returning invalid probability values. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9sE2ZsxnYH98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Comparing  AUC scores of various methods"
      ]
    },
    {
      "metadata": {
        "id": "vN9-Cf0tYH99",
        "colab_type": "code",
        "colab": {},
        "outputId": "b359b88a-1ee6-448b-e214-160716380ae1"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "auc = np.zeros((1,5))\n",
        "auc[0][0] = auc_FF_NN\n",
        "auc[0][1] = auc_FAKENOISE_FF_NN\n",
        "auc[0][2] = auc_OCSVM_linear\n",
        "auc[0][3] = auc_OCSVM_rbf\n",
        "auc[0][4] = auc_OCNN\n",
        "\n",
        "\n",
        "aucList = [auc_FF_NN,auc_FAKENOISE_FF_NN, auc_OCSVM_linear,auc_OCSVM_rbf, auc_OCNN]\n",
        "\n",
        "index = ['FF_NN', 'Fake_NN', 'OCSVM_L','OCSVM_rbf','OCNN']\n",
        "df = pd.DataFrame({'auc': aucList}, index=index)\n",
        "ax = df.plot.bar(rot=0)\n",
        "\n",
        "plt.ylabel('AUC')\n",
        "plt.xlabel('Methods')\n",
        "plt.title('AUC Comparision for MNIST Dataset ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'AUC Comparision for MNIST Dataset ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "7JXO0M-sYH-B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}